import boto3
import os
import logging
from typing import Optional, BinaryIO, Union
from botocore.exceptions import ClientError
from datetime import datetime
import json
import uuid
import mimetypes

from .company_file_manager import CompanyFileManager, FileType

logger = logging.getLogger(__name__)


def clean_schema_for_gemini(schema):
    """
    Clean JSON schema for Gemini API compatibility by removing unsupported fields.
    
    Args:
        schema: The JSON schema dictionary
        
    Returns:
        Cleaned schema dictionary safe for Gemini API
    """
    if not isinstance(schema, dict):
        return schema
    
    # Fields that cause Gemini API errors
    problematic_fields = ["$schema", "$id", "$ref", "definitions", "patternProperties"]
    
    cleaned_schema = {}
    for key, value in schema.items():
        if key in problematic_fields:
            logger.info(f"Removing problematic schema field for Gemini compatibility: {key}")
            continue
            
        if isinstance(value, dict):
            # Recursively clean nested dictionaries
            cleaned_value = clean_schema_for_gemini(value)
            # Only add if the cleaned value is not empty
            if cleaned_value:
                cleaned_schema[key] = cleaned_value
        elif isinstance(value, list):
            cleaned_schema[key] = [
                clean_schema_for_gemini(item) if isinstance(item, dict) else item
                for item in value
            ]
        else:
            cleaned_schema[key] = value
    
    return cleaned_schema


class S3StorageManager:
    """AWS S3Êñá‰ª∂Â≠òÂÇ®ÁÆ°ÁêÜÂô® - ‰ΩøÁî®ÂçïÂ≠òÂÇ®Ê°∂Â§öÊñá‰ª∂Â§πÁªìÊûÑ"""

    def __init__(self, bucket_name: str, region: str = "ap-southeast-1", enable_legacy_compatibility: bool = True):
        """
        ÂàùÂßãÂåñS3Â≠òÂÇ®ÁÆ°ÁêÜÂô®

        Args:
            bucket_name: S3Â≠òÂÇ®Ê°∂ÂêçÁß∞
            region: AWSÂå∫Âüü
            enable_legacy_compatibility: ÊòØÂê¶ÂêØÁî®ÊóßË∑ØÂæÑÂÖºÂÆπÊ®°Âºè
        """
        self.bucket_name = bucket_name
        self.region = region
        self.enable_legacy_compatibility = enable_legacy_compatibility
        
        # Initialize Company File Manager for ID-based paths
        self.company_file_manager = CompanyFileManager()
        
        # Legacy prefixes (kept for backward compatibility)
        self.upload_prefix = "upload/"
        self.results_prefix = "results/"
        self.exports_prefix = "exports/"
        self.prompts_prefix = "prompts/"
        self.schemas_prefix = "schemas/"
        
        self._s3_client = None
        self._s3_resource = None

    @property
    def s3_client(self):
        """Âª∂ËøüÂàùÂßãÂåñS3ÂÆ¢Êà∑Á´Ø"""
        if self._s3_client is None:
            try:
                self._s3_client = boto3.client("s3", region_name=self.region)
                logger.info(f"‚úÖ S3ÂÆ¢Êà∑Á´ØÂàùÂßãÂåñÊàêÂäüÔºåÂå∫ÂüüÔºö{self.region}")
            except Exception as e:
                logger.error(f"‚ùå S3ÂÆ¢Êà∑Á´ØÂàùÂßãÂåñÂ§±Ë¥•Ôºö{e}")
                raise
        return self._s3_client

    @property
    def s3_resource(self):
        """Âª∂ËøüÂàùÂßãÂåñS3ËµÑÊ∫ê"""
        if self._s3_resource is None:
            try:
                self._s3_resource = boto3.resource("s3", region_name=self.region)
                logger.info(f"‚úÖ S3ËµÑÊ∫êÂàùÂßãÂåñÊàêÂäüÔºåÂå∫ÂüüÔºö{self.region}")
            except Exception as e:
                logger.error(f"‚ùå S3ËµÑÊ∫êÂàùÂßãÂåñÂ§±Ë¥•Ôºö{e}")
                raise
        return self._s3_resource

    def ensure_bucket_exists(self) -> bool:
        """Á°Æ‰øùS3Â≠òÂÇ®Ê°∂Â≠òÂú®"""
        try:
            self.s3_client.head_bucket(Bucket=self.bucket_name)
            logger.info(f"‚úÖ S3Â≠òÂÇ®Ê°∂Â∑≤Â≠òÂú®Ôºö{self.bucket_name}")
            return True
        except ClientError as e:
            error_code = int(e.response["Error"]["Code"])

            if error_code == 404:
                # Â≠òÂÇ®Ê°∂‰∏çÂ≠òÂú®ÔºåÂ∞ùËØïÂàõÂª∫
                try:
                    if self.region == "us-east-1":
                        # us-east-1Âå∫ÂüüÂàõÂª∫Â≠òÂÇ®Ê°∂ÁöÑÁâπÊÆäÂ§ÑÁêÜ
                        self.s3_client.create_bucket(Bucket=self.bucket_name)
                    else:
                        self.s3_client.create_bucket(
                            Bucket=self.bucket_name,
                            CreateBucketConfiguration={
                                "LocationConstraint": self.region
                            },
                        )

                    logger.info(f"‚úÖ ÊàêÂäüÂàõÂª∫S3Â≠òÂÇ®Ê°∂Ôºö{self.bucket_name}")
                    return True
                except ClientError as create_error:
                    logger.error(f"‚ùå ÂàõÂª∫S3Â≠òÂÇ®Ê°∂Â§±Ë¥•Ôºö{create_error}")
                    return False
            else:
                logger.error(f"‚ùå Ê£ÄÊü•S3Â≠òÂÇ®Ê°∂Êó∂Âá∫ÈîôÔºö{e}")
                return False

    def upload_file(
        self,
        file_content: Union[BinaryIO, bytes],
        key: str,
        content_type: Optional[str] = None,
        metadata: Optional[dict] = None,
    ) -> bool:
        """
        ‰∏ä‰º†Êñá‰ª∂Âà∞S3

        Args:
            file_content: Êñá‰ª∂ÂÜÖÂÆπÔºàÊñá‰ª∂ÂØπË±°ÊàñÂ≠óËäÇÊï∞ÊçÆÔºâ
            key: S3‰∏≠ÁöÑÊñá‰ª∂ÈîÆÂêç
            content_type: Êñá‰ª∂ÁöÑMIMEÁ±ªÂûã
            metadata: Êñá‰ª∂ÂÖÉÊï∞ÊçÆ

        Returns:
            bool: ‰∏ä‰º†ÊòØÂê¶ÊàêÂäü
        """
        try:
            # Ëá™Âä®Êé®Êñ≠content_type
            if content_type is None:
                content_type, _ = mimetypes.guess_type(key)
                if content_type is None:
                    content_type = "application/octet-stream"

            # ÂáÜÂ§á‰∏ä‰º†ÂèÇÊï∞ - ÈÅøÂÖçÂèåÈáçÂâçÁºÄ
            # Â¶ÇÊûú key Â∑≤ÁªèÂåÖÂê´ upload_prefixÔºåÂ∞±Áõ¥Êé•Áî® keyÔºõÂê¶ÂàôÊâçÂä†ÂâçÁºÄ
            if key.startswith(self.upload_prefix):
                final_key = key
            else:
                final_key = f"{self.upload_prefix}{key}"

            upload_args = {
                "Bucket": self.bucket_name,
                "Key": final_key,
                "ContentType": content_type,
            }

            # Ê∑ªÂä†ÂÖÉÊï∞ÊçÆ
            if metadata:
                upload_args["Metadata"] = metadata

            # ÊâßË°å‰∏ä‰º†
            if hasattr(file_content, "read"):
                # Êñá‰ª∂ÂØπË±° - upload_fileobj ÈúÄË¶ÅÂçïÁã¨ÁöÑÂèÇÊï∞Ê†ºÂºè
                extra_args = {"ContentType": content_type}
                if metadata:
                    extra_args["Metadata"] = metadata

                self.s3_client.upload_fileobj(
                    file_content,
                    upload_args["Bucket"],
                    upload_args["Key"],
                    ExtraArgs=extra_args,
                )
            else:
                # Â≠óËäÇÊï∞ÊçÆ
                self.s3_client.put_object(Body=file_content, **upload_args)

            logger.info(
                f"‚úÖ Êñá‰ª∂‰∏ä‰º†ÊàêÂäüÔºös3://{self.bucket_name}/{final_key}"
            )
            return True

        except ClientError as e:
            logger.error(f"‚ùå Êñá‰ª∂‰∏ä‰º†Â§±Ë¥•Ôºö{e}")
            return False
        except Exception as e:
            logger.error(f"‚ùå Êñá‰ª∂‰∏ä‰º†Êó∂ÂèëÁîüÊú™Áü•ÈîôËØØÔºö{e}")
            return False

    def save_json_result(
        self, key: str, data: dict, metadata: Optional[dict] = None
    ) -> bool:
        """
        ‰øùÂ≠òJSONÁªìÊûúÂà∞resultsÊñá‰ª∂Â§π

        Args:
            key: Êñá‰ª∂ÈîÆÂêç
            data: JSONÊï∞ÊçÆ
            metadata: Êñá‰ª∂ÂÖÉÊï∞ÊçÆ

        Returns:
            bool: ‰øùÂ≠òÊòØÂê¶ÊàêÂäü
        """
        try:
            json_content = json.dumps(data, ensure_ascii=False, indent=2).encode(
                "utf-8"
            )

            upload_args = {
                "Bucket": self.bucket_name,
                "Key": f"{self.results_prefix}{key}",
                "Body": json_content,
                "ContentType": "application/json",
                "ContentEncoding": "utf-8",
            }

            if metadata:
                upload_args["Metadata"] = metadata

            self.s3_client.put_object(**upload_args)
            logger.info(
                f"‚úÖ JSONÁªìÊûú‰øùÂ≠òÊàêÂäüÔºös3://{self.bucket_name}/{self.results_prefix}{key}"
            )
            return True

        except Exception as e:
            logger.error(f"‚ùå JSONÁªìÊûú‰øùÂ≠òÂ§±Ë¥•Ôºö{e}")
            return False

    def get_json_result(self, key: str) -> Optional[dict]:
        """
        ‰ªéresultsÊñá‰ª∂Â§πËé∑ÂèñJSONÁªìÊûú

        Args:
            key: Êñá‰ª∂ÈîÆÂêç

        Returns:
            Optional[dict]: JSONÊï∞ÊçÆÔºåÂ§±Ë¥•Êó∂ËøîÂõûNone
        """
        try:
            response = self.s3_client.get_object(
                Bucket=self.bucket_name, Key=f"{self.results_prefix}{key}"
            )
            content = response["Body"].read().decode("utf-8")
            data = json.loads(content)
            logger.info(
                f"‚úÖ JSONÁªìÊûúËé∑ÂèñÊàêÂäüÔºös3://{self.bucket_name}/{self.results_prefix}{key}"
            )
            return data
        except ClientError as e:
            if e.response["Error"]["Code"] == "NoSuchKey":
                logger.warning(
                    f"‚ö†Ô∏è JSONÊñá‰ª∂‰∏çÂ≠òÂú®Ôºös3://{self.bucket_name}/{self.results_prefix}{key}"
                )
            else:
                logger.error(f"‚ùå JSONÁªìÊûúËé∑ÂèñÂ§±Ë¥•Ôºö{e}")
            return None
        except Exception as e:
            logger.error(f"‚ùå JSONÁªìÊûúËé∑ÂèñÊó∂ÂèëÁîüÊú™Áü•ÈîôËØØÔºö{e}")
            return None

    def save_excel_export(
        self,
        key: str,
        file_content: Union[BinaryIO, bytes],
        metadata: Optional[dict] = None,
    ) -> bool:
        """
        ‰øùÂ≠òExcelÊñá‰ª∂Âà∞exportsÊñá‰ª∂Â§π

        Args:
            key: Êñá‰ª∂ÈîÆÂêç
            file_content: ExcelÊñá‰ª∂ÂÜÖÂÆπ
            metadata: Êñá‰ª∂ÂÖÉÊï∞ÊçÆ

        Returns:
            bool: ‰øùÂ≠òÊòØÂê¶ÊàêÂäü
        """
        try:
            if hasattr(file_content, "read"):
                # Êñá‰ª∂ÂØπË±° - upload_fileobj ÈúÄË¶ÅÂçïÁã¨ÁöÑÂèÇÊï∞Ê†ºÂºè
                extra_args = {
                    "ContentType": "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                }
                if metadata:
                    extra_args["Metadata"] = metadata

                self.s3_client.upload_fileobj(
                    file_content,
                    self.bucket_name,
                    f"{self.exports_prefix}{key}",
                    ExtraArgs=extra_args,
                )
            else:
                upload_args = {
                    "Bucket": self.bucket_name,
                    "Key": f"{self.exports_prefix}{key}",
                    "Body": file_content,
                    "ContentType": "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                }
                if metadata:
                    upload_args["Metadata"] = metadata
                self.s3_client.put_object(**upload_args)

            logger.info(
                f"‚úÖ ExcelÊñá‰ª∂‰øùÂ≠òÊàêÂäüÔºös3://{self.bucket_name}/{self.exports_prefix}{key}"
            )
            return True

        except Exception as e:
            logger.error(f"‚ùå ExcelÊñá‰ª∂‰øùÂ≠òÂ§±Ë¥•Ôºö{e}")
            return False

    def get_excel_export(self, key: str) -> Optional[bytes]:
        """
        ‰ªéexportsÊñá‰ª∂Â§πËé∑ÂèñExcelÊñá‰ª∂

        Args:
            key: Êñá‰ª∂ÈîÆÂêç

        Returns:
            Optional[bytes]: ExcelÊñá‰ª∂ÂÜÖÂÆπÔºåÂ§±Ë¥•Êó∂ËøîÂõûNone
        """
        try:
            response = self.s3_client.get_object(
                Bucket=self.bucket_name, Key=f"{self.exports_prefix}{key}"
            )
            content = response["Body"].read()
            logger.info(
                f"‚úÖ ExcelÊñá‰ª∂Ëé∑ÂèñÊàêÂäüÔºös3://{self.bucket_name}/{self.exports_prefix}{key}"
            )
            return content
        except ClientError as e:
            if e.response["Error"]["Code"] == "NoSuchKey":
                logger.warning(
                    f"‚ö†Ô∏è ExcelÊñá‰ª∂‰∏çÂ≠òÂú®Ôºös3://{self.bucket_name}/{self.exports_prefix}{key}"
                )
            else:
                logger.error(f"‚ùå ExcelÊñá‰ª∂Ëé∑ÂèñÂ§±Ë¥•Ôºö{e}")
            return None
        except Exception as e:
            logger.error(f"‚ùå ExcelÊñá‰ª∂Ëé∑ÂèñÊó∂ÂèëÁîüÊú™Áü•ÈîôËØØÔºö{e}")
            return None

    def download_file(self, key: str, folder: str = "upload") -> Optional[bytes]:
        """
        ‰ªéS3‰∏ãËΩΩÊñá‰ª∂

        Args:
            key: Êñá‰ª∂ÈîÆÂêç
            folder: Êñá‰ª∂Â§πÂêçÁß∞ (upload/results/exports)

        Returns:
            Optional[bytes]: Êñá‰ª∂ÂÜÖÂÆπÔºåÂ§±Ë¥•Êó∂ËøîÂõûNone
        """
        folder_prefix = f"{folder}/" if not folder.endswith("/") else folder
        full_key = f"{folder_prefix}{key}"

        try:
            response = self.s3_client.get_object(Bucket=self.bucket_name, Key=full_key)
            content = response["Body"].read()
            logger.info(f"‚úÖ Êñá‰ª∂‰∏ãËΩΩÊàêÂäüÔºös3://{self.bucket_name}/{full_key}")
            return content
        except ClientError as e:
            if e.response["Error"]["Code"] == "NoSuchKey":
                logger.warning(f"‚ö†Ô∏è Êñá‰ª∂‰∏çÂ≠òÂú®Ôºös3://{self.bucket_name}/{full_key}")
            else:
                logger.error(f"‚ùå Êñá‰ª∂‰∏ãËΩΩÂ§±Ë¥•Ôºö{e}")
            return None
        except Exception as e:
            logger.error(f"‚ùå Êñá‰ª∂‰∏ãËΩΩÊó∂ÂèëÁîüÊú™Áü•ÈîôËØØÔºö{e}")
            return None

    def delete_file(self, key: str, folder: str = "upload") -> bool:
        """
        ‰ªéS3Âà†Èô§Êñá‰ª∂

        Args:
            key: Êñá‰ª∂ÈîÆÂêç
            folder: Êñá‰ª∂Â§πÂêçÁß∞ (upload/results/exports)

        Returns:
            bool: Âà†Èô§ÊòØÂê¶ÊàêÂäü
        """
        folder_prefix = f"{folder}/" if not folder.endswith("/") else folder
        full_key = f"{folder_prefix}{key}"

        try:
            self.s3_client.delete_object(Bucket=self.bucket_name, Key=full_key)
            logger.info(f"‚úÖ Êñá‰ª∂Âà†Èô§ÊàêÂäüÔºös3://{self.bucket_name}/{full_key}")
            return True
        except ClientError as e:
            logger.error(f"‚ùå Êñá‰ª∂Âà†Èô§Â§±Ë¥•Ôºö{e}")
            return False
        except Exception as e:
            logger.error(f"‚ùå Êñá‰ª∂Âà†Èô§Êó∂ÂèëÁîüÊú™Áü•ÈîôËØØÔºö{e}")
            return False

    def file_exists(self, key: str, folder: str = "upload") -> bool:
        """
        Ê£ÄÊü•Êñá‰ª∂ÊòØÂê¶Âú®S3‰∏≠Â≠òÂú®

        Args:
            key: Êñá‰ª∂ÈîÆÂêç
            folder: Êñá‰ª∂Â§πÂêçÁß∞ (upload/results/exports)

        Returns:
            bool: Êñá‰ª∂ÊòØÂê¶Â≠òÂú®
        """
        folder_prefix = f"{folder}/" if not folder.endswith("/") else folder
        full_key = f"{folder_prefix}{key}"

        try:
            self.s3_client.head_object(Bucket=self.bucket_name, Key=full_key)
            return True
        except ClientError as e:
            if e.response["Error"]["Code"] == "404":
                return False
            else:
                logger.error(f"‚ùå Ê£ÄÊü•Êñá‰ª∂Â≠òÂú®Êó∂Âá∫ÈîôÔºö{e}")
                return False

    def get_file_info(self, key: str, folder: str = "upload") -> Optional[dict]:
        """
        Ëé∑ÂèñS3Êñá‰ª∂‰ø°ÊÅØ

        Args:
            key: Êñá‰ª∂ÈîÆÂêç
            folder: Êñá‰ª∂Â§πÂêçÁß∞ (upload/results/exports)

        Returns:
            Optional[dict]: Êñá‰ª∂‰ø°ÊÅØÔºåÂåÖÂê´Â§ßÂ∞è„ÄÅ‰øÆÊîπÊó∂Èó¥Á≠â
        """
        folder_prefix = f"{folder}/" if not folder.endswith("/") else folder
        full_key = f"{folder_prefix}{key}"

        try:
            response = self.s3_client.head_object(Bucket=self.bucket_name, Key=full_key)
            return {
                "size": response.get("ContentLength", 0),
                "last_modified": response.get("LastModified"),
                "content_type": response.get("ContentType", "application/octet-stream"),
                "metadata": response.get("Metadata", {}),
            }
        except ClientError as e:
            if e.response["Error"]["Code"] == "404":
                logger.warning(f"‚ö†Ô∏è Êñá‰ª∂‰∏çÂ≠òÂú®Ôºös3://{self.bucket_name}/{full_key}")
            else:
                logger.error(f"‚ùå Ëé∑ÂèñÊñá‰ª∂‰ø°ÊÅØÂ§±Ë¥•Ôºö{e}")
            return None

    def generate_presigned_url(
        self, key: str, expires_in: int = 3600, folder: str = "upload"
    ) -> Optional[str]:
        """
        ÁîüÊàêÈ¢ÑÁ≠æÂêçURLÁî®‰∫é‰∏¥Êó∂ËÆøÈóÆÊñá‰ª∂

        Args:
            key: Êñá‰ª∂ÈîÆÂêç
            expires_in: URLËøáÊúüÊó∂Èó¥ÔºàÁßíÔºâ
            folder: Êñá‰ª∂Â§πÂêçÁß∞ (upload/results/exports)

        Returns:
            Optional[str]: È¢ÑÁ≠æÂêçURL
        """
        folder_prefix = f"{folder}/" if not folder.endswith("/") else folder
        full_key = f"{folder_prefix}{key}"

        try:
            filename = os.path.basename(full_key) or "download"
            safe_filename = filename.replace('"', '')
            content_type, _ = mimetypes.guess_type(safe_filename)

            params = {
                "Bucket": self.bucket_name,
                "Key": full_key,
                "ResponseContentDisposition": f'attachment; filename="{safe_filename}"',
            }

            if content_type:
                params["ResponseContentType"] = content_type

            url = self.s3_client.generate_presigned_url(
                "get_object",
                Params=params,
                ExpiresIn=expires_in,
            )
            logger.info(f"‚úÖ ÁîüÊàêÈ¢ÑÁ≠æÂêçURLÊàêÂäüÔºös3://{self.bucket_name}/{full_key}")
            return url
        except ClientError as e:
            logger.error(f"‚ùå ÁîüÊàêÈ¢ÑÁ≠æÂêçURLÂ§±Ë¥•Ôºö{e}")
            return None

    def generate_presigned_url_for_path(self, stored_path: str, expires_in: int = 3600) -> Optional[str]:
        """
        Generate presigned URL for stored database path (S3 URI or relative path)
        
        Args:
            stored_path: Full S3 URI (s3://bucket/key) or relative path within current bucket
            expires_in: URL expiration time in seconds
            
        Returns:
            Optional[str]: Presigned URL for direct download
        """
        try:
            if not stored_path:
                logger.warning("‚ö†Ô∏è Empty stored path provided")
                return None
            
            # Handle S3 URI format: s3://bucket/key/path/file.ext
            if stored_path.startswith('s3://'):
                # Parse S3 URI
                s3_parts = stored_path[5:].split('/', 1)  # Remove 's3://' and split on first '/'
                if len(s3_parts) != 2:
                    logger.error(f"‚ùå Invalid S3 URI format: {stored_path}")
                    return None
                    
                bucket_name = s3_parts[0]
                s3_key = s3_parts[1]
                
                logger.info(f"üîó Generating presigned URL for S3 URI: bucket={bucket_name}, key={s3_key}")
                
                filename = os.path.basename(s3_key) or "download"
                safe_filename = filename.replace('"', '')
                content_type, _ = mimetypes.guess_type(safe_filename)

                params = {
                    "Bucket": bucket_name,
                    "Key": s3_key,
                    "ResponseContentDisposition": f'attachment; filename="{safe_filename}"',
                }

                if content_type:
                    params["ResponseContentType"] = content_type

                # Generate presigned URL
                url = self.s3_client.generate_presigned_url(
                    "get_object",
                    Params=params,
                    ExpiresIn=expires_in
                )
                
                logger.info(f"‚úÖ Successfully generated presigned URL for S3 URI: {stored_path}")
                return url
                
            # Handle relative paths (assume they're relative to current bucket)
            elif stored_path and not stored_path.startswith('/'):
                logger.info(f"üîó Generating presigned URL for relative path in bucket {self.bucket_name}: {stored_path}")
                
                filename = os.path.basename(stored_path) or "download"
                safe_filename = filename.replace('"', '')
                content_type, _ = mimetypes.guess_type(safe_filename)

                params = {
                    "Bucket": self.bucket_name,
                    "Key": stored_path,
                    "ResponseContentDisposition": f'attachment; filename="{safe_filename}"',
                }

                if content_type:
                    params["ResponseContentType"] = content_type

                url = self.s3_client.generate_presigned_url(
                    "get_object",
                    Params=params,
                    ExpiresIn=expires_in
                )
                
                logger.info(f"‚úÖ Successfully generated presigned URL for relative path: {stored_path}")
                return url
                
            else:
                logger.error(f"‚ùå Unsupported path format: {stored_path}")
                return None
                
        except ClientError as e:
            logger.error(f"‚ùå Failed to generate presigned URL: {e}")
            return None
        except Exception as e:
            logger.error(f"‚ùå Unknown error generating presigned URL: {e}")
            return None

    def list_files(
        self, prefix: str = "", max_keys: int = 1000, folder: str = "upload"
    ) -> list:
        """
        ÂàóÂá∫S3Â≠òÂÇ®Ê°∂‰∏≠ÁöÑÊñá‰ª∂

        Args:
            prefix: Êñá‰ª∂ÂâçÁºÄËøáÊª§
            max_keys: ÊúÄÂ§ßËøîÂõûÊñá‰ª∂Êï∞
            folder: Êñá‰ª∂Â§πÂêçÁß∞ (upload/results/exports)

        Returns:
            list: Êñá‰ª∂‰ø°ÊÅØÂàóË°®
        """
        folder_prefix = f"{folder}/" if not folder.endswith("/") else folder
        full_prefix = f"{folder_prefix}{prefix}"

        try:
            paginator = self.s3_client.get_paginator("list_objects_v2")
            pages = paginator.paginate(
                Bucket=self.bucket_name, Prefix=full_prefix, MaxKeys=max_keys
            )

            files = []
            for page in pages:
                if "Contents" in page:
                    for obj in page["Contents"]:
                        # ÁßªÈô§Êñá‰ª∂Â§πÂâçÁºÄÔºåÂè™ËøîÂõûÁõ∏ÂØπË∑ØÂæÑ
                        relative_key = (
                            obj["Key"][len(folder_prefix) :]
                            if obj["Key"].startswith(folder_prefix)
                            else obj["Key"]
                        )
                        files.append(
                            {
                                "key": relative_key,
                                "full_key": obj["Key"],
                                "size": obj["Size"],
                                "last_modified": obj["LastModified"],
                                "etag": obj["ETag"].strip('"'),
                            }
                        )

            logger.info(
                f"‚úÖ ÂàóÂá∫Êñá‰ª∂ÊàêÂäüÔºåÊñá‰ª∂Â§πÔºö{folder}ÔºåÂâçÁºÄÔºö{prefix}ÔºåÊï∞ÈáèÔºö{len(files)}"
            )
            return files
        except ClientError as e:
            logger.error(f"‚ùå ÂàóÂá∫Êñá‰ª∂Â§±Ë¥•Ôºö{e}")
            return []

    def upload_prompt(
        self,
        company_code: str,
        doc_type_code: str,
        prompt_content: str,
        filename: Optional[str] = None,
        metadata: Optional[dict] = None,
    ) -> Optional[str]:
        """
        ‰∏ä‰º†promptÊñá‰ª∂Âà∞S3

        Args:
            company_code: ÂÖ¨Âè∏‰ª£Á†Å
            doc_type_code: ÊñáÊ°£Á±ªÂûã‰ª£Á†Å
            prompt_content: promptÂÜÖÂÆπ
            filename: Êñá‰ª∂ÂêçÔºàÂèØÈÄâÔºåÈªòËÆ§‰∏∫prompt.txtÔºâ
            metadata: Êñá‰ª∂ÂÖÉÊï∞ÊçÆ

        Returns:
            Optional[str]: S3ÈîÆÂêçÔºåÂ§±Ë¥•Êó∂ËøîÂõûNone
        """
        try:
            if not filename:
                filename = "prompt.txt"

            # ÊûÑÂª∫S3ÈîÆÂêç
            key = f"{company_code}/{doc_type_code}/{filename}"
            full_key = f"{self.prompts_prefix}{key}"

            # ÂáÜÂ§áÂÖÉÊï∞ÊçÆ
            upload_metadata = {
                "company_code": company_code,
                "doc_type_code": doc_type_code,
                "file_type": "prompt",
                "uploaded_at": datetime.now().isoformat(),
            }
            if metadata:
                upload_metadata.update(metadata)

            # ‰∏ä‰º†ÂÜÖÂÆπ
            self.s3_client.put_object(
                Bucket=self.bucket_name,
                Key=full_key,
                Body=prompt_content.encode("utf-8"),
                ContentType="text/plain",
                ContentEncoding="utf-8",
                Metadata=upload_metadata,
            )

            logger.info(f"‚úÖ Prompt‰∏ä‰º†ÊàêÂäüÔºös3://{self.bucket_name}/{full_key}")
            return key

        except Exception as e:
            logger.error(f"‚ùå Prompt‰∏ä‰º†Â§±Ë¥•Ôºö{e}")
            return None

    def get_prompt(self, company_code: str, doc_type_code: str, filename: Optional[str] = None) -> Optional[str]:
        """
        ‰ªéS3Ëé∑ÂèñpromptÂÜÖÂÆπ

        Args:
            company_code: ÂÖ¨Âè∏‰ª£Á†Å
            doc_type_code: ÊñáÊ°£Á±ªÂûã‰ª£Á†Å
            filename: Êñá‰ª∂ÂêçÔºàÂèØÈÄâÔºåÈªòËÆ§‰∏∫prompt.txtÔºâ

        Returns:
            Optional[str]: promptÂÜÖÂÆπÔºåÂ§±Ë¥•Êó∂ËøîÂõûNone
        """
        try:
            if not filename:
                filename = "prompt.txt"

            key = f"{company_code}/{doc_type_code}/{filename}"
            full_key = f"{self.prompts_prefix}{key}"

            response = self.s3_client.get_object(Bucket=self.bucket_name, Key=full_key)
            content = response["Body"].read().decode("utf-8")

            logger.info(f"‚úÖ PromptËé∑ÂèñÊàêÂäüÔºös3://{self.bucket_name}/{full_key}")
            return content

        except ClientError as e:
            if e.response["Error"]["Code"] == "NoSuchKey":
                logger.warning(f"‚ö†Ô∏è PromptÊñá‰ª∂‰∏çÂ≠òÂú®Ôºös3://{self.bucket_name}/{self.prompts_prefix}{company_code}/{doc_type_code}/{filename or 'prompt.txt'}")
            else:
                logger.error(f"‚ùå PromptËé∑ÂèñÂ§±Ë¥•Ôºö{e}")
            return None
        except Exception as e:
            logger.error(f"‚ùå PromptËé∑ÂèñÊó∂ÂèëÁîüÊú™Áü•ÈîôËØØÔºö{e}")
            return None

    def get_file_by_key(self, s3_key: str) -> Optional[str]:
        """
        ÈÄöËøáÂÆåÊï¥S3 keyËé∑ÂèñÊñá‰ª∂ÂÜÖÂÆπÔºàÊñáÊú¨Êñá‰ª∂Ôºâ
        
        Args:
            s3_key: ÂÆåÊï¥ÁöÑS3 keyË∑ØÂæÑ
            
        Returns:
            Optional[str]: Êñá‰ª∂ÂÜÖÂÆπÔºåÂ§±Ë¥•Êó∂ËøîÂõûNone
        """
        try:
            response = self.s3_client.get_object(Bucket=self.bucket_name, Key=s3_key)
            content = response["Body"].read().decode("utf-8")
            
            logger.info(f"‚úÖ Êñá‰ª∂Ëé∑ÂèñÊàêÂäüÔºös3://{self.bucket_name}/{s3_key}")
            return content
            
        except ClientError as e:
            if e.response["Error"]["Code"] == "NoSuchKey":
                logger.warning(f"‚ö†Ô∏è Êñá‰ª∂‰∏çÂ≠òÂú®Ôºös3://{self.bucket_name}/{s3_key}")
            else:
                logger.error(f"‚ùå Êñá‰ª∂Ëé∑ÂèñÂ§±Ë¥•Ôºö{e}")
            return None
        except Exception as e:
            logger.error(f"‚ùå Êñá‰ª∂Ëé∑ÂèñÊó∂ÂèëÁîüÊú™Áü•ÈîôËØØÔºö{e}")
            return None

    def get_schema_by_key(self, s3_key: str) -> Optional[dict]:
        """
        ÈÄöËøáÂÆåÊï¥S3 keyËé∑ÂèñschemaÊñá‰ª∂ÂÜÖÂÆπ
        
        Args:
            s3_key: ÂÆåÊï¥ÁöÑS3 keyË∑ØÂæÑ
            
        Returns:
            Optional[dict]: schemaÊï∞ÊçÆÔºåÂ§±Ë¥•Êó∂ËøîÂõûNone
        """
        try:
            response = self.s3_client.get_object(Bucket=self.bucket_name, Key=s3_key)
            content = response["Body"].read().decode("utf-8")
            schema_data = json.loads(content)
            
            # Clean schema for Gemini API compatibility
            schema_data = clean_schema_for_gemini(schema_data)
            
            logger.info(f"‚úÖ SchemaËé∑ÂèñÊàêÂäüÔºös3://{self.bucket_name}/{s3_key}")
            return schema_data
            
        except ClientError as e:
            if e.response["Error"]["Code"] == "NoSuchKey":
                logger.warning(f"‚ö†Ô∏è SchemaÊñá‰ª∂‰∏çÂ≠òÂú®Ôºös3://{self.bucket_name}/{s3_key}")
            else:
                logger.error(f"‚ùå SchemaËé∑ÂèñÂ§±Ë¥•Ôºö{e}")
            return None
        except json.JSONDecodeError as e:
            logger.error(f"‚ùå Schema JSONËß£ÊûêÂ§±Ë¥•Ôºö{e}")
            return None
        except Exception as e:
            logger.error(f"‚ùå SchemaËé∑ÂèñÊó∂ÂèëÁîüÊú™Áü•ÈîôËØØÔºö{e}")
            return None

    def upload_schema(
        self,
        company_code: str,
        doc_type_code: str,
        schema_data: dict,
        filename: Optional[str] = None,
        metadata: Optional[dict] = None,
    ) -> Optional[str]:
        """
        ‰∏ä‰º†schemaÊñá‰ª∂Âà∞S3

        Args:
            company_code: ÂÖ¨Âè∏‰ª£Á†Å
            doc_type_code: ÊñáÊ°£Á±ªÂûã‰ª£Á†Å
            schema_data: schema JSONÊï∞ÊçÆ
            filename: Êñá‰ª∂ÂêçÔºàÂèØÈÄâÔºåÈªòËÆ§‰∏∫schema.jsonÔºâ
            metadata: Êñá‰ª∂ÂÖÉÊï∞ÊçÆ

        Returns:
            Optional[str]: S3ÈîÆÂêçÔºåÂ§±Ë¥•Êó∂ËøîÂõûNone
        """
        try:
            if not filename:
                filename = "schema.json"

            # È™åËØÅschemaÊ†ºÂºè
            if not isinstance(schema_data, dict):
                raise ValueError("Schema data must be a dictionary")

            # ÊûÑÂª∫S3ÈîÆÂêç
            key = f"{company_code}/{doc_type_code}/{filename}"
            full_key = f"{self.schemas_prefix}{key}"

            # ÂáÜÂ§áÂÖÉÊï∞ÊçÆ
            upload_metadata = {
                "company_code": company_code,
                "doc_type_code": doc_type_code,
                "file_type": "schema",
                "uploaded_at": datetime.now().isoformat(),
            }
            if metadata:
                upload_metadata.update(metadata)

            # ËΩ¨Êç¢‰∏∫JSONÂ≠óÁ¨¶‰∏≤
            schema_content = json.dumps(schema_data, ensure_ascii=False, indent=2)

            # ‰∏ä‰º†ÂÜÖÂÆπ
            self.s3_client.put_object(
                Bucket=self.bucket_name,
                Key=full_key,
                Body=schema_content.encode("utf-8"),
                ContentType="application/json",
                ContentEncoding="utf-8",
                Metadata=upload_metadata,
            )

            logger.info(f"‚úÖ Schema‰∏ä‰º†ÊàêÂäüÔºös3://{self.bucket_name}/{full_key}")
            return key

        except Exception as e:
            logger.error(f"‚ùå Schema‰∏ä‰º†Â§±Ë¥•Ôºö{e}")
            return None

    def get_schema(self, company_code: str, doc_type_code: str, filename: Optional[str] = None) -> Optional[dict]:
        """
        ‰ªéS3Ëé∑ÂèñschemaÊï∞ÊçÆ

        Args:
            company_code: ÂÖ¨Âè∏‰ª£Á†Å
            doc_type_code: ÊñáÊ°£Á±ªÂûã‰ª£Á†Å
            filename: Êñá‰ª∂ÂêçÔºàÂèØÈÄâÔºåÈªòËÆ§‰∏∫schema.jsonÔºâ

        Returns:
            Optional[dict]: schemaÊï∞ÊçÆÔºåÂ§±Ë¥•Êó∂ËøîÂõûNone
        """
        try:
            if not filename:
                filename = "schema.json"

            key = f"{company_code}/{doc_type_code}/{filename}"
            full_key = f"{self.schemas_prefix}{key}"

            response = self.s3_client.get_object(Bucket=self.bucket_name, Key=full_key)
            content = response["Body"].read().decode("utf-8")
            schema_data = json.loads(content)
            
            # Clean schema for Gemini API compatibility
            schema_data = clean_schema_for_gemini(schema_data)

            logger.info(f"‚úÖ SchemaËé∑ÂèñÊàêÂäüÔºös3://{self.bucket_name}/{full_key}")
            return schema_data

        except ClientError as e:
            if e.response["Error"]["Code"] == "NoSuchKey":
                logger.warning(f"‚ö†Ô∏è SchemaÊñá‰ª∂‰∏çÂ≠òÂú®Ôºös3://{self.bucket_name}/{self.schemas_prefix}{company_code}/{doc_type_code}/{filename or 'schema.json'}")
            else:
                logger.error(f"‚ùå SchemaËé∑ÂèñÂ§±Ë¥•Ôºö{e}")
            return None
        except json.JSONDecodeError as e:
            logger.error(f"‚ùå Schema JSONËß£ÊûêÂ§±Ë¥•Ôºö{e}")
            return None
        except Exception as e:
            logger.error(f"‚ùå SchemaËé∑ÂèñÊó∂ÂèëÁîüÊú™Áü•ÈîôËØØÔºö{e}")
            return None

    def download_prompt_raw(self, company_code: str, doc_type_code: str, filename: Optional[str] = None) -> Optional[bytes]:
        """
        ‰ªéS3‰∏ãËΩΩpromptÊñá‰ª∂ÁöÑÂéüÂßãÂÜÖÂÆπÔºàÁî®‰∫éÊñá‰ª∂‰∏ãËΩΩÔºâ

        Args:
            company_code: ÂÖ¨Âè∏‰ª£Á†Å
            doc_type_code: ÊñáÊ°£Á±ªÂûã‰ª£Á†Å
            filename: Êñá‰ª∂ÂêçÔºàÂèØÈÄâÔºåÈªòËÆ§‰∏∫prompt.txtÔºâ

        Returns:
            Optional[bytes]: Êñá‰ª∂ÂéüÂßãÂÜÖÂÆπÔºåÂ§±Ë¥•Êó∂ËøîÂõûNone
        """
        try:
            if not filename:
                filename = "prompt.txt"

            key = f"{company_code}/{doc_type_code}/{filename}"
            full_key = f"{self.prompts_prefix}{key}"

            response = self.s3_client.get_object(Bucket=self.bucket_name, Key=full_key)
            content = response["Body"].read()

            logger.info(f"‚úÖ PromptÂéüÂßãÂÜÖÂÆπ‰∏ãËΩΩÊàêÂäüÔºös3://{self.bucket_name}/{full_key}")
            return content

        except ClientError as e:
            if e.response["Error"]["Code"] == "NoSuchKey":
                logger.warning(f"‚ö†Ô∏è PromptÊñá‰ª∂‰∏çÂ≠òÂú®Ôºös3://{self.bucket_name}/{self.prompts_prefix}{company_code}/{doc_type_code}/{filename or 'prompt.txt'}")
            else:
                logger.error(f"‚ùå PromptÂéüÂßãÂÜÖÂÆπ‰∏ãËΩΩÂ§±Ë¥•Ôºö{e}")
            return None
        except Exception as e:
            logger.error(f"‚ùå PromptÂéüÂßãÂÜÖÂÆπ‰∏ãËΩΩÊó∂ÂèëÁîüÊú™Áü•ÈîôËØØÔºö{e}")
            return None

    def download_schema_raw(self, company_code: str, doc_type_code: str, filename: Optional[str] = None) -> Optional[bytes]:
        """
        ‰ªéS3‰∏ãËΩΩschemaÊñá‰ª∂ÁöÑÂéüÂßãÂÜÖÂÆπÔºàÁî®‰∫éÊñá‰ª∂‰∏ãËΩΩÔºâ

        Args:
            company_code: ÂÖ¨Âè∏‰ª£Á†Å
            doc_type_code: ÊñáÊ°£Á±ªÂûã‰ª£Á†Å
            filename: Êñá‰ª∂ÂêçÔºàÂèØÈÄâÔºåÈªòËÆ§‰∏∫schema.jsonÔºâ

        Returns:
            Optional[bytes]: Êñá‰ª∂ÂéüÂßãÂÜÖÂÆπÔºåÂ§±Ë¥•Êó∂ËøîÂõûNone
        """
        try:
            if not filename:
                filename = "schema.json"

            key = f"{company_code}/{doc_type_code}/{filename}"
            full_key = f"{self.schemas_prefix}{key}"

            response = self.s3_client.get_object(Bucket=self.bucket_name, Key=full_key)
            content = response["Body"].read()

            logger.info(f"‚úÖ SchemaÂéüÂßãÂÜÖÂÆπ‰∏ãËΩΩÊàêÂäüÔºös3://{self.bucket_name}/{full_key}")
            return content

        except ClientError as e:
            if e.response["Error"]["Code"] == "NoSuchKey":
                logger.warning(f"‚ö†Ô∏è SchemaÊñá‰ª∂‰∏çÂ≠òÂú®Ôºös3://{self.bucket_name}/{self.schemas_prefix}{company_code}/{doc_type_code}/{filename or 'schema.json'}")
            else:
                logger.error(f"‚ùå SchemaÂéüÂßãÂÜÖÂÆπ‰∏ãËΩΩÂ§±Ë¥•Ôºö{e}")
            return None
        except Exception as e:
            logger.error(f"‚ùå SchemaÂéüÂßãÂÜÖÂÆπ‰∏ãËΩΩÊó∂ÂèëÁîüÊú™Áü•ÈîôËØØÔºö{e}")
            return None

    def list_prompts(self, company_code: Optional[str] = None, doc_type_code: Optional[str] = None) -> list:
        """
        ÂàóÂá∫promptÊñá‰ª∂

        Args:
            company_code: ÂÖ¨Âè∏‰ª£Á†ÅÔºàÂèØÈÄâÔºâ
            doc_type_code: ÊñáÊ°£Á±ªÂûã‰ª£Á†ÅÔºàÂèØÈÄâÔºâ

        Returns:
            list: promptÊñá‰ª∂‰ø°ÊÅØÂàóË°®
        """
        try:
            # ÊûÑÂª∫ÂâçÁºÄ
            prefix = ""
            if company_code and doc_type_code:
                prefix = f"{company_code}/{doc_type_code}/"
            elif company_code:
                prefix = f"{company_code}/"

            return self.list_files(prefix=prefix, folder="prompts")

        except Exception as e:
            logger.error(f"‚ùå ÂàóÂá∫promptÊñá‰ª∂Â§±Ë¥•Ôºö{e}")
            return []

    def list_schemas(self, company_code: Optional[str] = None, doc_type_code: Optional[str] = None) -> list:
        """
        ÂàóÂá∫schemaÊñá‰ª∂

        Args:
            company_code: ÂÖ¨Âè∏‰ª£Á†ÅÔºàÂèØÈÄâÔºâ
            doc_type_code: ÊñáÊ°£Á±ªÂûã‰ª£Á†ÅÔºàÂèØÈÄâÔºâ

        Returns:
            list: schemaÊñá‰ª∂‰ø°ÊÅØÂàóË°®
        """
        try:
            # ÊûÑÂª∫ÂâçÁºÄ
            prefix = ""
            if company_code and doc_type_code:
                prefix = f"{company_code}/{doc_type_code}/"
            elif company_code:
                prefix = f"{company_code}/"

            return self.list_files(prefix=prefix, folder="schemas")

        except Exception as e:
            logger.error(f"‚ùå ÂàóÂá∫schemaÊñá‰ª∂Â§±Ë¥•Ôºö{e}")
            return []

    def delete_prompt(self, company_code: str, doc_type_code: str, filename: Optional[str] = None) -> bool:
        """
        Âà†Èô§promptÊñá‰ª∂

        Args:
            company_code: ÂÖ¨Âè∏‰ª£Á†Å
            doc_type_code: ÊñáÊ°£Á±ªÂûã‰ª£Á†Å
            filename: Êñá‰ª∂ÂêçÔºàÂèØÈÄâÔºåÈªòËÆ§‰∏∫prompt.txtÔºâ

        Returns:
            bool: Âà†Èô§ÊòØÂê¶ÊàêÂäü
        """
        try:
            if not filename:
                filename = "prompt.txt"

            key = f"{company_code}/{doc_type_code}/{filename}"
            return self.delete_file(key, folder="prompts")

        except Exception as e:
            logger.error(f"‚ùå Âà†Èô§promptÊñá‰ª∂Â§±Ë¥•Ôºö{e}")
            return False

    def delete_schema(self, company_code: str, doc_type_code: str, filename: Optional[str] = None) -> bool:
        """
        Âà†Èô§schemaÊñá‰ª∂

        Args:
            company_code: ÂÖ¨Âè∏‰ª£Á†Å
            doc_type_code: ÊñáÊ°£Á±ªÂûã‰ª£Á†Å
            filename: Êñá‰ª∂ÂêçÔºàÂèØÈÄâÔºåÈªòËÆ§‰∏∫schema.jsonÔºâ

        Returns:
            bool: Âà†Èô§ÊòØÂê¶ÊàêÂäü
        """
        try:
            if not filename:
                filename = "schema.json"

            key = f"{company_code}/{doc_type_code}/{filename}"
            return self.delete_file(key, folder="schemas")

        except Exception as e:
            logger.error(f"‚ùå Âà†Èô§schemaÊñá‰ª∂Â§±Ë¥•Ôºö{e}")
            return False

    def prompt_exists(self, company_code: str, doc_type_code: str, filename: Optional[str] = None) -> bool:
        """
        Ê£ÄÊü•promptÊñá‰ª∂ÊòØÂê¶Â≠òÂú®

        Args:
            company_code: ÂÖ¨Âè∏‰ª£Á†Å
            doc_type_code: ÊñáÊ°£Á±ªÂûã‰ª£Á†Å
            filename: Êñá‰ª∂ÂêçÔºàÂèØÈÄâÔºåÈªòËÆ§‰∏∫prompt.txtÔºâ

        Returns:
            bool: Êñá‰ª∂ÊòØÂê¶Â≠òÂú®
        """
        try:
            if not filename:
                filename = "prompt.txt"

            key = f"{company_code}/{doc_type_code}/{filename}"
            return self.file_exists(key, folder="prompts")

        except Exception as e:
            logger.error(f"‚ùå Ê£ÄÊü•promptÊñá‰ª∂Â≠òÂú®Êó∂Âá∫ÈîôÔºö{e}")
            return False

    def schema_exists(self, company_code: str, doc_type_code: str, filename: Optional[str] = None) -> bool:
        """
        Ê£ÄÊü•schemaÊñá‰ª∂ÊòØÂê¶Â≠òÂú®

        Args:
            company_code: ÂÖ¨Âè∏‰ª£Á†Å
            doc_type_code: ÊñáÊ°£Á±ªÂûã‰ª£Á†Å
            filename: Êñá‰ª∂ÂêçÔºàÂèØÈÄâÔºåÈªòËÆ§‰∏∫schema.jsonÔºâ

        Returns:
            bool: Êñá‰ª∂ÊòØÂê¶Â≠òÂú®
        """
        try:
            if not filename:
                filename = "schema.json"

            key = f"{company_code}/{doc_type_code}/{filename}"
            return self.file_exists(key, folder="schemas")

        except Exception as e:
            logger.error(f"‚ùå Ê£ÄÊü•schemaÊñá‰ª∂Â≠òÂú®Êó∂Âá∫ÈîôÔºö{e}")
            return False

    @staticmethod
    def generate_file_key(
        company_code: str,
        doc_type_code: str,
        filename: str,
        job_id: Optional[int] = None,
    ) -> str:
        """
        ÁîüÊàêÊ†áÂáÜÂåñÁöÑS3Êñá‰ª∂ÈîÆÂêç

        Args:
            company_code: ÂÖ¨Âè∏‰ª£Á†Å
            doc_type_code: ÊñáÊ°£Á±ªÂûã‰ª£Á†Å
            filename: ÂéüÂßãÊñá‰ª∂Âêç
            job_id: ‰ªªÂä°IDÔºàÂèØÈÄâÔºâ

        Returns:
            str: S3Êñá‰ª∂ÈîÆÂêç
        """
        # Ê∏ÖÁêÜÊñá‰ª∂Âêç‰∏≠ÁöÑÁâπÊÆäÂ≠óÁ¨¶
        safe_filename = "".join(
            c for c in filename if c.isalnum() or c in (" ", "-", "_", ".")
        ).rstrip()

        # ÊûÑÂª∫Êñá‰ª∂Ë∑ØÂæÑ
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        unique_id = str(uuid.uuid4())[:8]

        if job_id:
            key = f"uploads/{company_code}/{doc_type_code}/jobs/{job_id}/{timestamp}_{unique_id}_{safe_filename}"
        else:
            key = f"uploads/{company_code}/{doc_type_code}/{timestamp}_{unique_id}_{safe_filename}"

        return key

    def get_health_status(self) -> dict:
        """Ëé∑ÂèñS3Â≠òÂÇ®ÁöÑÂÅ•Â∫∑Áä∂ÊÄÅ"""
        try:
            # ÊµãËØïÂ≠òÂÇ®Ê°∂ËøûÊé•
            self.s3_client.head_bucket(Bucket=self.bucket_name)

            # Ëé∑ÂèñÂ≠òÂÇ®Ê°∂‰ø°ÊÅØ
            location = self.s3_client.get_bucket_location(Bucket=self.bucket_name)

            return {
                "status": "healthy",
                "bucket": self.bucket_name,
                "region": location.get("LocationConstraint", "us-east-1"),
                "accessible": True,
                "folders": ["upload", "results", "exports", "prompts", "schemas"],
            }
        except Exception as e:
            return {
                "status": "unhealthy",
                "bucket": self.bucket_name,
                "region": self.region,
                "accessible": False,
                "error": str(e),
            }

    def list_awb_invoices_for_month(self, month: str, debug: bool = False) -> list:
        """List invoice PDFs for given month from canonical and fallback S3 prefixes.

        Args:
            month: Month in YYYY-MM format (e.g., "2025-10")
            debug: If True, return sample keys for diagnostics

        Returns:
            list: List of dictionaries with keys: key, full_key, size, last_modified, prefix_source
        """
        try:
            year, mm = month.split('-')
            yyyymm = f"{year}{mm}"

            # Canonical and fallback prefixes for AWB invoices
            # Supports both YYYY/MM/ (original) and YYYYMM/ (new format)
            prefixes = [
                # New format: YYYYMM/ (without upload/ prefix, since S3StorageManager adds it)
                f"onedrive/airway-bills/{yyyymm}/",
                f"upload/onedrive/airway-bills/{yyyymm}/",
                # Original format: YYYY/MM/ (for backward compatibility)
                f"upload/onedrive/airway-bills/{year}/{mm}/",
                f"uploads/onedrive/airway-bills/{year}/{mm}/",
                f"upload/upload/onedrive/airway-bills/{year}/{mm}/",
            ]

            # Get minimum file size from env (default 10KB)
            min_size = int(os.getenv("AWB_S3_MIN_FILE_SIZE_BYTES", "10240"))

            all_files = []
            prefix_stats = {}  # Track statistics per prefix for diagnostics

            for prefix in prefixes:
                logger.info(f"üîç Scanning S3 prefix for invoices: bucket={self.bucket_name}, prefix={prefix}")
                prefix_stats[prefix] = {
                    "total_objects": 0,
                    "pdf_files": 0,
                    "skipped_small": 0,
                    "added": 0,
                    "sample_keys": []
                }

                try:
                    paginator = self.s3_client.get_paginator("list_objects_v2")
                    pages = paginator.paginate(Bucket=self.bucket_name, Prefix=prefix)

                    for page in pages:
                        if "Contents" not in page:
                            continue

                        prefix_stats[prefix]["total_objects"] += len(page["Contents"])

                        for obj in page["Contents"]:
                            # Filter for PDF files only
                            if not obj["Key"].lower().endswith('.pdf'):
                                continue

                            prefix_stats[prefix]["pdf_files"] += 1

                            # Filter by minimum size
                            if obj["Size"] < min_size:
                                prefix_stats[prefix]["skipped_small"] += 1
                                logger.debug(f"‚äò Skipping {obj['Key']} - size {obj['Size']} bytes < {min_size} bytes (min_size)")
                                continue

                            # Track sample keys for debugging
                            if len(prefix_stats[prefix]["sample_keys"]) < 5:
                                prefix_stats[prefix]["sample_keys"].append({
                                    "key": os.path.basename(obj["Key"]),
                                    "size": obj["Size"]
                                })

                            prefix_stats[prefix]["added"] += 1

                            all_files.append({
                                "key": os.path.basename(obj["Key"]),
                                "full_key": obj["Key"],
                                "size": obj["Size"],
                                "last_modified": obj["LastModified"],
                                "prefix_source": prefix,
                            })

                except ClientError as e:
                    if e.response["Error"]["Code"] != "NoSuchKey":
                        logger.warning(f"‚ö†Ô∏è Error scanning prefix {prefix}: {e}")

            # Log prefix statistics for diagnostics
            for prefix, stats in prefix_stats.items():
                if stats["total_objects"] > 0 or stats["pdf_files"] > 0:
                    logger.info(f"üìä Prefix stats [{prefix}]: {stats['total_objects']} objects, "
                              f"{stats['pdf_files']} PDFs, {stats['skipped_small']} skipped (size < {min_size}), "
                              f"{stats['added']} added")
                    if stats["sample_keys"]:
                        for sample in stats["sample_keys"]:
                            logger.debug(f"   Sample: {sample['key']} ({sample['size']} bytes)")

            # Deduplicate by filename, prefer latest LastModified
            seen = {}
            for f in all_files:
                name = f["key"]
                if name not in seen or f["last_modified"] > seen[name]["last_modified"]:
                    seen[name] = f

            result = list(seen.values())
            logger.info(f"‚úÖ Found {len(result)} unique invoice PDFs for month {month} (min_size_threshold={min_size} bytes)")

            # In debug mode, return statistics along with results
            if debug and result:
                logger.debug(f"üîç Debug mode: Returning {len(result)} results with statistics")
                for item in result[:3]:  # Log first 3 for debug
                    logger.debug(f"   - {item['key']} ({item['size']} bytes) from {item['prefix_source']}")

            return result

        except ValueError:
            logger.error(f"‚ùå Invalid month format: {month}. Expected YYYY-MM")
            return []
        except Exception as e:
            logger.error(f"‚ùå Error listing AWB invoices for {month}: {e}")
            return []

    def list_awb_objects_for_month_raw(self, month: str) -> list:
        """List ALL AWS objects for given month from canonical and fallback S3 prefixes.

        This is used for reconciliation - does NOT filter by file size, returns all objects.

        Args:
            month: Month in YYYY-MM format (e.g., "2025-10")

        Returns:
            list: List of dictionaries with keys: key, full_key, size, last_modified, prefix_source
        """
        try:
            year, mm = month.split('-')
            yyyymm = f"{year}{mm}"

            # Same prefixes as list_awb_invoices_for_month
            prefixes = [
                f"onedrive/airway-bills/{yyyymm}/",
                f"upload/onedrive/airway-bills/{yyyymm}/",
                f"upload/onedrive/airway-bills/{year}/{mm}/",
                f"uploads/onedrive/airway-bills/{year}/{mm}/",
                f"upload/upload/onedrive/airway-bills/{year}/{mm}/",
            ]

            all_files = []
            prefix_stats = {}

            for prefix in prefixes:
                logger.info(f"üîç Scanning S3 prefix (raw): bucket={self.bucket_name}, prefix={prefix}")
                prefix_stats[prefix] = {
                    "total_objects": 0,
                    "pdf_files": 0,
                }

                try:
                    paginator = self.s3_client.get_paginator("list_objects_v2")
                    pages = paginator.paginate(Bucket=self.bucket_name, Prefix=prefix)

                    for page in pages:
                        if "Contents" not in page:
                            continue

                        prefix_stats[prefix]["total_objects"] += len(page["Contents"])

                        for obj in page["Contents"]:
                            # Filter for PDF files only (no size filter for raw listing)
                            if not obj["Key"].lower().endswith('.pdf'):
                                continue

                            prefix_stats[prefix]["pdf_files"] += 1

                            all_files.append({
                                "key": os.path.basename(obj["Key"]),
                                "full_key": obj["Key"],
                                "size": obj["Size"],
                                "last_modified": obj["LastModified"],
                                "prefix_source": prefix,
                            })

                except ClientError as e:
                    if e.response["Error"]["Code"] != "NoSuchKey":
                        logger.warning(f"‚ö†Ô∏è Error scanning prefix {prefix}: {e}")

            # Log prefix statistics
            for prefix, stats in prefix_stats.items():
                if stats["total_objects"] > 0 or stats["pdf_files"] > 0:
                    logger.info(f"üìä Prefix stats (raw) [{prefix}]: {stats['total_objects']} objects, {stats['pdf_files']} PDFs")

            logger.info(f"‚úÖ Found {len(all_files)} total PDF objects for month {month} (no size filter)")
            return all_files

        except ValueError:
            logger.error(f"‚ùå Invalid month format: {month}. Expected YYYY-MM")
            return []
        except Exception as e:
            logger.error(f"‚ùå Error listing raw AWB objects for {month}: {e}")
            return []

    def index_awb_month_by_name(self, month: str) -> dict:
        """Create a filename-based index of all S3 objects for given month.

        Returns a dictionary where:
        - key: filename
        - value: dict with {largest: {...}, any_tiny: bool, all_records: [...]}

        Used for fast lookup to determine if file exists and if it's undersized.

        Args:
            month: Month in YYYY-MM format (e.g., "2025-10")

        Returns:
            dict: Filename-indexed objects
        """
        try:
            min_size = int(os.getenv("AWB_S3_MIN_FILE_SIZE_BYTES", "10240"))

            # Get all raw objects for the month
            all_objects = self.list_awb_objects_for_month_raw(month)

            # Build index by filename
            index = {}
            for obj in all_objects:
                name = obj["key"]

                if name not in index:
                    index[name] = {
                        "largest": obj,
                        "any_tiny": obj["size"] < min_size,
                        "all_records": [obj],
                    }
                else:
                    # Update to largest by size if needed
                    if obj["size"] > index[name]["largest"]["size"]:
                        index[name]["largest"] = obj
                    # Mark if ANY version is tiny
                    if obj["size"] < min_size:
                        index[name]["any_tiny"] = True
                    # Keep all records
                    index[name]["all_records"].append(obj)

            logger.info(f"‚úÖ Created filename index for {month}: {len(index)} unique filenames")
            return index

        except Exception as e:
            logger.error(f"‚ùå Error creating AWB month index for {month}: {e}")
            return {}

    # ========================================
    # NEW ID-BASED METHODS FOR FILE MANAGEMENT
    # ========================================
    
    def upload_company_file(self, 
                          company_id: int,
                          file_type: FileType,
                          content: Union[str, bytes],
                          filename: str,
                          doc_type_id: Optional[int] = None,
                          config_id: Optional[int] = None,
                          job_id: Optional[str] = None,
                          export_id: Optional[str] = None,
                          metadata: Optional[dict] = None) -> Optional[str]:
        """
        Upload file using new ID-based path structure
        
        Args:
            company_id: Company ID
            file_type: Type of file (prompt, schema, upload, result, export)
            content: File content (string or bytes)
            filename: Original filename
            doc_type_id: Document type ID (for prompts/schemas)
            config_id: Configuration ID (for prompts/schemas)
            job_id: Job ID (for uploads/results)
            export_id: Export ID (for exports)
            metadata: Additional metadata
            
        Returns:
            Optional[str]: S3 key if successful, None if failed
        """
        try:
            # Generate ID-based path
            s3_path = self.company_file_manager.get_company_file_path(
                company_id=company_id,
                file_type=file_type,
                filename=filename,
                doc_type_id=doc_type_id,
                config_id=config_id,
                job_id=job_id,
                export_id=export_id
            )
            
            # Prepare metadata
            upload_metadata = {
                "company_id": str(company_id),
                "file_type": file_type.value,
                "uploaded_at": datetime.now().isoformat(),
            }
            
            # Add type-specific metadata
            if doc_type_id:
                upload_metadata["doc_type_id"] = str(doc_type_id)
            if config_id:
                upload_metadata["config_id"] = str(config_id)
            if job_id:
                upload_metadata["job_id"] = job_id
            if export_id:
                upload_metadata["export_id"] = export_id
                
            if metadata:
                upload_metadata.update(metadata)
            
            # Determine content type
            content_type = "text/plain"
            if file_type == FileType.SCHEMA or filename.endswith('.json'):
                content_type = "application/json"
            elif filename.endswith(('.pdf', '.png', '.jpg', '.jpeg')):
                content_type, _ = mimetypes.guess_type(filename)
                if not content_type:
                    content_type = "application/octet-stream"
            
            # Convert content to bytes if needed
            if isinstance(content, str):
                body = content.encode('utf-8')
                encoding = "utf-8"
            else:
                body = content
                encoding = None
                
            # Upload to S3
            put_args = {
                "Bucket": self.bucket_name,
                "Key": s3_path,
                "Body": body,
                "ContentType": content_type,
                "Metadata": upload_metadata,
            }
            
            if encoding:
                put_args["ContentEncoding"] = encoding
            
            self.s3_client.put_object(**put_args)
            
            logger.info(f"‚úÖ ID-based file upload successful: s3://{self.bucket_name}/{s3_path}")
            return s3_path
            
        except Exception as e:
            logger.error(f"‚ùå ID-based file upload failed: {e}")
            return None
    
    def download_company_file(self,
                            company_id: int,
                            file_type: FileType,
                            filename: str,
                            doc_type_id: Optional[int] = None,
                            config_id: Optional[int] = None,
                            job_id: Optional[str] = None,
                            export_id: Optional[str] = None) -> Optional[bytes]:
        """
        Download file using new ID-based path structure
        
        Args:
            company_id: Company ID
            file_type: Type of file
            filename: Original filename
            doc_type_id: Document type ID (for prompts/schemas)
            config_id: Configuration ID (for prompts/schemas)
            job_id: Job ID (for uploads/results)
            export_id: Export ID (for exports)
            
        Returns:
            Optional[bytes]: File content if found, None if not found
        """
        try:
            # Generate ID-based path
            s3_path = self.company_file_manager.get_company_file_path(
                company_id=company_id,
                file_type=file_type,
                filename=filename,
                doc_type_id=doc_type_id,
                config_id=config_id,
                job_id=job_id,
                export_id=export_id
            )
            
            # Download from S3
            response = self.s3_client.get_object(Bucket=self.bucket_name, Key=s3_path)
            content = response["Body"].read()
            
            logger.info(f"‚úÖ ID-based file download successful: s3://{self.bucket_name}/{s3_path}")
            return content
            
        except ClientError as e:
            if e.response['Error']['Code'] == 'NoSuchKey':
                logger.warning(f"‚ö†Ô∏è ID-based file not found: s3://{self.bucket_name}/{s3_path}")
            else:
                logger.error(f"‚ùå ID-based file download failed: {e}")
            return None
        except Exception as e:
            logger.error(f"‚ùå ID-based file download failed: {e}")
            return None
    
    def upload_prompt_by_id(self,
                          company_id: int,
                          doc_type_id: int,
                          config_id: int,
                          prompt_content: str,
                          filename: Optional[str] = None,
                          metadata: Optional[dict] = None) -> Optional[str]:
        """
        Upload prompt using clean ID-based path (convenience method)
        
        Args:
            company_id: Company ID
            doc_type_id: Document type ID
            config_id: Configuration ID
            prompt_content: Prompt content
            filename: Filename (should be original filename)
            metadata: Additional metadata
            
        Returns:
            Optional[str]: S3 key if successful
        """
        if not filename:
            filename = "prompt.txt"  # Use clean default filename
            
        return self.upload_company_file(
            company_id=company_id,
            file_type=FileType.PROMPT,
            content=prompt_content,
            filename=filename,
            doc_type_id=doc_type_id,
            config_id=config_id,
            metadata=metadata
        )
    
    def upload_schema_by_id(self,
                          company_id: int,
                          doc_type_id: int,
                          config_id: int,
                          schema_data: dict,
                          filename: Optional[str] = None,
                          metadata: Optional[dict] = None) -> Optional[str]:
        """
        Upload schema using clean ID-based path (convenience method)
        
        Args:
            company_id: Company ID
            doc_type_id: Document type ID
            config_id: Configuration ID
            schema_data: Schema data as dict
            filename: Filename (should be original filename)
            metadata: Additional metadata
            
        Returns:
            Optional[str]: S3 key if successful
        """
        import json
        
        if not filename:
            filename = "schema.json"  # Use clean default filename
            
        # Convert schema data to JSON string
        schema_content = json.dumps(schema_data, indent=2, ensure_ascii=False)
            
        return self.upload_company_file(
            company_id=company_id,
            file_type=FileType.SCHEMA,
            content=schema_content,
            filename=filename,
            doc_type_id=doc_type_id,
            config_id=config_id,
            metadata=metadata
        )
    
    def download_prompt_by_id(self,
                            company_id: int,
                            doc_type_id: int,
                            config_id: int,
                            filename: Optional[str] = None) -> Optional[str]:
        """
        Download prompt using clean ID-based path with fallbacks (convenience method)
        
        Returns:
            Optional[str]: Prompt content as string
        """
        if not filename:
            filename = "prompt.txt"  # Clean default filename
            
        # Try primary ID-based path
        content = self.download_company_file(
            company_id=company_id,
            file_type=FileType.PROMPT,
            filename=filename,
            doc_type_id=doc_type_id,
            config_id=config_id
        )
        
        # If not found, try temp path fallback
        if content is None:
            logger.info(f"üîÑ Primary path not found, trying temp path fallback for config {config_id}")
            temp_paths = [
                f"companies/{company_id}/prompts/{doc_type_id}/temp_{config_id}/{filename}",
                f"companies/{company_id}/prompts/{doc_type_id}/temp_{int(config_id) * 1000}/{filename}",
                # Try some common temp timestamp patterns
                f"companies/{company_id}/prompts/{doc_type_id}/temp_1758089852851/{filename}",
            ]
            
            for temp_path in temp_paths:
                logger.info(f"üîç Trying temp path: {temp_path}")
                try:
                    response = self.s3_client.get_object(Bucket=self.bucket_name, Key=temp_path)
                    content = response["Body"].read()
                    logger.info(f"‚úÖ Found prompt at temp path: {temp_path}")
                    break
                except ClientError as e:
                    if e.response['Error']['Code'] != 'NoSuchKey':
                        logger.warning(f"‚ö†Ô∏è S3 error on temp path {temp_path}: {e}")
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Error trying temp path {temp_path}: {e}")
        
        if content:
            return content.decode('utf-8')
        return None
    
    def download_schema_by_id(self,
                            company_id: int,
                            doc_type_id: int,
                            config_id: int,
                            filename: Optional[str] = None) -> Optional[dict]:
        """
        Download schema using clean ID-based path with fallbacks (convenience method)
        
        Returns:
            Optional[dict]: Schema data as dictionary
        """
        if not filename:
            filename = "schema.json"  # Clean default filename
            
        # Try primary ID-based path
        content = self.download_company_file(
            company_id=company_id,
            file_type=FileType.SCHEMA,
            filename=filename,
            doc_type_id=doc_type_id,
            config_id=config_id
        )
        
        # If not found, try temp path fallback
        if content is None:
            logger.info(f"üîÑ Primary schema path not found, trying temp path fallback for config {config_id}")
            temp_paths = [
                f"companies/{company_id}/schemas/{doc_type_id}/temp_{config_id}/{filename}",
                f"companies/{company_id}/schemas/{doc_type_id}/temp_{int(config_id) * 1000}/{filename}",
                # Try some common temp timestamp patterns
                f"companies/{company_id}/schemas/{doc_type_id}/temp_1758089852982/{filename}",
            ]
            
            for temp_path in temp_paths:
                logger.info(f"üîç Trying temp schema path: {temp_path}")
                try:
                    response = self.s3_client.get_object(Bucket=self.bucket_name, Key=temp_path)
                    content = response["Body"].read()
                    logger.info(f"‚úÖ Found schema at temp path: {temp_path}")
                    break
                except ClientError as e:
                    if e.response['Error']['Code'] != 'NoSuchKey':
                        logger.warning(f"‚ö†Ô∏è S3 error on temp schema path {temp_path}: {e}")
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Error trying temp schema path {temp_path}: {e}")
        
        if content:
            try:
                import json
                schema_data = json.loads(content.decode('utf-8'))
                
                # Clean schema for Gemini API compatibility
                schema_data = clean_schema_for_gemini(schema_data)
                
                return schema_data
            except json.JSONDecodeError as e:
                logger.error(f"‚ùå Failed to parse schema JSON: {e}")
                return None
        return None
    
    def download_file_by_stored_path(self, stored_path: str) -> Optional[bytes]:
        """
        Download file using stored database path (S3 URI or relative path)
        
        Args:
            stored_path: Full S3 URI (s3://bucket/key) or relative path within current bucket
            
        Returns:
            Optional[bytes]: File content if found, None if not found
        """
        try:
            if not stored_path:
                logger.warning("‚ö†Ô∏è Empty stored path provided")
                return None
            
            # Handle S3 URI format: s3://bucket/key/path/file.ext
            if stored_path.startswith('s3://'):
                # Parse S3 URI
                s3_parts = stored_path[5:].split('/', 1)  # Remove 's3://' and split on first '/'
                if len(s3_parts) != 2:
                    logger.error(f"‚ùå Invalid S3 URI format: {stored_path}")
                    return None
                    
                bucket_name = s3_parts[0]
                s3_key = s3_parts[1]
                
                logger.info(f"üì• Downloading from S3 URI: bucket={bucket_name}, key={s3_key}")

                # Validate bucket name matches current instance
                if bucket_name != self.bucket_name:
                    logger.warning(f"‚ö†Ô∏è Bucket mismatch: URI bucket={bucket_name}, current bucket={self.bucket_name}")

                # Direct S3 download
                response = self.s3_client.get_object(Bucket=bucket_name, Key=s3_key)
                content = response["Body"].read()

                logger.info(f"‚úÖ Successfully downloaded from S3 URI: {stored_path} (size: {len(content)} bytes)")
                return content
                
            # Handle relative paths (assume they're relative to current bucket)
            elif stored_path and not stored_path.startswith('/'):
                logger.info(f"üì• Downloading relative path from bucket {self.bucket_name}: {stored_path}")
                
                response = self.s3_client.get_object(Bucket=self.bucket_name, Key=stored_path)
                content = response["Body"].read()
                
                logger.info(f"‚úÖ Successfully downloaded relative path: {stored_path}")
                return content
                
            else:
                logger.error(f"‚ùå Unsupported path format: {stored_path}")
                return None
                
        except ClientError as e:
            if e.response['Error']['Code'] == 'NoSuchKey':
                logger.warning(f"‚ö†Ô∏è File not found at stored path: {stored_path}")
            else:
                logger.error(f"‚ùå S3 error downloading from stored path: {e}")
            return None
        except Exception as e:
            logger.error(f"‚ùå Failed to download from stored path '{stored_path}': {e}")
            return None


# ÂÖ®Â±ÄS3Â≠òÂÇ®ÁÆ°ÁêÜÂô®ÂÆû‰æã
_s3_manager = None


def get_s3_manager() -> Optional[S3StorageManager]:
    """Ëé∑ÂèñÂÖ®Â±ÄS3Â≠òÂÇ®ÁÆ°ÁêÜÂô®ÂÆû‰æã"""
    global _s3_manager

    if _s3_manager is None:
        try:
            # ‰ªéÁéØÂ¢ÉÂèòÈáèËé∑ÂèñS3ËÆæÁΩÆ
            bucket_name = os.getenv("S3_BUCKET_NAME")
            region = os.getenv("AWS_DEFAULT_REGION", "ap-southeast-1")

            if bucket_name:
                _s3_manager = S3StorageManager(bucket_name, region)
                _s3_manager.ensure_bucket_exists()
                logger.info(f"‚úÖ S3Â≠òÂÇ®ÁÆ°ÁêÜÂô®ÂàùÂßãÂåñÊàêÂäüÔºöbucket={bucket_name}")
            else:
                logger.warning("‚ö†Ô∏è Êú™ÈÖçÁΩÆS3Â≠òÂÇ®Ê°∂ÂêçÁß∞ÔºåÂ∞Ü‰ΩøÁî®Êú¨Âú∞Êñá‰ª∂Â≠òÂÇ®")

        except Exception as e:
            logger.error(f"‚ùå S3Â≠òÂÇ®ÁÆ°ÁêÜÂô®ÂàùÂßãÂåñÂ§±Ë¥•Ôºö{e}")
            _s3_manager = None

    return _s3_manager


def is_s3_enabled() -> bool:
    """Ê£ÄÊü•ÊòØÂê¶ÂêØÁî®‰∫ÜS3Â≠òÂÇ®"""
    return get_s3_manager() is not None
