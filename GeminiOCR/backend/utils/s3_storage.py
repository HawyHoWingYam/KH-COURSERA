import boto3
import os
import logging
from typing import Optional, BinaryIO, Union
from botocore.exceptions import ClientError
from datetime import datetime
import json
import uuid
import mimetypes

from .company_file_manager import CompanyFileManager, FileType

logger = logging.getLogger(__name__)


def clean_schema_for_gemini(schema):
    """
    Clean JSON schema for Gemini API compatibility by removing unsupported fields.
    
    Args:
        schema: The JSON schema dictionary
        
    Returns:
        Cleaned schema dictionary safe for Gemini API
    """
    if not isinstance(schema, dict):
        return schema
    
    # Fields that cause Gemini API errors
    problematic_fields = ["$schema", "$id", "$ref", "definitions", "patternProperties"]
    
    cleaned_schema = {}
    for key, value in schema.items():
        if key in problematic_fields:
            logger.info(f"Removing problematic schema field for Gemini compatibility: {key}")
            continue
            
        if isinstance(value, dict):
            # Recursively clean nested dictionaries
            cleaned_value = clean_schema_for_gemini(value)
            # Only add if the cleaned value is not empty
            if cleaned_value:
                cleaned_schema[key] = cleaned_value
        elif isinstance(value, list):
            cleaned_schema[key] = [
                clean_schema_for_gemini(item) if isinstance(item, dict) else item
                for item in value
            ]
        else:
            cleaned_schema[key] = value
    
    return cleaned_schema


class S3StorageManager:
    """AWS S3æ–‡ä»¶å­˜å‚¨ç®¡ç†å™¨ - ä½¿ç”¨å•å­˜å‚¨æ¡¶å¤šæ–‡ä»¶å¤¹ç»“æž„"""

    def __init__(self, bucket_name: str, region: str = "ap-southeast-1", enable_legacy_compatibility: bool = True):
        """
        åˆå§‹åŒ–S3å­˜å‚¨ç®¡ç†å™¨

        Args:
            bucket_name: S3å­˜å‚¨æ¡¶åç§°
            region: AWSåŒºåŸŸ
            enable_legacy_compatibility: æ˜¯å¦å¯ç”¨æ—§è·¯å¾„å…¼å®¹æ¨¡å¼
        """
        self.bucket_name = bucket_name
        self.region = region
        self.enable_legacy_compatibility = enable_legacy_compatibility
        
        # Initialize Company File Manager for ID-based paths
        self.company_file_manager = CompanyFileManager()
        
        # Legacy prefixes (kept for backward compatibility)
        self.upload_prefix = "upload/"
        self.results_prefix = "results/"
        self.exports_prefix = "exports/"
        self.prompts_prefix = "prompts/"
        self.schemas_prefix = "schemas/"
        
        self._s3_client = None
        self._s3_resource = None

    @property
    def s3_client(self):
        """å»¶è¿Ÿåˆå§‹åŒ–S3å®¢æˆ·ç«¯"""
        if self._s3_client is None:
            try:
                self._s3_client = boto3.client("s3", region_name=self.region)
                logger.info(f"âœ… S3å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸï¼ŒåŒºåŸŸï¼š{self.region}")
            except Exception as e:
                logger.error(f"âŒ S3å®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥ï¼š{e}")
                raise
        return self._s3_client

    @property
    def s3_resource(self):
        """å»¶è¿Ÿåˆå§‹åŒ–S3èµ„æº"""
        if self._s3_resource is None:
            try:
                self._s3_resource = boto3.resource("s3", region_name=self.region)
                logger.info(f"âœ… S3èµ„æºåˆå§‹åŒ–æˆåŠŸï¼ŒåŒºåŸŸï¼š{self.region}")
            except Exception as e:
                logger.error(f"âŒ S3èµ„æºåˆå§‹åŒ–å¤±è´¥ï¼š{e}")
                raise
        return self._s3_resource

    def ensure_bucket_exists(self) -> bool:
        """ç¡®ä¿S3å­˜å‚¨æ¡¶å­˜åœ¨"""
        try:
            self.s3_client.head_bucket(Bucket=self.bucket_name)
            logger.info(f"âœ… S3å­˜å‚¨æ¡¶å·²å­˜åœ¨ï¼š{self.bucket_name}")
            return True
        except ClientError as e:
            error_code = int(e.response["Error"]["Code"])

            if error_code == 404:
                # å­˜å‚¨æ¡¶ä¸å­˜åœ¨ï¼Œå°è¯•åˆ›å»º
                try:
                    if self.region == "us-east-1":
                        # us-east-1åŒºåŸŸåˆ›å»ºå­˜å‚¨æ¡¶çš„ç‰¹æ®Šå¤„ç†
                        self.s3_client.create_bucket(Bucket=self.bucket_name)
                    else:
                        self.s3_client.create_bucket(
                            Bucket=self.bucket_name,
                            CreateBucketConfiguration={
                                "LocationConstraint": self.region
                            },
                        )

                    logger.info(f"âœ… æˆåŠŸåˆ›å»ºS3å­˜å‚¨æ¡¶ï¼š{self.bucket_name}")
                    return True
                except ClientError as create_error:
                    logger.error(f"âŒ åˆ›å»ºS3å­˜å‚¨æ¡¶å¤±è´¥ï¼š{create_error}")
                    return False
            else:
                logger.error(f"âŒ æ£€æŸ¥S3å­˜å‚¨æ¡¶æ—¶å‡ºé”™ï¼š{e}")
                return False

    def upload_file(
        self,
        file_content: Union[BinaryIO, bytes],
        key: str,
        content_type: Optional[str] = None,
        metadata: Optional[dict] = None,
    ) -> bool:
        """
        ä¸Šä¼ æ–‡ä»¶åˆ°S3

        Args:
            file_content: æ–‡ä»¶å†…å®¹ï¼ˆæ–‡ä»¶å¯¹è±¡æˆ–å­—èŠ‚æ•°æ®ï¼‰
            key: S3ä¸­çš„æ–‡ä»¶é”®å
            content_type: æ–‡ä»¶çš„MIMEç±»åž‹
            metadata: æ–‡ä»¶å…ƒæ•°æ®

        Returns:
            bool: ä¸Šä¼ æ˜¯å¦æˆåŠŸ
        """
        try:
            # è‡ªåŠ¨æŽ¨æ–­content_type
            if content_type is None:
                content_type, _ = mimetypes.guess_type(key)
                if content_type is None:
                    content_type = "application/octet-stream"

            # å‡†å¤‡ä¸Šä¼ å‚æ•°
            upload_args = {
                "Bucket": self.bucket_name,
                "Key": f"{self.upload_prefix}{key}",
                "ContentType": content_type,
            }

            # æ·»åŠ å…ƒæ•°æ®
            if metadata:
                upload_args["Metadata"] = metadata

            # æ‰§è¡Œä¸Šä¼ 
            if hasattr(file_content, "read"):
                # æ–‡ä»¶å¯¹è±¡ - upload_fileobj éœ€è¦å•ç‹¬çš„å‚æ•°æ ¼å¼
                extra_args = {"ContentType": content_type}
                if metadata:
                    extra_args["Metadata"] = metadata

                self.s3_client.upload_fileobj(
                    file_content,
                    upload_args["Bucket"],
                    upload_args["Key"],
                    ExtraArgs=extra_args,
                )
            else:
                # å­—èŠ‚æ•°æ®
                self.s3_client.put_object(Body=file_content, **upload_args)

            logger.info(
                f"âœ… æ–‡ä»¶ä¸Šä¼ æˆåŠŸï¼šs3://{self.bucket_name}/{self.upload_prefix}{key}"
            )
            return True

        except ClientError as e:
            logger.error(f"âŒ æ–‡ä»¶ä¸Šä¼ å¤±è´¥ï¼š{e}")
            return False
        except Exception as e:
            logger.error(f"âŒ æ–‡ä»¶ä¸Šä¼ æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯ï¼š{e}")
            return False

    def save_json_result(
        self, key: str, data: dict, metadata: Optional[dict] = None
    ) -> bool:
        """
        ä¿å­˜JSONç»“æžœåˆ°resultsæ–‡ä»¶å¤¹

        Args:
            key: æ–‡ä»¶é”®å
            data: JSONæ•°æ®
            metadata: æ–‡ä»¶å…ƒæ•°æ®

        Returns:
            bool: ä¿å­˜æ˜¯å¦æˆåŠŸ
        """
        try:
            json_content = json.dumps(data, ensure_ascii=False, indent=2).encode(
                "utf-8"
            )

            upload_args = {
                "Bucket": self.bucket_name,
                "Key": f"{self.results_prefix}{key}",
                "Body": json_content,
                "ContentType": "application/json",
                "ContentEncoding": "utf-8",
            }

            if metadata:
                upload_args["Metadata"] = metadata

            self.s3_client.put_object(**upload_args)
            logger.info(
                f"âœ… JSONç»“æžœä¿å­˜æˆåŠŸï¼šs3://{self.bucket_name}/{self.results_prefix}{key}"
            )
            return True

        except Exception as e:
            logger.error(f"âŒ JSONç»“æžœä¿å­˜å¤±è´¥ï¼š{e}")
            return False

    def get_json_result(self, key: str) -> Optional[dict]:
        """
        ä»Žresultsæ–‡ä»¶å¤¹èŽ·å–JSONç»“æžœ

        Args:
            key: æ–‡ä»¶é”®å

        Returns:
            Optional[dict]: JSONæ•°æ®ï¼Œå¤±è´¥æ—¶è¿”å›žNone
        """
        try:
            response = self.s3_client.get_object(
                Bucket=self.bucket_name, Key=f"{self.results_prefix}{key}"
            )
            content = response["Body"].read().decode("utf-8")
            data = json.loads(content)
            logger.info(
                f"âœ… JSONç»“æžœèŽ·å–æˆåŠŸï¼šs3://{self.bucket_name}/{self.results_prefix}{key}"
            )
            return data
        except ClientError as e:
            if e.response["Error"]["Code"] == "NoSuchKey":
                logger.warning(
                    f"âš ï¸ JSONæ–‡ä»¶ä¸å­˜åœ¨ï¼šs3://{self.bucket_name}/{self.results_prefix}{key}"
                )
            else:
                logger.error(f"âŒ JSONç»“æžœèŽ·å–å¤±è´¥ï¼š{e}")
            return None
        except Exception as e:
            logger.error(f"âŒ JSONç»“æžœèŽ·å–æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯ï¼š{e}")
            return None

    def save_excel_export(
        self,
        key: str,
        file_content: Union[BinaryIO, bytes],
        metadata: Optional[dict] = None,
    ) -> bool:
        """
        ä¿å­˜Excelæ–‡ä»¶åˆ°exportsæ–‡ä»¶å¤¹

        Args:
            key: æ–‡ä»¶é”®å
            file_content: Excelæ–‡ä»¶å†…å®¹
            metadata: æ–‡ä»¶å…ƒæ•°æ®

        Returns:
            bool: ä¿å­˜æ˜¯å¦æˆåŠŸ
        """
        try:
            if hasattr(file_content, "read"):
                # æ–‡ä»¶å¯¹è±¡ - upload_fileobj éœ€è¦å•ç‹¬çš„å‚æ•°æ ¼å¼
                extra_args = {
                    "ContentType": "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                }
                if metadata:
                    extra_args["Metadata"] = metadata

                self.s3_client.upload_fileobj(
                    file_content,
                    self.bucket_name,
                    f"{self.exports_prefix}{key}",
                    ExtraArgs=extra_args,
                )
            else:
                upload_args = {
                    "Bucket": self.bucket_name,
                    "Key": f"{self.exports_prefix}{key}",
                    "Body": file_content,
                    "ContentType": "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                }
                if metadata:
                    upload_args["Metadata"] = metadata
                self.s3_client.put_object(**upload_args)

            logger.info(
                f"âœ… Excelæ–‡ä»¶ä¿å­˜æˆåŠŸï¼šs3://{self.bucket_name}/{self.exports_prefix}{key}"
            )
            return True

        except Exception as e:
            logger.error(f"âŒ Excelæ–‡ä»¶ä¿å­˜å¤±è´¥ï¼š{e}")
            return False

    def get_excel_export(self, key: str) -> Optional[bytes]:
        """
        ä»Žexportsæ–‡ä»¶å¤¹èŽ·å–Excelæ–‡ä»¶

        Args:
            key: æ–‡ä»¶é”®å

        Returns:
            Optional[bytes]: Excelæ–‡ä»¶å†…å®¹ï¼Œå¤±è´¥æ—¶è¿”å›žNone
        """
        try:
            response = self.s3_client.get_object(
                Bucket=self.bucket_name, Key=f"{self.exports_prefix}{key}"
            )
            content = response["Body"].read()
            logger.info(
                f"âœ… Excelæ–‡ä»¶èŽ·å–æˆåŠŸï¼šs3://{self.bucket_name}/{self.exports_prefix}{key}"
            )
            return content
        except ClientError as e:
            if e.response["Error"]["Code"] == "NoSuchKey":
                logger.warning(
                    f"âš ï¸ Excelæ–‡ä»¶ä¸å­˜åœ¨ï¼šs3://{self.bucket_name}/{self.exports_prefix}{key}"
                )
            else:
                logger.error(f"âŒ Excelæ–‡ä»¶èŽ·å–å¤±è´¥ï¼š{e}")
            return None
        except Exception as e:
            logger.error(f"âŒ Excelæ–‡ä»¶èŽ·å–æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯ï¼š{e}")
            return None

    def download_file(self, key: str, folder: str = "upload") -> Optional[bytes]:
        """
        ä»ŽS3ä¸‹è½½æ–‡ä»¶

        Args:
            key: æ–‡ä»¶é”®å
            folder: æ–‡ä»¶å¤¹åç§° (upload/results/exports)

        Returns:
            Optional[bytes]: æ–‡ä»¶å†…å®¹ï¼Œå¤±è´¥æ—¶è¿”å›žNone
        """
        folder_prefix = f"{folder}/" if not folder.endswith("/") else folder
        full_key = f"{folder_prefix}{key}"

        try:
            response = self.s3_client.get_object(Bucket=self.bucket_name, Key=full_key)
            content = response["Body"].read()
            logger.info(f"âœ… æ–‡ä»¶ä¸‹è½½æˆåŠŸï¼šs3://{self.bucket_name}/{full_key}")
            return content
        except ClientError as e:
            if e.response["Error"]["Code"] == "NoSuchKey":
                logger.warning(f"âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨ï¼šs3://{self.bucket_name}/{full_key}")
            else:
                logger.error(f"âŒ æ–‡ä»¶ä¸‹è½½å¤±è´¥ï¼š{e}")
            return None
        except Exception as e:
            logger.error(f"âŒ æ–‡ä»¶ä¸‹è½½æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯ï¼š{e}")
            return None

    def delete_file(self, key: str, folder: str = "upload") -> bool:
        """
        ä»ŽS3åˆ é™¤æ–‡ä»¶

        Args:
            key: æ–‡ä»¶é”®å
            folder: æ–‡ä»¶å¤¹åç§° (upload/results/exports)

        Returns:
            bool: åˆ é™¤æ˜¯å¦æˆåŠŸ
        """
        folder_prefix = f"{folder}/" if not folder.endswith("/") else folder
        full_key = f"{folder_prefix}{key}"

        try:
            self.s3_client.delete_object(Bucket=self.bucket_name, Key=full_key)
            logger.info(f"âœ… æ–‡ä»¶åˆ é™¤æˆåŠŸï¼šs3://{self.bucket_name}/{full_key}")
            return True
        except ClientError as e:
            logger.error(f"âŒ æ–‡ä»¶åˆ é™¤å¤±è´¥ï¼š{e}")
            return False
        except Exception as e:
            logger.error(f"âŒ æ–‡ä»¶åˆ é™¤æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯ï¼š{e}")
            return False

    def file_exists(self, key: str, folder: str = "upload") -> bool:
        """
        æ£€æŸ¥æ–‡ä»¶æ˜¯å¦åœ¨S3ä¸­å­˜åœ¨

        Args:
            key: æ–‡ä»¶é”®å
            folder: æ–‡ä»¶å¤¹åç§° (upload/results/exports)

        Returns:
            bool: æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        """
        folder_prefix = f"{folder}/" if not folder.endswith("/") else folder
        full_key = f"{folder_prefix}{key}"

        try:
            self.s3_client.head_object(Bucket=self.bucket_name, Key=full_key)
            return True
        except ClientError as e:
            if e.response["Error"]["Code"] == "404":
                return False
            else:
                logger.error(f"âŒ æ£€æŸ¥æ–‡ä»¶å­˜åœ¨æ—¶å‡ºé”™ï¼š{e}")
                return False

    def get_file_info(self, key: str, folder: str = "upload") -> Optional[dict]:
        """
        èŽ·å–S3æ–‡ä»¶ä¿¡æ¯

        Args:
            key: æ–‡ä»¶é”®å
            folder: æ–‡ä»¶å¤¹åç§° (upload/results/exports)

        Returns:
            Optional[dict]: æ–‡ä»¶ä¿¡æ¯ï¼ŒåŒ…å«å¤§å°ã€ä¿®æ”¹æ—¶é—´ç­‰
        """
        folder_prefix = f"{folder}/" if not folder.endswith("/") else folder
        full_key = f"{folder_prefix}{key}"

        try:
            response = self.s3_client.head_object(Bucket=self.bucket_name, Key=full_key)
            return {
                "size": response.get("ContentLength", 0),
                "last_modified": response.get("LastModified"),
                "content_type": response.get("ContentType", "application/octet-stream"),
                "metadata": response.get("Metadata", {}),
            }
        except ClientError as e:
            if e.response["Error"]["Code"] == "404":
                logger.warning(f"âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨ï¼šs3://{self.bucket_name}/{full_key}")
            else:
                logger.error(f"âŒ èŽ·å–æ–‡ä»¶ä¿¡æ¯å¤±è´¥ï¼š{e}")
            return None

    def generate_presigned_url(
        self, key: str, expires_in: int = 3600, folder: str = "upload"
    ) -> Optional[str]:
        """
        ç”Ÿæˆé¢„ç­¾åURLç”¨äºŽä¸´æ—¶è®¿é—®æ–‡ä»¶

        Args:
            key: æ–‡ä»¶é”®å
            expires_in: URLè¿‡æœŸæ—¶é—´ï¼ˆç§’ï¼‰
            folder: æ–‡ä»¶å¤¹åç§° (upload/results/exports)

        Returns:
            Optional[str]: é¢„ç­¾åURL
        """
        folder_prefix = f"{folder}/" if not folder.endswith("/") else folder
        full_key = f"{folder_prefix}{key}"

        try:
            filename = os.path.basename(full_key) or "download"
            safe_filename = filename.replace('"', '')
            content_type, _ = mimetypes.guess_type(safe_filename)

            params = {
                "Bucket": self.bucket_name,
                "Key": full_key,
                "ResponseContentDisposition": f'attachment; filename="{safe_filename}"',
            }

            if content_type:
                params["ResponseContentType"] = content_type

            url = self.s3_client.generate_presigned_url(
                "get_object",
                Params=params,
                ExpiresIn=expires_in,
            )
            logger.info(f"âœ… ç”Ÿæˆé¢„ç­¾åURLæˆåŠŸï¼šs3://{self.bucket_name}/{full_key}")
            return url
        except ClientError as e:
            logger.error(f"âŒ ç”Ÿæˆé¢„ç­¾åURLå¤±è´¥ï¼š{e}")
            return None

    def generate_presigned_url_for_path(self, stored_path: str, expires_in: int = 3600) -> Optional[str]:
        """
        Generate presigned URL for stored database path (S3 URI or relative path)
        
        Args:
            stored_path: Full S3 URI (s3://bucket/key) or relative path within current bucket
            expires_in: URL expiration time in seconds
            
        Returns:
            Optional[str]: Presigned URL for direct download
        """
        try:
            if not stored_path:
                logger.warning("âš ï¸ Empty stored path provided")
                return None
            
            # Handle S3 URI format: s3://bucket/key/path/file.ext
            if stored_path.startswith('s3://'):
                # Parse S3 URI
                s3_parts = stored_path[5:].split('/', 1)  # Remove 's3://' and split on first '/'
                if len(s3_parts) != 2:
                    logger.error(f"âŒ Invalid S3 URI format: {stored_path}")
                    return None
                    
                bucket_name = s3_parts[0]
                s3_key = s3_parts[1]
                
                logger.info(f"ðŸ”— Generating presigned URL for S3 URI: bucket={bucket_name}, key={s3_key}")
                
                filename = os.path.basename(s3_key) or "download"
                safe_filename = filename.replace('"', '')
                content_type, _ = mimetypes.guess_type(safe_filename)

                params = {
                    "Bucket": bucket_name,
                    "Key": s3_key,
                    "ResponseContentDisposition": f'attachment; filename="{safe_filename}"',
                }

                if content_type:
                    params["ResponseContentType"] = content_type

                # Generate presigned URL
                url = self.s3_client.generate_presigned_url(
                    "get_object",
                    Params=params,
                    ExpiresIn=expires_in
                )
                
                logger.info(f"âœ… Successfully generated presigned URL for S3 URI: {stored_path}")
                return url
                
            # Handle relative paths (assume they're relative to current bucket)
            elif stored_path and not stored_path.startswith('/'):
                logger.info(f"ðŸ”— Generating presigned URL for relative path in bucket {self.bucket_name}: {stored_path}")
                
                filename = os.path.basename(stored_path) or "download"
                safe_filename = filename.replace('"', '')
                content_type, _ = mimetypes.guess_type(safe_filename)

                params = {
                    "Bucket": self.bucket_name,
                    "Key": stored_path,
                    "ResponseContentDisposition": f'attachment; filename="{safe_filename}"',
                }

                if content_type:
                    params["ResponseContentType"] = content_type

                url = self.s3_client.generate_presigned_url(
                    "get_object",
                    Params=params,
                    ExpiresIn=expires_in
                )
                
                logger.info(f"âœ… Successfully generated presigned URL for relative path: {stored_path}")
                return url
                
            else:
                logger.error(f"âŒ Unsupported path format: {stored_path}")
                return None
                
        except ClientError as e:
            logger.error(f"âŒ Failed to generate presigned URL: {e}")
            return None
        except Exception as e:
            logger.error(f"âŒ Unknown error generating presigned URL: {e}")
            return None

    def list_files(
        self, prefix: str = "", max_keys: int = 1000, folder: str = "upload"
    ) -> list:
        """
        åˆ—å‡ºS3å­˜å‚¨æ¡¶ä¸­çš„æ–‡ä»¶

        Args:
            prefix: æ–‡ä»¶å‰ç¼€è¿‡æ»¤
            max_keys: æœ€å¤§è¿”å›žæ–‡ä»¶æ•°
            folder: æ–‡ä»¶å¤¹åç§° (upload/results/exports)

        Returns:
            list: æ–‡ä»¶ä¿¡æ¯åˆ—è¡¨
        """
        folder_prefix = f"{folder}/" if not folder.endswith("/") else folder
        full_prefix = f"{folder_prefix}{prefix}"

        try:
            paginator = self.s3_client.get_paginator("list_objects_v2")
            pages = paginator.paginate(
                Bucket=self.bucket_name, Prefix=full_prefix, MaxKeys=max_keys
            )

            files = []
            for page in pages:
                if "Contents" in page:
                    for obj in page["Contents"]:
                        # ç§»é™¤æ–‡ä»¶å¤¹å‰ç¼€ï¼Œåªè¿”å›žç›¸å¯¹è·¯å¾„
                        relative_key = (
                            obj["Key"][len(folder_prefix) :]
                            if obj["Key"].startswith(folder_prefix)
                            else obj["Key"]
                        )
                        files.append(
                            {
                                "key": relative_key,
                                "full_key": obj["Key"],
                                "size": obj["Size"],
                                "last_modified": obj["LastModified"],
                                "etag": obj["ETag"].strip('"'),
                            }
                        )

            logger.info(
                f"âœ… åˆ—å‡ºæ–‡ä»¶æˆåŠŸï¼Œæ–‡ä»¶å¤¹ï¼š{folder}ï¼Œå‰ç¼€ï¼š{prefix}ï¼Œæ•°é‡ï¼š{len(files)}"
            )
            return files
        except ClientError as e:
            logger.error(f"âŒ åˆ—å‡ºæ–‡ä»¶å¤±è´¥ï¼š{e}")
            return []

    def upload_prompt(
        self,
        company_code: str,
        doc_type_code: str,
        prompt_content: str,
        filename: Optional[str] = None,
        metadata: Optional[dict] = None,
    ) -> Optional[str]:
        """
        ä¸Šä¼ promptæ–‡ä»¶åˆ°S3

        Args:
            company_code: å…¬å¸ä»£ç 
            doc_type_code: æ–‡æ¡£ç±»åž‹ä»£ç 
            prompt_content: promptå†…å®¹
            filename: æ–‡ä»¶åï¼ˆå¯é€‰ï¼Œé»˜è®¤ä¸ºprompt.txtï¼‰
            metadata: æ–‡ä»¶å…ƒæ•°æ®

        Returns:
            Optional[str]: S3é”®åï¼Œå¤±è´¥æ—¶è¿”å›žNone
        """
        try:
            if not filename:
                filename = "prompt.txt"

            # æž„å»ºS3é”®å
            key = f"{company_code}/{doc_type_code}/{filename}"
            full_key = f"{self.prompts_prefix}{key}"

            # å‡†å¤‡å…ƒæ•°æ®
            upload_metadata = {
                "company_code": company_code,
                "doc_type_code": doc_type_code,
                "file_type": "prompt",
                "uploaded_at": datetime.now().isoformat(),
            }
            if metadata:
                upload_metadata.update(metadata)

            # ä¸Šä¼ å†…å®¹
            self.s3_client.put_object(
                Bucket=self.bucket_name,
                Key=full_key,
                Body=prompt_content.encode("utf-8"),
                ContentType="text/plain",
                ContentEncoding="utf-8",
                Metadata=upload_metadata,
            )

            logger.info(f"âœ… Promptä¸Šä¼ æˆåŠŸï¼šs3://{self.bucket_name}/{full_key}")
            return key

        except Exception as e:
            logger.error(f"âŒ Promptä¸Šä¼ å¤±è´¥ï¼š{e}")
            return None

    def get_prompt(self, company_code: str, doc_type_code: str, filename: Optional[str] = None) -> Optional[str]:
        """
        ä»ŽS3èŽ·å–promptå†…å®¹

        Args:
            company_code: å…¬å¸ä»£ç 
            doc_type_code: æ–‡æ¡£ç±»åž‹ä»£ç 
            filename: æ–‡ä»¶åï¼ˆå¯é€‰ï¼Œé»˜è®¤ä¸ºprompt.txtï¼‰

        Returns:
            Optional[str]: promptå†…å®¹ï¼Œå¤±è´¥æ—¶è¿”å›žNone
        """
        try:
            if not filename:
                filename = "prompt.txt"

            key = f"{company_code}/{doc_type_code}/{filename}"
            full_key = f"{self.prompts_prefix}{key}"

            response = self.s3_client.get_object(Bucket=self.bucket_name, Key=full_key)
            content = response["Body"].read().decode("utf-8")

            logger.info(f"âœ… PromptèŽ·å–æˆåŠŸï¼šs3://{self.bucket_name}/{full_key}")
            return content

        except ClientError as e:
            if e.response["Error"]["Code"] == "NoSuchKey":
                logger.warning(f"âš ï¸ Promptæ–‡ä»¶ä¸å­˜åœ¨ï¼šs3://{self.bucket_name}/{self.prompts_prefix}{company_code}/{doc_type_code}/{filename or 'prompt.txt'}")
            else:
                logger.error(f"âŒ PromptèŽ·å–å¤±è´¥ï¼š{e}")
            return None
        except Exception as e:
            logger.error(f"âŒ PromptèŽ·å–æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯ï¼š{e}")
            return None

    def get_file_by_key(self, s3_key: str) -> Optional[str]:
        """
        é€šè¿‡å®Œæ•´S3 keyèŽ·å–æ–‡ä»¶å†…å®¹ï¼ˆæ–‡æœ¬æ–‡ä»¶ï¼‰
        
        Args:
            s3_key: å®Œæ•´çš„S3 keyè·¯å¾„
            
        Returns:
            Optional[str]: æ–‡ä»¶å†…å®¹ï¼Œå¤±è´¥æ—¶è¿”å›žNone
        """
        try:
            response = self.s3_client.get_object(Bucket=self.bucket_name, Key=s3_key)
            content = response["Body"].read().decode("utf-8")
            
            logger.info(f"âœ… æ–‡ä»¶èŽ·å–æˆåŠŸï¼šs3://{self.bucket_name}/{s3_key}")
            return content
            
        except ClientError as e:
            if e.response["Error"]["Code"] == "NoSuchKey":
                logger.warning(f"âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨ï¼šs3://{self.bucket_name}/{s3_key}")
            else:
                logger.error(f"âŒ æ–‡ä»¶èŽ·å–å¤±è´¥ï¼š{e}")
            return None
        except Exception as e:
            logger.error(f"âŒ æ–‡ä»¶èŽ·å–æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯ï¼š{e}")
            return None

    def get_schema_by_key(self, s3_key: str) -> Optional[dict]:
        """
        é€šè¿‡å®Œæ•´S3 keyèŽ·å–schemaæ–‡ä»¶å†…å®¹
        
        Args:
            s3_key: å®Œæ•´çš„S3 keyè·¯å¾„
            
        Returns:
            Optional[dict]: schemaæ•°æ®ï¼Œå¤±è´¥æ—¶è¿”å›žNone
        """
        try:
            response = self.s3_client.get_object(Bucket=self.bucket_name, Key=s3_key)
            content = response["Body"].read().decode("utf-8")
            schema_data = json.loads(content)
            
            # Clean schema for Gemini API compatibility
            schema_data = clean_schema_for_gemini(schema_data)
            
            logger.info(f"âœ… SchemaèŽ·å–æˆåŠŸï¼šs3://{self.bucket_name}/{s3_key}")
            return schema_data
            
        except ClientError as e:
            if e.response["Error"]["Code"] == "NoSuchKey":
                logger.warning(f"âš ï¸ Schemaæ–‡ä»¶ä¸å­˜åœ¨ï¼šs3://{self.bucket_name}/{s3_key}")
            else:
                logger.error(f"âŒ SchemaèŽ·å–å¤±è´¥ï¼š{e}")
            return None
        except json.JSONDecodeError as e:
            logger.error(f"âŒ Schema JSONè§£æžå¤±è´¥ï¼š{e}")
            return None
        except Exception as e:
            logger.error(f"âŒ SchemaèŽ·å–æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯ï¼š{e}")
            return None

    def upload_schema(
        self,
        company_code: str,
        doc_type_code: str,
        schema_data: dict,
        filename: Optional[str] = None,
        metadata: Optional[dict] = None,
    ) -> Optional[str]:
        """
        ä¸Šä¼ schemaæ–‡ä»¶åˆ°S3

        Args:
            company_code: å…¬å¸ä»£ç 
            doc_type_code: æ–‡æ¡£ç±»åž‹ä»£ç 
            schema_data: schema JSONæ•°æ®
            filename: æ–‡ä»¶åï¼ˆå¯é€‰ï¼Œé»˜è®¤ä¸ºschema.jsonï¼‰
            metadata: æ–‡ä»¶å…ƒæ•°æ®

        Returns:
            Optional[str]: S3é”®åï¼Œå¤±è´¥æ—¶è¿”å›žNone
        """
        try:
            if not filename:
                filename = "schema.json"

            # éªŒè¯schemaæ ¼å¼
            if not isinstance(schema_data, dict):
                raise ValueError("Schema data must be a dictionary")

            # æž„å»ºS3é”®å
            key = f"{company_code}/{doc_type_code}/{filename}"
            full_key = f"{self.schemas_prefix}{key}"

            # å‡†å¤‡å…ƒæ•°æ®
            upload_metadata = {
                "company_code": company_code,
                "doc_type_code": doc_type_code,
                "file_type": "schema",
                "uploaded_at": datetime.now().isoformat(),
            }
            if metadata:
                upload_metadata.update(metadata)

            # è½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²
            schema_content = json.dumps(schema_data, ensure_ascii=False, indent=2)

            # ä¸Šä¼ å†…å®¹
            self.s3_client.put_object(
                Bucket=self.bucket_name,
                Key=full_key,
                Body=schema_content.encode("utf-8"),
                ContentType="application/json",
                ContentEncoding="utf-8",
                Metadata=upload_metadata,
            )

            logger.info(f"âœ… Schemaä¸Šä¼ æˆåŠŸï¼šs3://{self.bucket_name}/{full_key}")
            return key

        except Exception as e:
            logger.error(f"âŒ Schemaä¸Šä¼ å¤±è´¥ï¼š{e}")
            return None

    def get_schema(self, company_code: str, doc_type_code: str, filename: Optional[str] = None) -> Optional[dict]:
        """
        ä»ŽS3èŽ·å–schemaæ•°æ®

        Args:
            company_code: å…¬å¸ä»£ç 
            doc_type_code: æ–‡æ¡£ç±»åž‹ä»£ç 
            filename: æ–‡ä»¶åï¼ˆå¯é€‰ï¼Œé»˜è®¤ä¸ºschema.jsonï¼‰

        Returns:
            Optional[dict]: schemaæ•°æ®ï¼Œå¤±è´¥æ—¶è¿”å›žNone
        """
        try:
            if not filename:
                filename = "schema.json"

            key = f"{company_code}/{doc_type_code}/{filename}"
            full_key = f"{self.schemas_prefix}{key}"

            response = self.s3_client.get_object(Bucket=self.bucket_name, Key=full_key)
            content = response["Body"].read().decode("utf-8")
            schema_data = json.loads(content)
            
            # Clean schema for Gemini API compatibility
            schema_data = clean_schema_for_gemini(schema_data)

            logger.info(f"âœ… SchemaèŽ·å–æˆåŠŸï¼šs3://{self.bucket_name}/{full_key}")
            return schema_data

        except ClientError as e:
            if e.response["Error"]["Code"] == "NoSuchKey":
                logger.warning(f"âš ï¸ Schemaæ–‡ä»¶ä¸å­˜åœ¨ï¼šs3://{self.bucket_name}/{self.schemas_prefix}{company_code}/{doc_type_code}/{filename or 'schema.json'}")
            else:
                logger.error(f"âŒ SchemaèŽ·å–å¤±è´¥ï¼š{e}")
            return None
        except json.JSONDecodeError as e:
            logger.error(f"âŒ Schema JSONè§£æžå¤±è´¥ï¼š{e}")
            return None
        except Exception as e:
            logger.error(f"âŒ SchemaèŽ·å–æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯ï¼š{e}")
            return None

    def download_prompt_raw(self, company_code: str, doc_type_code: str, filename: Optional[str] = None) -> Optional[bytes]:
        """
        ä»ŽS3ä¸‹è½½promptæ–‡ä»¶çš„åŽŸå§‹å†…å®¹ï¼ˆç”¨äºŽæ–‡ä»¶ä¸‹è½½ï¼‰

        Args:
            company_code: å…¬å¸ä»£ç 
            doc_type_code: æ–‡æ¡£ç±»åž‹ä»£ç 
            filename: æ–‡ä»¶åï¼ˆå¯é€‰ï¼Œé»˜è®¤ä¸ºprompt.txtï¼‰

        Returns:
            Optional[bytes]: æ–‡ä»¶åŽŸå§‹å†…å®¹ï¼Œå¤±è´¥æ—¶è¿”å›žNone
        """
        try:
            if not filename:
                filename = "prompt.txt"

            key = f"{company_code}/{doc_type_code}/{filename}"
            full_key = f"{self.prompts_prefix}{key}"

            response = self.s3_client.get_object(Bucket=self.bucket_name, Key=full_key)
            content = response["Body"].read()

            logger.info(f"âœ… PromptåŽŸå§‹å†…å®¹ä¸‹è½½æˆåŠŸï¼šs3://{self.bucket_name}/{full_key}")
            return content

        except ClientError as e:
            if e.response["Error"]["Code"] == "NoSuchKey":
                logger.warning(f"âš ï¸ Promptæ–‡ä»¶ä¸å­˜åœ¨ï¼šs3://{self.bucket_name}/{self.prompts_prefix}{company_code}/{doc_type_code}/{filename or 'prompt.txt'}")
            else:
                logger.error(f"âŒ PromptåŽŸå§‹å†…å®¹ä¸‹è½½å¤±è´¥ï¼š{e}")
            return None
        except Exception as e:
            logger.error(f"âŒ PromptåŽŸå§‹å†…å®¹ä¸‹è½½æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯ï¼š{e}")
            return None

    def download_schema_raw(self, company_code: str, doc_type_code: str, filename: Optional[str] = None) -> Optional[bytes]:
        """
        ä»ŽS3ä¸‹è½½schemaæ–‡ä»¶çš„åŽŸå§‹å†…å®¹ï¼ˆç”¨äºŽæ–‡ä»¶ä¸‹è½½ï¼‰

        Args:
            company_code: å…¬å¸ä»£ç 
            doc_type_code: æ–‡æ¡£ç±»åž‹ä»£ç 
            filename: æ–‡ä»¶åï¼ˆå¯é€‰ï¼Œé»˜è®¤ä¸ºschema.jsonï¼‰

        Returns:
            Optional[bytes]: æ–‡ä»¶åŽŸå§‹å†…å®¹ï¼Œå¤±è´¥æ—¶è¿”å›žNone
        """
        try:
            if not filename:
                filename = "schema.json"

            key = f"{company_code}/{doc_type_code}/{filename}"
            full_key = f"{self.schemas_prefix}{key}"

            response = self.s3_client.get_object(Bucket=self.bucket_name, Key=full_key)
            content = response["Body"].read()

            logger.info(f"âœ… SchemaåŽŸå§‹å†…å®¹ä¸‹è½½æˆåŠŸï¼šs3://{self.bucket_name}/{full_key}")
            return content

        except ClientError as e:
            if e.response["Error"]["Code"] == "NoSuchKey":
                logger.warning(f"âš ï¸ Schemaæ–‡ä»¶ä¸å­˜åœ¨ï¼šs3://{self.bucket_name}/{self.schemas_prefix}{company_code}/{doc_type_code}/{filename or 'schema.json'}")
            else:
                logger.error(f"âŒ SchemaåŽŸå§‹å†…å®¹ä¸‹è½½å¤±è´¥ï¼š{e}")
            return None
        except Exception as e:
            logger.error(f"âŒ SchemaåŽŸå§‹å†…å®¹ä¸‹è½½æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯ï¼š{e}")
            return None

    def list_prompts(self, company_code: Optional[str] = None, doc_type_code: Optional[str] = None) -> list:
        """
        åˆ—å‡ºpromptæ–‡ä»¶

        Args:
            company_code: å…¬å¸ä»£ç ï¼ˆå¯é€‰ï¼‰
            doc_type_code: æ–‡æ¡£ç±»åž‹ä»£ç ï¼ˆå¯é€‰ï¼‰

        Returns:
            list: promptæ–‡ä»¶ä¿¡æ¯åˆ—è¡¨
        """
        try:
            # æž„å»ºå‰ç¼€
            prefix = ""
            if company_code and doc_type_code:
                prefix = f"{company_code}/{doc_type_code}/"
            elif company_code:
                prefix = f"{company_code}/"

            return self.list_files(prefix=prefix, folder="prompts")

        except Exception as e:
            logger.error(f"âŒ åˆ—å‡ºpromptæ–‡ä»¶å¤±è´¥ï¼š{e}")
            return []

    def list_schemas(self, company_code: Optional[str] = None, doc_type_code: Optional[str] = None) -> list:
        """
        åˆ—å‡ºschemaæ–‡ä»¶

        Args:
            company_code: å…¬å¸ä»£ç ï¼ˆå¯é€‰ï¼‰
            doc_type_code: æ–‡æ¡£ç±»åž‹ä»£ç ï¼ˆå¯é€‰ï¼‰

        Returns:
            list: schemaæ–‡ä»¶ä¿¡æ¯åˆ—è¡¨
        """
        try:
            # æž„å»ºå‰ç¼€
            prefix = ""
            if company_code and doc_type_code:
                prefix = f"{company_code}/{doc_type_code}/"
            elif company_code:
                prefix = f"{company_code}/"

            return self.list_files(prefix=prefix, folder="schemas")

        except Exception as e:
            logger.error(f"âŒ åˆ—å‡ºschemaæ–‡ä»¶å¤±è´¥ï¼š{e}")
            return []

    def delete_prompt(self, company_code: str, doc_type_code: str, filename: Optional[str] = None) -> bool:
        """
        åˆ é™¤promptæ–‡ä»¶

        Args:
            company_code: å…¬å¸ä»£ç 
            doc_type_code: æ–‡æ¡£ç±»åž‹ä»£ç 
            filename: æ–‡ä»¶åï¼ˆå¯é€‰ï¼Œé»˜è®¤ä¸ºprompt.txtï¼‰

        Returns:
            bool: åˆ é™¤æ˜¯å¦æˆåŠŸ
        """
        try:
            if not filename:
                filename = "prompt.txt"

            key = f"{company_code}/{doc_type_code}/{filename}"
            return self.delete_file(key, folder="prompts")

        except Exception as e:
            logger.error(f"âŒ åˆ é™¤promptæ–‡ä»¶å¤±è´¥ï¼š{e}")
            return False

    def delete_schema(self, company_code: str, doc_type_code: str, filename: Optional[str] = None) -> bool:
        """
        åˆ é™¤schemaæ–‡ä»¶

        Args:
            company_code: å…¬å¸ä»£ç 
            doc_type_code: æ–‡æ¡£ç±»åž‹ä»£ç 
            filename: æ–‡ä»¶åï¼ˆå¯é€‰ï¼Œé»˜è®¤ä¸ºschema.jsonï¼‰

        Returns:
            bool: åˆ é™¤æ˜¯å¦æˆåŠŸ
        """
        try:
            if not filename:
                filename = "schema.json"

            key = f"{company_code}/{doc_type_code}/{filename}"
            return self.delete_file(key, folder="schemas")

        except Exception as e:
            logger.error(f"âŒ åˆ é™¤schemaæ–‡ä»¶å¤±è´¥ï¼š{e}")
            return False

    def prompt_exists(self, company_code: str, doc_type_code: str, filename: Optional[str] = None) -> bool:
        """
        æ£€æŸ¥promptæ–‡ä»¶æ˜¯å¦å­˜åœ¨

        Args:
            company_code: å…¬å¸ä»£ç 
            doc_type_code: æ–‡æ¡£ç±»åž‹ä»£ç 
            filename: æ–‡ä»¶åï¼ˆå¯é€‰ï¼Œé»˜è®¤ä¸ºprompt.txtï¼‰

        Returns:
            bool: æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        """
        try:
            if not filename:
                filename = "prompt.txt"

            key = f"{company_code}/{doc_type_code}/{filename}"
            return self.file_exists(key, folder="prompts")

        except Exception as e:
            logger.error(f"âŒ æ£€æŸ¥promptæ–‡ä»¶å­˜åœ¨æ—¶å‡ºé”™ï¼š{e}")
            return False

    def schema_exists(self, company_code: str, doc_type_code: str, filename: Optional[str] = None) -> bool:
        """
        æ£€æŸ¥schemaæ–‡ä»¶æ˜¯å¦å­˜åœ¨

        Args:
            company_code: å…¬å¸ä»£ç 
            doc_type_code: æ–‡æ¡£ç±»åž‹ä»£ç 
            filename: æ–‡ä»¶åï¼ˆå¯é€‰ï¼Œé»˜è®¤ä¸ºschema.jsonï¼‰

        Returns:
            bool: æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        """
        try:
            if not filename:
                filename = "schema.json"

            key = f"{company_code}/{doc_type_code}/{filename}"
            return self.file_exists(key, folder="schemas")

        except Exception as e:
            logger.error(f"âŒ æ£€æŸ¥schemaæ–‡ä»¶å­˜åœ¨æ—¶å‡ºé”™ï¼š{e}")
            return False

    @staticmethod
    def generate_file_key(
        company_code: str,
        doc_type_code: str,
        filename: str,
        job_id: Optional[int] = None,
    ) -> str:
        """
        ç”Ÿæˆæ ‡å‡†åŒ–çš„S3æ–‡ä»¶é”®å

        Args:
            company_code: å…¬å¸ä»£ç 
            doc_type_code: æ–‡æ¡£ç±»åž‹ä»£ç 
            filename: åŽŸå§‹æ–‡ä»¶å
            job_id: ä»»åŠ¡IDï¼ˆå¯é€‰ï¼‰

        Returns:
            str: S3æ–‡ä»¶é”®å
        """
        # æ¸…ç†æ–‡ä»¶åä¸­çš„ç‰¹æ®Šå­—ç¬¦
        safe_filename = "".join(
            c for c in filename if c.isalnum() or c in (" ", "-", "_", ".")
        ).rstrip()

        # æž„å»ºæ–‡ä»¶è·¯å¾„
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        unique_id = str(uuid.uuid4())[:8]

        if job_id:
            key = f"uploads/{company_code}/{doc_type_code}/jobs/{job_id}/{timestamp}_{unique_id}_{safe_filename}"
        else:
            key = f"uploads/{company_code}/{doc_type_code}/{timestamp}_{unique_id}_{safe_filename}"

        return key

    def get_health_status(self) -> dict:
        """èŽ·å–S3å­˜å‚¨çš„å¥åº·çŠ¶æ€"""
        try:
            # æµ‹è¯•å­˜å‚¨æ¡¶è¿žæŽ¥
            self.s3_client.head_bucket(Bucket=self.bucket_name)

            # èŽ·å–å­˜å‚¨æ¡¶ä¿¡æ¯
            location = self.s3_client.get_bucket_location(Bucket=self.bucket_name)

            return {
                "status": "healthy",
                "bucket": self.bucket_name,
                "region": location.get("LocationConstraint", "us-east-1"),
                "accessible": True,
                "folders": ["upload", "results", "exports", "prompts", "schemas"],
            }
        except Exception as e:
            return {
                "status": "unhealthy",
                "bucket": self.bucket_name,
                "region": self.region,
                "accessible": False,
                "error": str(e),
            }

    # ========================================
    # NEW ID-BASED METHODS FOR FILE MANAGEMENT
    # ========================================
    
    def upload_company_file(self, 
                          company_id: int,
                          file_type: FileType,
                          content: Union[str, bytes],
                          filename: str,
                          doc_type_id: Optional[int] = None,
                          config_id: Optional[int] = None,
                          job_id: Optional[str] = None,
                          export_id: Optional[str] = None,
                          metadata: Optional[dict] = None) -> Optional[str]:
        """
        Upload file using new ID-based path structure
        
        Args:
            company_id: Company ID
            file_type: Type of file (prompt, schema, upload, result, export)
            content: File content (string or bytes)
            filename: Original filename
            doc_type_id: Document type ID (for prompts/schemas)
            config_id: Configuration ID (for prompts/schemas)
            job_id: Job ID (for uploads/results)
            export_id: Export ID (for exports)
            metadata: Additional metadata
            
        Returns:
            Optional[str]: S3 key if successful, None if failed
        """
        try:
            # Generate ID-based path
            s3_path = self.company_file_manager.get_company_file_path(
                company_id=company_id,
                file_type=file_type,
                filename=filename,
                doc_type_id=doc_type_id,
                config_id=config_id,
                job_id=job_id,
                export_id=export_id
            )
            
            # Prepare metadata
            upload_metadata = {
                "company_id": str(company_id),
                "file_type": file_type.value,
                "uploaded_at": datetime.now().isoformat(),
            }
            
            # Add type-specific metadata
            if doc_type_id:
                upload_metadata["doc_type_id"] = str(doc_type_id)
            if config_id:
                upload_metadata["config_id"] = str(config_id)
            if job_id:
                upload_metadata["job_id"] = job_id
            if export_id:
                upload_metadata["export_id"] = export_id
                
            if metadata:
                upload_metadata.update(metadata)
            
            # Determine content type
            content_type = "text/plain"
            if file_type == FileType.SCHEMA or filename.endswith('.json'):
                content_type = "application/json"
            elif filename.endswith(('.pdf', '.png', '.jpg', '.jpeg')):
                content_type, _ = mimetypes.guess_type(filename)
                if not content_type:
                    content_type = "application/octet-stream"
            
            # Convert content to bytes if needed
            if isinstance(content, str):
                body = content.encode('utf-8')
                encoding = "utf-8"
            else:
                body = content
                encoding = None
                
            # Upload to S3
            put_args = {
                "Bucket": self.bucket_name,
                "Key": s3_path,
                "Body": body,
                "ContentType": content_type,
                "Metadata": upload_metadata,
            }
            
            if encoding:
                put_args["ContentEncoding"] = encoding
            
            self.s3_client.put_object(**put_args)
            
            logger.info(f"âœ… ID-based file upload successful: s3://{self.bucket_name}/{s3_path}")
            return s3_path
            
        except Exception as e:
            logger.error(f"âŒ ID-based file upload failed: {e}")
            return None
    
    def download_company_file(self,
                            company_id: int,
                            file_type: FileType,
                            filename: str,
                            doc_type_id: Optional[int] = None,
                            config_id: Optional[int] = None,
                            job_id: Optional[str] = None,
                            export_id: Optional[str] = None) -> Optional[bytes]:
        """
        Download file using new ID-based path structure
        
        Args:
            company_id: Company ID
            file_type: Type of file
            filename: Original filename
            doc_type_id: Document type ID (for prompts/schemas)
            config_id: Configuration ID (for prompts/schemas)
            job_id: Job ID (for uploads/results)
            export_id: Export ID (for exports)
            
        Returns:
            Optional[bytes]: File content if found, None if not found
        """
        try:
            # Generate ID-based path
            s3_path = self.company_file_manager.get_company_file_path(
                company_id=company_id,
                file_type=file_type,
                filename=filename,
                doc_type_id=doc_type_id,
                config_id=config_id,
                job_id=job_id,
                export_id=export_id
            )
            
            # Download from S3
            response = self.s3_client.get_object(Bucket=self.bucket_name, Key=s3_path)
            content = response["Body"].read()
            
            logger.info(f"âœ… ID-based file download successful: s3://{self.bucket_name}/{s3_path}")
            return content
            
        except ClientError as e:
            if e.response['Error']['Code'] == 'NoSuchKey':
                logger.warning(f"âš ï¸ ID-based file not found: s3://{self.bucket_name}/{s3_path}")
            else:
                logger.error(f"âŒ ID-based file download failed: {e}")
            return None
        except Exception as e:
            logger.error(f"âŒ ID-based file download failed: {e}")
            return None
    
    def upload_prompt_by_id(self,
                          company_id: int,
                          doc_type_id: int,
                          config_id: int,
                          prompt_content: str,
                          filename: Optional[str] = None,
                          metadata: Optional[dict] = None) -> Optional[str]:
        """
        Upload prompt using clean ID-based path (convenience method)
        
        Args:
            company_id: Company ID
            doc_type_id: Document type ID
            config_id: Configuration ID
            prompt_content: Prompt content
            filename: Filename (should be original filename)
            metadata: Additional metadata
            
        Returns:
            Optional[str]: S3 key if successful
        """
        if not filename:
            filename = "prompt.txt"  # Use clean default filename
            
        return self.upload_company_file(
            company_id=company_id,
            file_type=FileType.PROMPT,
            content=prompt_content,
            filename=filename,
            doc_type_id=doc_type_id,
            config_id=config_id,
            metadata=metadata
        )
    
    def upload_schema_by_id(self,
                          company_id: int,
                          doc_type_id: int,
                          config_id: int,
                          schema_data: dict,
                          filename: Optional[str] = None,
                          metadata: Optional[dict] = None) -> Optional[str]:
        """
        Upload schema using clean ID-based path (convenience method)
        
        Args:
            company_id: Company ID
            doc_type_id: Document type ID
            config_id: Configuration ID
            schema_data: Schema data as dict
            filename: Filename (should be original filename)
            metadata: Additional metadata
            
        Returns:
            Optional[str]: S3 key if successful
        """
        import json
        
        if not filename:
            filename = "schema.json"  # Use clean default filename
            
        # Convert schema data to JSON string
        schema_content = json.dumps(schema_data, indent=2, ensure_ascii=False)
            
        return self.upload_company_file(
            company_id=company_id,
            file_type=FileType.SCHEMA,
            content=schema_content,
            filename=filename,
            doc_type_id=doc_type_id,
            config_id=config_id,
            metadata=metadata
        )
    
    def download_prompt_by_id(self,
                            company_id: int,
                            doc_type_id: int,
                            config_id: int,
                            filename: Optional[str] = None) -> Optional[str]:
        """
        Download prompt using clean ID-based path with fallbacks (convenience method)
        
        Returns:
            Optional[str]: Prompt content as string
        """
        if not filename:
            filename = "prompt.txt"  # Clean default filename
            
        # Try primary ID-based path
        content = self.download_company_file(
            company_id=company_id,
            file_type=FileType.PROMPT,
            filename=filename,
            doc_type_id=doc_type_id,
            config_id=config_id
        )
        
        # If not found, try temp path fallback
        if content is None:
            logger.info(f"ðŸ”„ Primary path not found, trying temp path fallback for config {config_id}")
            temp_paths = [
                f"companies/{company_id}/prompts/{doc_type_id}/temp_{config_id}/{filename}",
                f"companies/{company_id}/prompts/{doc_type_id}/temp_{int(config_id) * 1000}/{filename}",
                # Try some common temp timestamp patterns
                f"companies/{company_id}/prompts/{doc_type_id}/temp_1758089852851/{filename}",
            ]
            
            for temp_path in temp_paths:
                logger.info(f"ðŸ” Trying temp path: {temp_path}")
                try:
                    response = self.s3_client.get_object(Bucket=self.bucket_name, Key=temp_path)
                    content = response["Body"].read()
                    logger.info(f"âœ… Found prompt at temp path: {temp_path}")
                    break
                except ClientError as e:
                    if e.response['Error']['Code'] != 'NoSuchKey':
                        logger.warning(f"âš ï¸ S3 error on temp path {temp_path}: {e}")
                except Exception as e:
                    logger.warning(f"âš ï¸ Error trying temp path {temp_path}: {e}")
        
        if content:
            return content.decode('utf-8')
        return None
    
    def download_schema_by_id(self,
                            company_id: int,
                            doc_type_id: int,
                            config_id: int,
                            filename: Optional[str] = None) -> Optional[dict]:
        """
        Download schema using clean ID-based path with fallbacks (convenience method)
        
        Returns:
            Optional[dict]: Schema data as dictionary
        """
        if not filename:
            filename = "schema.json"  # Clean default filename
            
        # Try primary ID-based path
        content = self.download_company_file(
            company_id=company_id,
            file_type=FileType.SCHEMA,
            filename=filename,
            doc_type_id=doc_type_id,
            config_id=config_id
        )
        
        # If not found, try temp path fallback
        if content is None:
            logger.info(f"ðŸ”„ Primary schema path not found, trying temp path fallback for config {config_id}")
            temp_paths = [
                f"companies/{company_id}/schemas/{doc_type_id}/temp_{config_id}/{filename}",
                f"companies/{company_id}/schemas/{doc_type_id}/temp_{int(config_id) * 1000}/{filename}",
                # Try some common temp timestamp patterns
                f"companies/{company_id}/schemas/{doc_type_id}/temp_1758089852982/{filename}",
            ]
            
            for temp_path in temp_paths:
                logger.info(f"ðŸ” Trying temp schema path: {temp_path}")
                try:
                    response = self.s3_client.get_object(Bucket=self.bucket_name, Key=temp_path)
                    content = response["Body"].read()
                    logger.info(f"âœ… Found schema at temp path: {temp_path}")
                    break
                except ClientError as e:
                    if e.response['Error']['Code'] != 'NoSuchKey':
                        logger.warning(f"âš ï¸ S3 error on temp schema path {temp_path}: {e}")
                except Exception as e:
                    logger.warning(f"âš ï¸ Error trying temp schema path {temp_path}: {e}")
        
        if content:
            try:
                import json
                schema_data = json.loads(content.decode('utf-8'))
                
                # Clean schema for Gemini API compatibility
                schema_data = clean_schema_for_gemini(schema_data)
                
                return schema_data
            except json.JSONDecodeError as e:
                logger.error(f"âŒ Failed to parse schema JSON: {e}")
                return None
        return None
    
    def download_file_by_stored_path(self, stored_path: str) -> Optional[bytes]:
        """
        Download file using stored database path (S3 URI or relative path)
        
        Args:
            stored_path: Full S3 URI (s3://bucket/key) or relative path within current bucket
            
        Returns:
            Optional[bytes]: File content if found, None if not found
        """
        try:
            if not stored_path:
                logger.warning("âš ï¸ Empty stored path provided")
                return None
            
            # Handle S3 URI format: s3://bucket/key/path/file.ext
            if stored_path.startswith('s3://'):
                # Parse S3 URI
                s3_parts = stored_path[5:].split('/', 1)  # Remove 's3://' and split on first '/'
                if len(s3_parts) != 2:
                    logger.error(f"âŒ Invalid S3 URI format: {stored_path}")
                    return None
                    
                bucket_name = s3_parts[0]
                s3_key = s3_parts[1]
                
                logger.info(f"ðŸ“¥ Downloading from S3 URI: bucket={bucket_name}, key={s3_key}")
                
                # Direct S3 download
                response = self.s3_client.get_object(Bucket=bucket_name, Key=s3_key)
                content = response["Body"].read()
                
                logger.info(f"âœ… Successfully downloaded from S3 URI: {stored_path}")
                return content
                
            # Handle relative paths (assume they're relative to current bucket)
            elif stored_path and not stored_path.startswith('/'):
                logger.info(f"ðŸ“¥ Downloading relative path from bucket {self.bucket_name}: {stored_path}")
                
                response = self.s3_client.get_object(Bucket=self.bucket_name, Key=stored_path)
                content = response["Body"].read()
                
                logger.info(f"âœ… Successfully downloaded relative path: {stored_path}")
                return content
                
            else:
                logger.error(f"âŒ Unsupported path format: {stored_path}")
                return None
                
        except ClientError as e:
            if e.response['Error']['Code'] == 'NoSuchKey':
                logger.warning(f"âš ï¸ File not found at stored path: {stored_path}")
            else:
                logger.error(f"âŒ S3 error downloading from stored path: {e}")
            return None
        except Exception as e:
            logger.error(f"âŒ Failed to download from stored path '{stored_path}': {e}")
            return None


# å…¨å±€S3å­˜å‚¨ç®¡ç†å™¨å®žä¾‹
_s3_manager = None


def get_s3_manager() -> Optional[S3StorageManager]:
    """èŽ·å–å…¨å±€S3å­˜å‚¨ç®¡ç†å™¨å®žä¾‹"""
    global _s3_manager

    if _s3_manager is None:
        try:
            # ä»ŽçŽ¯å¢ƒå˜é‡èŽ·å–S3è®¾ç½®
            bucket_name = os.getenv("S3_BUCKET_NAME")
            region = os.getenv("AWS_DEFAULT_REGION", "ap-southeast-1")

            if bucket_name:
                _s3_manager = S3StorageManager(bucket_name, region)
                _s3_manager.ensure_bucket_exists()
                logger.info(f"âœ… S3å­˜å‚¨ç®¡ç†å™¨åˆå§‹åŒ–æˆåŠŸï¼šbucket={bucket_name}")
            else:
                logger.warning("âš ï¸ æœªé…ç½®S3å­˜å‚¨æ¡¶åç§°ï¼Œå°†ä½¿ç”¨æœ¬åœ°æ–‡ä»¶å­˜å‚¨")

        except Exception as e:
            logger.error(f"âŒ S3å­˜å‚¨ç®¡ç†å™¨åˆå§‹åŒ–å¤±è´¥ï¼š{e}")
            _s3_manager = None

    return _s3_manager


def is_s3_enabled() -> bool:
    """æ£€æŸ¥æ˜¯å¦å¯ç”¨äº†S3å­˜å‚¨"""
    return get_s3_manager() is not None
