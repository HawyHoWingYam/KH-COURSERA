"""
Prompt and Schema Management System
ç»Ÿä¸€çš„promptå’Œschemaç®¡ç†ç³»ç»Ÿï¼Œæ”¯æŒS3å’Œæœ¬åœ°å­˜å‚¨

åŠŸèƒ½ç‰¹æ€§:
- S3å’Œæœ¬åœ°å­˜å‚¨çš„è‡ªåŠ¨åˆ‡æ¢
- å†…å­˜ç¼“å­˜æœºåˆ¶æå‡æ€§èƒ½
- è‡ªåŠ¨éªŒè¯promptå’Œschemaæ ¼å¼
- ç»Ÿä¸€çš„é”™è¯¯å¤„ç†å’Œæ—¥å¿—è®°å½•
- æ”¯æŒæ‰¹é‡æ“ä½œå’Œè¿ç§»
"""

import json
import logging
from typing import Optional, Dict, Tuple, Union
from datetime import datetime, timedelta
from pathlib import Path
import asyncio
from concurrent.futures import ThreadPoolExecutor

from .s3_storage import get_s3_manager, is_s3_enabled

logger = logging.getLogger(__name__)


class PromptSchemaCache:
    """å†…å­˜ç¼“å­˜ç®¡ç†å™¨"""
    
    def __init__(self, max_size: int = 100, ttl_minutes: int = 30):
        self.cache: Dict[str, Dict] = {}
        self.max_size = max_size
        self.ttl = timedelta(minutes=ttl_minutes)
        
    def _generate_key(self, company_code: str, doc_type_code: str, file_type: str, filename: str) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        return f"{company_code}:{doc_type_code}:{file_type}:{filename}"
    
    def get(self, company_code: str, doc_type_code: str, file_type: str, filename: str) -> Optional[Union[str, dict]]:
        """ä»ç¼“å­˜è·å–å†…å®¹"""
        key = self._generate_key(company_code, doc_type_code, file_type, filename)
        
        if key in self.cache:
            cached_item = self.cache[key]
            # æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
            if datetime.now() - cached_item["cached_at"] < self.ttl:
                logger.debug(f"ğŸŸ¢ ç¼“å­˜å‘½ä¸­: {key}")
                return cached_item["content"]
            else:
                # è¿‡æœŸï¼Œåˆ é™¤
                del self.cache[key]
                logger.debug(f"ğŸ”„ ç¼“å­˜è¿‡æœŸï¼Œå·²åˆ é™¤: {key}")
        
        return None
    
    def set(self, company_code: str, doc_type_code: str, file_type: str, filename: str, content: Union[str, dict]):
        """è®¾ç½®ç¼“å­˜å†…å®¹"""
        key = self._generate_key(company_code, doc_type_code, file_type, filename)
        
        # å¦‚æœç¼“å­˜å·²æ»¡ï¼Œåˆ é™¤æœ€æ—§çš„é¡¹ç›®
        if len(self.cache) >= self.max_size:
            oldest_key = min(self.cache.keys(), key=lambda k: self.cache[k]["cached_at"])
            del self.cache[oldest_key]
            logger.debug(f"ğŸ—‘ï¸ ç¼“å­˜å·²æ»¡ï¼Œåˆ é™¤æœ€æ—§é¡¹: {oldest_key}")
        
        self.cache[key] = {
            "content": content,
            "cached_at": datetime.now()
        }
        logger.debug(f"ğŸ’¾ ç¼“å­˜å·²è®¾ç½®: {key}")
    
    def invalidate(self, company_code: str = None, doc_type_code: str = None):
        """ä½¿ç¼“å­˜å¤±æ•ˆ"""
        if company_code and doc_type_code:
            # åˆ é™¤ç‰¹å®šå…¬å¸å’Œæ–‡æ¡£ç±»å‹çš„ç¼“å­˜
            keys_to_delete = [k for k in self.cache.keys() if k.startswith(f"{company_code}:{doc_type_code}:")]
            for key in keys_to_delete:
                del self.cache[key]
            logger.debug(f"ğŸ—‘ï¸ å·²æ¸…ç†ç¼“å­˜: {company_code}:{doc_type_code}")
        else:
            # æ¸…ç©ºæ‰€æœ‰ç¼“å­˜
            self.cache.clear()
            logger.debug("ğŸ—‘ï¸ å·²æ¸…ç©ºæ‰€æœ‰ç¼“å­˜")
    
    def get_stats(self) -> dict:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "total_items": len(self.cache),
            "max_size": self.max_size,
            "ttl_minutes": self.ttl.total_seconds() / 60,
            "items": [
                {
                    "key": key,
                    "cached_at": item["cached_at"].isoformat(),
                    "size_bytes": len(str(item["content"]))
                }
                for key, item in self.cache.items()
            ]
        }


class PromptSchemaValidator:
    """Promptå’ŒSchemaéªŒè¯å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–éªŒè¯å™¨"""
        self.config = {
            "strict_mode": True,
            "prompt_min_length": 10,
            "prompt_max_length": 50000,
            "required_prompt_keywords": ["extract", "analyze", "identify", "process"],
            "schema_required_fields": ["type", "properties"]
        }
    
    def update_config(self, config: dict):
        """æ›´æ–°éªŒè¯å™¨é…ç½®"""
        self.config.update(config)
    
    def validate_prompt(self, content: str) -> Tuple[bool, str]:
        """éªŒè¯promptå†…å®¹"""
        try:
            if not content or not content.strip():
                return False, "Promptå†…å®¹ä¸èƒ½ä¸ºç©º"
            
            # æ£€æŸ¥æœ€å°é•¿åº¦
            min_length = self.config.get("prompt_min_length", 10)
            if len(content.strip()) < min_length:
                return False, f"Promptå†…å®¹è¿‡çŸ­ï¼ˆæœ€å°‘{min_length}ä¸ªå­—ç¬¦ï¼‰"
            
            # æ£€æŸ¥æœ€å¤§é•¿åº¦
            max_length = self.config.get("prompt_max_length", 50000)
            if len(content) > max_length:
                return False, f"Promptå†…å®¹è¿‡é•¿ï¼ˆæœ€å¤§{max_length}ä¸ªå­—ç¬¦ï¼‰"
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«åŸºæœ¬çš„æŒ‡ä»¤è¯æ±‡ï¼ˆå¦‚æœå¯ç”¨ä¸¥æ ¼æ¨¡å¼ï¼‰
            if self.config.get("strict_mode", True):
                required_keywords = self.config.get("required_prompt_keywords", [])
                # æ·»åŠ ä¸­æ–‡å…³é”®è¯
                all_keywords = required_keywords + ["è¯·", "æå–", "åˆ†æ", "è¯†åˆ«"]
                if not any(keyword.lower() in content.lower() for keyword in all_keywords):
                    if self.config.get("strict_mode", True):
                        return False, f"Promptå¿…é¡»åŒ…å«ä»¥ä¸‹æŒ‡ä»¤å…³é”®è¯ä¹‹ä¸€: {', '.join(required_keywords)}"
                    else:
                        logger.warning("âš ï¸ Promptä¼¼ä¹ä¸åŒ…å«å¸¸è§çš„æŒ‡ä»¤å…³é”®è¯")
            
            return True, "PromptéªŒè¯é€šè¿‡"
            
        except Exception as e:
            return False, f"PromptéªŒè¯å¤±è´¥: {e}"
    
    def validate_schema(self, schema_data: dict) -> Tuple[bool, str]:
        """éªŒè¯schemaæ ¼å¼"""
        try:
            if not isinstance(schema_data, dict):
                return False, "Schemaå¿…é¡»æ˜¯JSONå¯¹è±¡"
            
            # æ£€æŸ¥å¿…éœ€çš„å­—æ®µ
            required_fields = self.config.get("schema_required_fields", ["type", "properties"])
            for field in required_fields:
                if field not in schema_data:
                    return False, f"Schemaç¼ºå°‘å¿…éœ€å­—æ®µ: {field}"
            
            # éªŒè¯typeå­—æ®µï¼ˆå¦‚æœéœ€è¦ï¼‰
            if "type" in required_fields and schema_data.get("type") != "object":
                return False, "Schemaçš„typeå­—æ®µå¿…é¡»æ˜¯'object'"
            
            # éªŒè¯propertieså­—æ®µï¼ˆå¦‚æœéœ€è¦ï¼‰
            if "properties" in required_fields:
                properties = schema_data.get("properties", {})
                if not isinstance(properties, dict):
                    return False, "Schemaçš„propertieså­—æ®µå¿…é¡»æ˜¯å¯¹è±¡"
                
                if self.config.get("strict_mode", True) and len(properties) == 0:
                    return False, "Schemaçš„propertiesä¸èƒ½ä¸ºç©º"
                
                # æ£€æŸ¥æ¯ä¸ªå±æ€§çš„æ ¼å¼
                for prop_name, prop_def in properties.items():
                    if not isinstance(prop_def, dict):
                        return False, f"å±æ€§'{prop_name}'çš„å®šä¹‰å¿…é¡»æ˜¯å¯¹è±¡"
                    
                    if self.config.get("strict_mode", True) and "type" not in prop_def:
                        return False, f"å±æ€§'{prop_name}'ç¼ºå°‘typeå­—æ®µ"
                    
                    if "type" in prop_def:
                        valid_types = ["string", "number", "integer", "boolean", "array", "object"]
                        if prop_def["type"] not in valid_types:
                            return False, f"å±æ€§'{prop_name}'çš„typeå€¼æ— æ•ˆ: {prop_def['type']}"
            
            return True, "SchemaéªŒè¯é€šè¿‡"
            
        except Exception as e:
            return False, f"SchemaéªŒè¯å¤±è´¥: {e}"


class PromptSchemaManager:
    """Promptå’ŒSchemaç»Ÿä¸€ç®¡ç†å™¨"""
    
    def __init__(self, config: Optional[dict] = None):
        """
        åˆå§‹åŒ–ç®¡ç†å™¨
        
        Args:
            config: å¯é€‰çš„é…ç½®å­—å…¸ï¼Œå¦‚æœæœªæä¾›åˆ™ä»config_loaderåŠ è½½
        """
        # åŠ è½½é…ç½®
        if config is None:
            try:
                from config_loader import config_loader
                self.config = config_loader.get_prompt_schema_config()
            except ImportError:
                # å¦‚æœconfig_loaderä¸å¯ç”¨ï¼Œä½¿ç”¨é»˜è®¤é…ç½®
                self.config = self._get_default_config()
        else:
            self.config = config
            
        # åˆå§‹åŒ–æ•°æ®åº“ä¼šè¯ï¼ˆç”¨äºæŸ¥è¯¢é…ç½®è·¯å¾„ï¼‰
        self.db_session = None
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.s3_manager = get_s3_manager()
        
        # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦å¯ç”¨ç¼“å­˜
        cache_config = self.config.get("cache", {})
        if cache_config.get("enabled", True):
            self.cache = PromptSchemaCache(
                cache_config.get("max_size", 100), 
                cache_config.get("ttl_minutes", 30)
            )
        else:
            self.cache = None
        
        self.validator = PromptSchemaValidator()
        
        # é…ç½®çº¿ç¨‹æ± 
        performance_config = self.config.get("performance", {})
        thread_pool_size = performance_config.get("thread_pool_size", 4)
        self.executor = ThreadPoolExecutor(max_workers=thread_pool_size)
        
        # æœ¬åœ°å¤‡ä»½è·¯å¾„
        backup_config = self.config.get("local_backup", {})
        backup_path = backup_config.get("path", "uploads/prompt_schema_backup")
        self.local_backup_path = Path(backup_path)
        
        if backup_config.get("enabled", True):
            self.local_backup_path.mkdir(parents=True, exist_ok=True)
        
        # æ›´æ–°éªŒè¯å™¨é…ç½®
        validation_config = self.config.get("validation", {})
        self.validator.update_config(validation_config)
        
        logger.info("âœ… PromptSchemaManageråˆå§‹åŒ–å®Œæˆ")
        logger.info(f"   - å­˜å‚¨åç«¯: {self.config.get('storage_backend', 'auto')}")
        logger.info(f"   - S3å¯ç”¨: {is_s3_enabled()}")
        logger.info(f"   - ç¼“å­˜å¯ç”¨: {self.cache is not None}")
        logger.info(f"   - æœ¬åœ°å¤‡ä»½: {backup_config.get('enabled', True)}")
    
    def _get_default_config(self) -> dict:
        """è·å–é»˜è®¤é…ç½®"""
        return {
            "storage_backend": "auto",
            "cache": {"enabled": True, "max_size": 100, "ttl_minutes": 30},
            "s3": {
                "enabled": True, "bucket_name": None, "region": "ap-southeast-1",
                "prompt_prefix": "prompts/", "schema_prefix": "schemas/",
                "auto_backup": True, "encryption": True
            },
            "local_backup": {"enabled": True, "path": "uploads/prompt_schema_backup", "auto_sync": True},
            "validation": {
                "strict_mode": True, "prompt_min_length": 10, "prompt_max_length": 50000,
                "required_prompt_keywords": ["extract", "analyze", "identify", "process"],
                "schema_required_fields": ["type", "properties"]
            },
            "performance": {
                "thread_pool_size": 4, "concurrent_uploads": 3,
                "retry_attempts": 3, "timeout_seconds": 30
            }
        }
    
    def _get_local_path(self, company_code: str, doc_type_code: str, filename: str, file_type: str) -> Path:
        """ç”Ÿæˆæœ¬åœ°æ–‡ä»¶è·¯å¾„"""
        return self.local_backup_path / company_code / doc_type_code / file_type / filename
    
    def _save_local_backup(self, company_code: str, doc_type_code: str, filename: str, 
                          file_type: str, content: Union[str, dict]) -> bool:
        """ä¿å­˜æœ¬åœ°å¤‡ä»½"""
        try:
            local_path = self._get_local_path(company_code, doc_type_code, filename, file_type)
            local_path.parent.mkdir(parents=True, exist_ok=True)
            
            if file_type == "prompt":
                local_path.write_text(content, encoding="utf-8")
            else:  # schema
                local_path.write_text(json.dumps(content, ensure_ascii=False, indent=2), encoding="utf-8")
            
            logger.debug(f"ğŸ’¾ æœ¬åœ°å¤‡ä»½å·²ä¿å­˜: {local_path}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜æœ¬åœ°å¤‡ä»½å¤±è´¥: {e}")
            return False
    
    def _load_local_backup(self, company_code: str, doc_type_code: str, filename: str, 
                          file_type: str) -> Optional[Union[str, dict]]:
        """åŠ è½½æœ¬åœ°å¤‡ä»½"""
        try:
            local_path = self._get_local_path(company_code, doc_type_code, filename, file_type)
            
            if not local_path.exists():
                return None
            
            if file_type == "prompt":
                content = local_path.read_text(encoding="utf-8")
                return content
            else:  # schema
                content = local_path.read_text(encoding="utf-8")
                return json.loads(content)
                
        except Exception as e:
            logger.error(f"âŒ åŠ è½½æœ¬åœ°å¤‡ä»½å¤±è´¥: {e}")
            return None
    
    def _get_db_session(self):
        """è·å–æ•°æ®åº“ä¼šè¯"""
        if self.db_session is None:
            try:
                from db.database import get_db
                self.db_session = next(get_db())
            except Exception as e:
                logger.error(f"âŒ è·å–æ•°æ®åº“ä¼šè¯å¤±è´¥: {e}")
                return None
        return self.db_session
    
    def _get_config_paths(self, company_code: str, doc_type_code: str) -> Tuple[Optional[str], Optional[str]]:
        """
        ä»æ•°æ®åº“è·å–å…¬å¸æ–‡æ¡£é…ç½®çš„è·¯å¾„
        
        Args:
            company_code: å…¬å¸ä»£ç 
            doc_type_code: æ–‡æ¡£ç±»å‹ä»£ç 
            
        Returns:
            Tuple[Optional[str], Optional[str]]: (prompt_path, schema_path)
        """
        try:
            db = self._get_db_session()
            if db is None:
                return None, None
                
            from db.models import Company, DocumentType, CompanyDocumentConfig
            
            # æŸ¥è¯¢é…ç½®
            config = db.query(CompanyDocumentConfig).join(
                Company, CompanyDocumentConfig.company_id == Company.company_id
            ).join(
                DocumentType, CompanyDocumentConfig.doc_type_id == DocumentType.doc_type_id
            ).filter(
                Company.company_code == company_code,
                DocumentType.type_code == doc_type_code,
                CompanyDocumentConfig.active == True
            ).first()
            
            if config:
                return config.prompt_path, config.schema_path
            else:
                logger.warning(f"âš ï¸ æœªæ‰¾åˆ°é…ç½®: {company_code}/{doc_type_code}")
                return None, None
                
        except Exception as e:
            logger.error(f"âŒ æŸ¥è¯¢æ•°æ®åº“é…ç½®å¤±è´¥: {e}")
            return None, None
    
    def _extract_s3_key_from_path(self, s3_path: str) -> Optional[str]:
        """
        ä»å®Œæ•´S3è·¯å¾„ä¸­æå–key
        
        Args:
            s3_path: å®Œæ•´S3è·¯å¾„ï¼Œå¦‚ s3://bucket/key/path
            
        Returns:
            Optional[str]: S3 keyéƒ¨åˆ†ï¼Œå¦‚ key/path
        """
        try:
            if not s3_path or not s3_path.startswith('s3://'):
                return None
                
            # ç§»é™¤s3://å‰ç¼€
            path_without_protocol = s3_path[5:]
            
            # æ‰¾åˆ°ç¬¬ä¸€ä¸ª/ï¼Œåˆ†ç¦»bucketå’Œkey
            slash_index = path_without_protocol.find('/')
            if slash_index == -1:
                return None
                
            # è¿”å›keyéƒ¨åˆ†
            key = path_without_protocol[slash_index + 1:]
            return key if key else None
            
        except Exception as e:
            logger.error(f"âŒ è§£æS3è·¯å¾„å¤±è´¥: {e}")
            return None
    
    async def get_prompt(self, company_code: str, doc_type_code: str, filename: str = "prompt.txt") -> Optional[str]:
        """
        è·å–promptå†…å®¹
        
        Args:
            company_code: å…¬å¸ä»£ç 
            doc_type_code: æ–‡æ¡£ç±»å‹ä»£ç 
            filename: æ–‡ä»¶å
            
        Returns:
            Optional[str]: promptå†…å®¹
        """
        try:
            # 1. å°è¯•ä»ç¼“å­˜è·å–
            if self.cache:
                cached_content = self.cache.get(company_code, doc_type_code, "prompt", filename)
                if cached_content is not None:
                    return cached_content
            
            # 2. ä»æ•°æ®åº“è·å–å®é™…é…ç½®è·¯å¾„
            prompt_path, _ = self._get_config_paths(company_code, doc_type_code)
            
            if prompt_path and self.s3_manager:
                # 2a. å¦‚æœæ˜¯S3è·¯å¾„ï¼Œç›´æ¥ä»S3è¯»å–
                if prompt_path.startswith('s3://'):
                    s3_key = self._extract_s3_key_from_path(prompt_path)
                    if s3_key:
                        content = await asyncio.get_event_loop().run_in_executor(
                            self.executor,
                            self.s3_manager.get_file_by_key,
                            s3_key
                        )
                        
                        if content is not None:
                            # éªŒè¯å†…å®¹
                            is_valid, message = self.validator.validate_prompt(content)
                            if not is_valid:
                                logger.warning(f"âš ï¸ PromptéªŒè¯å¤±è´¥: {message}")
                            
                            # ç¼“å­˜ç»“æœ
                            if self.cache:
                                self.cache.set(company_code, doc_type_code, "prompt", filename, content)
                            
                            # ä¿å­˜æœ¬åœ°å¤‡ä»½
                            self._save_local_backup(company_code, doc_type_code, filename, "prompt", content)
                            
                            return content
                
                # 2b. å›é€€åˆ°æ—§çš„çº¦å®šè·¯å¾„æ–¹å¼
                content = await asyncio.get_event_loop().run_in_executor(
                    self.executor,
                    self.s3_manager.get_prompt,
                    company_code, doc_type_code, filename
                )
                
                if content is not None:
                    # éªŒè¯å†…å®¹
                    is_valid, message = self.validator.validate_prompt(content)
                    if not is_valid:
                        logger.warning(f"âš ï¸ PromptéªŒè¯å¤±è´¥: {message}")
                    
                    # ç¼“å­˜ç»“æœ
                    if self.cache:
                        self.cache.set(company_code, doc_type_code, "prompt", filename, content)
                    
                    # ä¿å­˜æœ¬åœ°å¤‡ä»½
                    self._save_local_backup(company_code, doc_type_code, filename, "prompt", content)
                    
                    return content
            
            # 3. å›é€€åˆ°æœ¬åœ°å¤‡ä»½
            content = self._load_local_backup(company_code, doc_type_code, filename, "prompt")
            if content is not None:
                logger.info(f"ğŸ“‚ ä½¿ç”¨æœ¬åœ°å¤‡ä»½prompt: {company_code}/{doc_type_code}/{filename}")
                
                # ç¼“å­˜ç»“æœ
                if self.cache:
                    self.cache.set(company_code, doc_type_code, "prompt", filename, content)
                
                return content
            
            logger.error(f"âŒ æœªæ‰¾åˆ°promptæ–‡ä»¶: {company_code}/{doc_type_code}/{filename}")
            return None
            
        except Exception as e:
            logger.error(f"âŒ è·å–promptå¤±è´¥: {e}")
            return None
    
    async def get_schema(self, company_code: str, doc_type_code: str, filename: str = "schema.json") -> Optional[dict]:
        """
        è·å–schemaæ•°æ®
        
        Args:
            company_code: å…¬å¸ä»£ç 
            doc_type_code: æ–‡æ¡£ç±»å‹ä»£ç 
            filename: æ–‡ä»¶å
            
        Returns:
            Optional[dict]: schemaæ•°æ®
        """
        try:
            # 1. å°è¯•ä»ç¼“å­˜è·å–
            if self.cache:
                cached_content = self.cache.get(company_code, doc_type_code, "schema", filename)
                if cached_content is not None:
                    return cached_content
            
            # 2. ä»æ•°æ®åº“è·å–å®é™…é…ç½®è·¯å¾„
            _, schema_path = self._get_config_paths(company_code, doc_type_code)
            
            if schema_path and self.s3_manager:
                # 2a. å¦‚æœæ˜¯S3è·¯å¾„ï¼Œç›´æ¥ä»S3è¯»å–
                if schema_path.startswith('s3://'):
                    s3_key = self._extract_s3_key_from_path(schema_path)
                    if s3_key:
                        content = await asyncio.get_event_loop().run_in_executor(
                            self.executor,
                            self.s3_manager.get_schema_by_key,
                            s3_key
                        )
                        
                        if content is not None:
                            # éªŒè¯å†…å®¹
                            is_valid, message = self.validator.validate_schema(content)
                            if not is_valid:
                                logger.warning(f"âš ï¸ SchemaéªŒè¯å¤±è´¥: {message}")
                            
                            # ç¼“å­˜ç»“æœ
                            if self.cache:
                                self.cache.set(company_code, doc_type_code, "schema", filename, content)
                            
                            # ä¿å­˜æœ¬åœ°å¤‡ä»½
                            self._save_local_backup(company_code, doc_type_code, filename, "schema", content)
                            
                            return content
                
                # 2b. å›é€€åˆ°æ—§çš„çº¦å®šè·¯å¾„æ–¹å¼
                content = await asyncio.get_event_loop().run_in_executor(
                    self.executor,
                    self.s3_manager.get_schema,
                    company_code, doc_type_code, filename
                )
                
                if content is not None:
                    # éªŒè¯å†…å®¹
                    is_valid, message = self.validator.validate_schema(content)
                    if not is_valid:
                        logger.warning(f"âš ï¸ SchemaéªŒè¯å¤±è´¥: {message}")
                    
                    # ç¼“å­˜ç»“æœ
                    if self.cache:
                        self.cache.set(company_code, doc_type_code, "schema", filename, content)
                    
                    # ä¿å­˜æœ¬åœ°å¤‡ä»½
                    self._save_local_backup(company_code, doc_type_code, filename, "schema", content)
                    
                    return content
            
            # 3. å›é€€åˆ°æœ¬åœ°å¤‡ä»½
            content = self._load_local_backup(company_code, doc_type_code, filename, "schema")
            if content is not None:
                logger.info(f"ğŸ“‚ ä½¿ç”¨æœ¬åœ°å¤‡ä»½schema: {company_code}/{doc_type_code}/{filename}")
                
                # ç¼“å­˜ç»“æœ
                if self.cache:
                    self.cache.set(company_code, doc_type_code, "schema", filename, content)
                
                return content
            
            logger.error(f"âŒ æœªæ‰¾åˆ°schemaæ–‡ä»¶: {company_code}/{doc_type_code}/{filename}")
            return None
            
        except Exception as e:
            logger.error(f"âŒ è·å–schemaå¤±è´¥: {e}")
            return None
    
    async def upload_prompt(self, company_code: str, doc_type_code: str, content: str, 
                           filename: str = "prompt.txt", metadata: Optional[dict] = None) -> bool:
        """
        ä¸Šä¼ prompt
        
        Args:
            company_code: å…¬å¸ä»£ç 
            doc_type_code: æ–‡æ¡£ç±»å‹ä»£ç 
            content: promptå†…å®¹
            filename: æ–‡ä»¶å
            metadata: å…ƒæ•°æ®
            
        Returns:
            bool: ä¸Šä¼ æ˜¯å¦æˆåŠŸ
        """
        try:
            # éªŒè¯å†…å®¹
            is_valid, message = self.validator.validate_prompt(content)
            if not is_valid:
                logger.error(f"âŒ PromptéªŒè¯å¤±è´¥: {message}")
                return False
            
            # ä¸Šä¼ åˆ°S3
            success = False
            if self.s3_manager:
                s3_key = await asyncio.get_event_loop().run_in_executor(
                    self.executor,
                    self.s3_manager.upload_prompt,
                    company_code, doc_type_code, content, filename, metadata
                )
                success = s3_key is not None
            
            # ä¿å­˜æœ¬åœ°å¤‡ä»½ï¼ˆæ— è®ºS3æ˜¯å¦æˆåŠŸï¼‰
            self._save_local_backup(company_code, doc_type_code, filename, "prompt", content)
            
            # æ¸…é™¤ç¼“å­˜
            if self.cache:
                self.cache.invalidate(company_code, doc_type_code)
            
            if success:
                logger.info(f"âœ… Promptä¸Šä¼ æˆåŠŸ: {company_code}/{doc_type_code}/{filename}")
            else:
                logger.warning(f"âš ï¸ S3ä¸Šä¼ å¤±è´¥ï¼Œä½†æœ¬åœ°å¤‡ä»½å·²ä¿å­˜: {company_code}/{doc_type_code}/{filename}")
            
            return True  # åªè¦æœ¬åœ°å¤‡ä»½æˆåŠŸå°±è®¤ä¸ºæˆåŠŸ
            
        except Exception as e:
            logger.error(f"âŒ ä¸Šä¼ promptå¤±è´¥: {e}")
            return False
    
    async def upload_schema(self, company_code: str, doc_type_code: str, schema_data: dict, 
                           filename: str = "schema.json", metadata: Optional[dict] = None) -> bool:
        """
        ä¸Šä¼ schema
        
        Args:
            company_code: å…¬å¸ä»£ç 
            doc_type_code: æ–‡æ¡£ç±»å‹ä»£ç 
            schema_data: schemaæ•°æ®
            filename: æ–‡ä»¶å
            metadata: å…ƒæ•°æ®
            
        Returns:
            bool: ä¸Šä¼ æ˜¯å¦æˆåŠŸ
        """
        try:
            # éªŒè¯å†…å®¹
            is_valid, message = self.validator.validate_schema(schema_data)
            if not is_valid:
                logger.error(f"âŒ SchemaéªŒè¯å¤±è´¥: {message}")
                return False
            
            # ä¸Šä¼ åˆ°S3
            success = False
            if self.s3_manager:
                s3_key = await asyncio.get_event_loop().run_in_executor(
                    self.executor,
                    self.s3_manager.upload_schema,
                    company_code, doc_type_code, schema_data, filename, metadata
                )
                success = s3_key is not None
            
            # ä¿å­˜æœ¬åœ°å¤‡ä»½ï¼ˆæ— è®ºS3æ˜¯å¦æˆåŠŸï¼‰
            self._save_local_backup(company_code, doc_type_code, filename, "schema", schema_data)
            
            # æ¸…é™¤ç¼“å­˜
            if self.cache:
                self.cache.invalidate(company_code, doc_type_code)
            
            if success:
                logger.info(f"âœ… Schemaä¸Šä¼ æˆåŠŸ: {company_code}/{doc_type_code}/{filename}")
            else:
                logger.warning(f"âš ï¸ S3ä¸Šä¼ å¤±è´¥ï¼Œä½†æœ¬åœ°å¤‡ä»½å·²ä¿å­˜: {company_code}/{doc_type_code}/{filename}")
            
            return True  # åªè¦æœ¬åœ°å¤‡ä»½æˆåŠŸå°±è®¤ä¸ºæˆåŠŸ
            
        except Exception as e:
            logger.error(f"âŒ ä¸Šä¼ schemaå¤±è´¥: {e}")
            return False
    
    def get_health_status(self) -> dict:
        """è·å–å¥åº·çŠ¶æ€"""
        try:
            s3_status = self.s3_manager.get_health_status() if self.s3_manager else {"status": "disabled"}
            cache_stats = self.cache.get_stats() if self.cache else {"status": "disabled"}
            
            return {
                "status": "healthy",
                "s3_storage": s3_status,
                "cache": cache_stats,
                "local_backup_path": str(self.local_backup_path),
                "backup_files_count": len(list(self.local_backup_path.rglob("*"))) if self.local_backup_path.exists() else 0
            }
            
        except Exception as e:
            return {
                "status": "unhealthy",
                "error": str(e)
            }
    
    async def list_available_templates(self, company_code: Optional[str] = None) -> dict:
        """åˆ—å‡ºå¯ç”¨çš„æ¨¡æ¿"""
        try:
            result = {
                "prompts": [],
                "schemas": []
            }
            
            # ä»S3è·å–
            if self.s3_manager:
                prompts = await asyncio.get_event_loop().run_in_executor(
                    self.executor,
                    self.s3_manager.list_prompts,
                    company_code
                )
                schemas = await asyncio.get_event_loop().run_in_executor(
                    self.executor,
                    self.s3_manager.list_schemas,
                    company_code
                )
                
                result["prompts"].extend(prompts)
                result["schemas"].extend(schemas)
            
            # è¡¥å……æœ¬åœ°å¤‡ä»½ä¸­çš„æ–‡ä»¶
            if self.local_backup_path.exists():
                for company_dir in self.local_backup_path.iterdir():
                    if company_dir.is_dir() and (not company_code or company_dir.name == company_code):
                        for doc_type_dir in company_dir.iterdir():
                            if doc_type_dir.is_dir():
                                # æ£€æŸ¥prompts
                                prompt_dir = doc_type_dir / "prompt"
                                if prompt_dir.exists():
                                    for prompt_file in prompt_dir.glob("*.txt"):
                                        result["prompts"].append({
                                            "key": f"{company_dir.name}/{doc_type_dir.name}/{prompt_file.name}",
                                            "source": "local_backup",
                                            "size": prompt_file.stat().st_size,
                                            "last_modified": datetime.fromtimestamp(prompt_file.stat().st_mtime)
                                        })
                                
                                # æ£€æŸ¥schemas
                                schema_dir = doc_type_dir / "schema"
                                if schema_dir.exists():
                                    for schema_file in schema_dir.glob("*.json"):
                                        result["schemas"].append({
                                            "key": f"{company_dir.name}/{doc_type_dir.name}/{schema_file.name}",
                                            "source": "local_backup",
                                            "size": schema_file.stat().st_size,
                                            "last_modified": datetime.fromtimestamp(schema_file.stat().st_mtime)
                                        })
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ åˆ—å‡ºæ¨¡æ¿å¤±è´¥: {e}")
            return {"prompts": [], "schemas": []}


# å…¨å±€ç®¡ç†å™¨å®ä¾‹
_prompt_schema_manager = None


def get_prompt_schema_manager() -> PromptSchemaManager:
    """è·å–å…¨å±€PromptSchemaManagerå®ä¾‹"""
    global _prompt_schema_manager
    
    if _prompt_schema_manager is None:
        _prompt_schema_manager = PromptSchemaManager()
    
    return _prompt_schema_manager


def clean_schema_for_gemini(schema):
    """
    Clean JSON schema for Gemini API compatibility by removing unsupported fields.
    
    Args:
        schema: The JSON schema dictionary
        
    Returns:
        Cleaned schema dictionary safe for Gemini API
    """
    if not isinstance(schema, dict):
        return schema
    
    # Fields that cause Gemini API errors
    problematic_fields = ["$schema", "$id", "$ref", "definitions", "patternProperties"]
    
    cleaned_schema = {}
    for key, value in schema.items():
        if key in problematic_fields:
            logger.info(f"Removing problematic schema field for Gemini compatibility: {key}")
            continue
            
        if isinstance(value, dict):
            # Recursively clean nested dictionaries
            cleaned_value = clean_schema_for_gemini(value)
            # Only add if the cleaned value is not empty
            if cleaned_value:
                cleaned_schema[key] = cleaned_value
        elif isinstance(value, list):
            cleaned_schema[key] = [
                clean_schema_for_gemini(item) if isinstance(item, dict) else item
                for item in value
            ]
        else:
            cleaned_schema[key] = value
    
    return cleaned_schema


async def load_prompt_and_schema(company_code: str, doc_type_code: str) -> Tuple[Optional[str], Optional[dict]]:
    """
    ä¾¿æ·å‡½æ•°ï¼šåŒæ—¶åŠ è½½promptå’Œschemaï¼Œå¹¶è‡ªåŠ¨æ¸…ç†schemaä»¥å…¼å®¹Gemini API
    
    Args:
        company_code: å…¬å¸ä»£ç 
        doc_type_code: æ–‡æ¡£ç±»å‹ä»£ç 
        
    Returns:
        Tuple[Optional[str], Optional[dict]]: (promptå†…å®¹, æ¸…ç†åçš„schemaæ•°æ®)
    """
    manager = get_prompt_schema_manager()
    
    # å¹¶è¡ŒåŠ è½½
    prompt_task = manager.get_prompt(company_code, doc_type_code)
    schema_task = manager.get_schema(company_code, doc_type_code)
    
    prompt_content, schema_data = await asyncio.gather(prompt_task, schema_task)
    
    # Clean schema for Gemini API compatibility
    if schema_data:
        schema_data = clean_schema_for_gemini(schema_data)
        logger.debug(f"Schema cleaned for Gemini API compatibility: {company_code}/{doc_type_code}")
    
    return prompt_content, schema_data