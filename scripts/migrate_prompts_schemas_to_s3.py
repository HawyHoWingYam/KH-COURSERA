#!/usr/bin/env python3
"""
Prompt and Schema Migration to S3
å°†ç°æœ‰çš„promptå’Œschemaæ–‡ä»¶è¿ç§»åˆ°S3å­˜å‚¨

åŠŸèƒ½:
- æ‰«ææœ¬åœ°uploads/document_type/ç›®å½•ç»“æ„
- ä¸Šä¼ æ‰€æœ‰.txt promptså’Œ.json schemasåˆ°S3
- æ›´æ–°æ•°æ®åº“ä¸­çš„è·¯å¾„å¼•ç”¨
- ç”Ÿæˆè¯¦ç»†çš„è¿ç§»æŠ¥å‘Š
- æ”¯æŒå›æ»šæ“ä½œ
"""

import os
import sys
import json
import asyncio
import argparse
from pathlib import Path
from typing import List, Dict, Tuple, Optional
from datetime import datetime
import logging
import hashlib
import shutil

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
backend_path = project_root / "GeminiOCR" / "backend"
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(backend_path))

from database_manager import DatabaseManager
from db.models import CompanyDocumentConfig, Company, DocumentType
from utils.s3_storage import get_s3_manager, is_s3_enabled
from utils.prompt_schema_manager import get_prompt_schema_manager

logger = logging.getLogger(__name__)


class MigrationReporter:
    """è¿ç§»æŠ¥å‘Šç”Ÿæˆå™¨"""
    
    def __init__(self):
        self.start_time = datetime.now()
        self.report = {
            "migration_id": self._generate_migration_id(),
            "start_time": self.start_time.isoformat(),
            "end_time": None,
            "status": "running",
            "summary": {
                "total_files_found": 0,
                "total_files_migrated": 0,
                "total_files_failed": 0,
                "prompts_migrated": 0,
                "schemas_migrated": 0,
                "database_updates": 0,
                "database_update_failures": 0
            },
            "files": [],
            "database_updates": [],
            "errors": [],
            "warnings": []
        }
    
    def _generate_migration_id(self) -> str:
        """ç”Ÿæˆå”¯ä¸€çš„è¿ç§»ID"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        random_suffix = hashlib.md5(str(datetime.now().timestamp()).encode()).hexdigest()[:8]
        return f"migrate_s3_{timestamp}_{random_suffix}"
    
    def add_file_result(self, file_path: str, file_type: str, status: str, 
                       s3_key: Optional[str] = None, error: Optional[str] = None):
        """æ·»åŠ æ–‡ä»¶å¤„ç†ç»“æœ"""
        self.report["files"].append({
            "file_path": file_path,
            "file_type": file_type,
            "status": status,
            "s3_key": s3_key,
            "error": error,
            "processed_at": datetime.now().isoformat()
        })
        
        self.report["summary"]["total_files_found"] += 1
        if status == "success":
            self.report["summary"]["total_files_migrated"] += 1
            if file_type == "prompt":
                self.report["summary"]["prompts_migrated"] += 1
            else:
                self.report["summary"]["schemas_migrated"] += 1
        else:
            self.report["summary"]["total_files_failed"] += 1
    
    def add_database_update(self, config_id: int, old_path: str, new_path: str, 
                           path_type: str, status: str, error: Optional[str] = None):
        """æ·»åŠ æ•°æ®åº“æ›´æ–°ç»“æœ"""
        self.report["database_updates"].append({
            "config_id": config_id,
            "old_path": old_path,
            "new_path": new_path,
            "path_type": path_type,
            "status": status,
            "error": error,
            "updated_at": datetime.now().isoformat()
        })
        
        if status == "success":
            self.report["summary"]["database_updates"] += 1
        else:
            self.report["summary"]["database_update_failures"] += 1
    
    def add_error(self, error: str):
        """æ·»åŠ é”™è¯¯ä¿¡æ¯"""
        self.report["errors"].append({
            "error": error,
            "timestamp": datetime.now().isoformat()
        })
    
    def add_warning(self, warning: str):
        """æ·»åŠ è­¦å‘Šä¿¡æ¯"""
        self.report["warnings"].append({
            "warning": warning,
            "timestamp": datetime.now().isoformat()
        })
    
    def finalize(self, status: str):
        """å®ŒæˆæŠ¥å‘Š"""
        self.report["end_time"] = datetime.now().isoformat()
        self.report["status"] = status
        self.report["duration_seconds"] = (datetime.now() - self.start_time).total_seconds()
    
    def save_report(self, output_dir: Path):
        """ä¿å­˜æŠ¥å‘Š"""
        try:
            output_dir.mkdir(parents=True, exist_ok=True)
            report_file = output_dir / f"{self.report['migration_id']}_report.json"
            
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(self.report, f, indent=2, ensure_ascii=False)
            
            logger.info(f"âœ… è¿ç§»æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
            return report_file
            
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜è¿ç§»æŠ¥å‘Šå¤±è´¥: {e}")
            return None
    
    def print_summary(self):
        """æ‰“å°æ‘˜è¦"""
        print("\n" + "="*60)
        print(f"ğŸ“Š è¿ç§»æ‘˜è¦ - {self.report['migration_id']}")
        print("="*60)
        print(f"å¼€å§‹æ—¶é—´: {self.report['start_time']}")
        print(f"ç»“æŸæ—¶é—´: {self.report['end_time']}")
        print(f"æ€»è€—æ—¶: {self.report.get('duration_seconds', 0):.2f} ç§’")
        print(f"çŠ¶æ€: {self.report['status']}")
        print()
        
        summary = self.report["summary"]
        print(f"ğŸ“ æ–‡ä»¶è¿ç§»:")
        print(f"  å‘ç°æ–‡ä»¶: {summary['total_files_found']}")
        print(f"  æˆåŠŸè¿ç§»: {summary['total_files_migrated']}")
        print(f"  å¤±è´¥æ–‡ä»¶: {summary['total_files_failed']}")
        print(f"  - Prompts: {summary['prompts_migrated']}")
        print(f"  - Schemas: {summary['schemas_migrated']}")
        print()
        
        print(f"ğŸ—„ï¸ æ•°æ®åº“æ›´æ–°:")
        print(f"  æˆåŠŸæ›´æ–°: {summary['database_updates']}")
        print(f"  æ›´æ–°å¤±è´¥: {summary['database_update_failures']}")
        print()
        
        if self.report["errors"]:
            print(f"âŒ é”™è¯¯æ•°é‡: {len(self.report['errors'])}")
        
        if self.report["warnings"]:
            print(f"âš ï¸ è­¦å‘Šæ•°é‡: {len(self.report['warnings'])}")
        
        print("="*60)


class PromptSchemaMigrator:
    """Promptå’ŒSchemaè¿ç§»å™¨"""
    
    def __init__(self, dry_run: bool = False, backup_enabled: bool = True):
        """
        åˆå§‹åŒ–è¿ç§»å™¨
        
        Args:
            dry_run: æ˜¯å¦ä¸ºè¯•è¿è¡Œæ¨¡å¼
            backup_enabled: æ˜¯å¦å¯ç”¨å¤‡ä»½
        """
        self.dry_run = dry_run
        self.backup_enabled = backup_enabled
        self.db_manager = None
        self.s3_manager = get_s3_manager()
        self.prompt_schema_manager = get_prompt_schema_manager()
        self.reporter = MigrationReporter()
        
        # æºç›®å½•è·¯å¾„
        self.source_dir = backend_path / "uploads" / "document_type"
        
        # å¤‡ä»½ç›®å½•
        if backup_enabled:
            self.backup_dir = project_root / "migration_backups" / self.reporter.report["migration_id"]
            self.backup_dir.mkdir(parents=True, exist_ok=True)
    
    async def initialize(self):
        """åˆå§‹åŒ–æ•°æ®åº“è¿æ¥"""
        try:
            environment = os.getenv('DATABASE_ENV', 'local')
            self.db_manager = DatabaseManager(environment)
            await self.db_manager.initialize()
            logger.info(f"âœ… æ•°æ®åº“è¿æ¥å·²å»ºç«‹: {environment}")
            
        except Exception as e:
            logger.error(f"âŒ æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
            raise
    
    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        if self.db_manager:
            await self.db_manager.close()
    
    def _discover_files(self) -> List[Dict]:
        """å‘ç°éœ€è¦è¿ç§»çš„æ–‡ä»¶"""
        logger.info(f"ğŸ” æ‰«æç›®å½•: {self.source_dir}")
        
        if not self.source_dir.exists():
            logger.error(f"âŒ æºç›®å½•ä¸å­˜åœ¨: {self.source_dir}")
            return []
        
        files_to_migrate = []
        
        for doc_type_dir in self.source_dir.iterdir():
            if not doc_type_dir.is_dir():
                continue
                
            logger.debug(f"ğŸ“‚ æ‰«ææ–‡æ¡£ç±»å‹ç›®å½•: {doc_type_dir.name}")
            
            for provider_dir in doc_type_dir.iterdir():
                if not provider_dir.is_dir():
                    continue
                
                logger.debug(f"ğŸ“‚ æ‰«ææä¾›å•†ç›®å½•: {provider_dir.name}")
                
                # æ‰«æpromptæ–‡ä»¶
                prompt_dir = provider_dir / "prompt"
                if prompt_dir.exists():
                    for prompt_file in prompt_dir.glob("*.txt"):
                        files_to_migrate.append({
                            "file_path": prompt_file,
                            "file_type": "prompt",
                            "doc_type": doc_type_dir.name,
                            "provider": provider_dir.name,
                            "filename": prompt_file.name,
                            "company_code": provider_dir.name,  # å‡è®¾provideråå°±æ˜¯company_code
                            "doc_type_code": doc_type_dir.name
                        })
                
                # æ‰«æschemaæ–‡ä»¶
                schema_dir = provider_dir / "schema"
                if schema_dir.exists():
                    for schema_file in schema_dir.glob("*.json"):
                        files_to_migrate.append({
                            "file_path": schema_file,
                            "file_type": "schema",
                            "doc_type": doc_type_dir.name,
                            "provider": provider_dir.name,
                            "filename": schema_file.name,
                            "company_code": provider_dir.name,  # å‡è®¾provideråå°±æ˜¯company_code
                            "doc_type_code": doc_type_dir.name
                        })
        
        logger.info(f"ğŸ“Š å‘ç° {len(files_to_migrate)} ä¸ªæ–‡ä»¶éœ€è¦è¿ç§»")
        return files_to_migrate
    
    def _create_backup(self, file_path: Path) -> bool:
        """åˆ›å»ºæ–‡ä»¶å¤‡ä»½"""
        if not self.backup_enabled:
            return True
        
        try:
            # ä¿æŒåŸæœ‰çš„ç›®å½•ç»“æ„
            relative_path = file_path.relative_to(backend_path)
            backup_path = self.backup_dir / relative_path
            backup_path.parent.mkdir(parents=True, exist_ok=True)
            
            shutil.copy2(file_path, backup_path)
            logger.debug(f"ğŸ’¾ å¤‡ä»½å·²åˆ›å»º: {backup_path}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ åˆ›å»ºå¤‡ä»½å¤±è´¥: {e}")
            return False
    
    async def _migrate_file(self, file_info: Dict) -> Tuple[bool, Optional[str], Optional[str]]:
        """
        è¿ç§»å•ä¸ªæ–‡ä»¶
        
        Returns:
            Tuple[bool, Optional[str], Optional[str]]: (æˆåŠŸçŠ¶æ€, S3é”®å, é”™è¯¯ä¿¡æ¯)
        """
        file_path = file_info["file_path"]
        file_type = file_info["file_type"]
        company_code = file_info["company_code"]
        doc_type_code = file_info["doc_type_code"]
        filename = file_info["filename"]
        
        try:
            # åˆ›å»ºå¤‡ä»½
            if not self._create_backup(file_path):
                return False, None, "å¤‡ä»½åˆ›å»ºå¤±è´¥"
            
            # è¯»å–æ–‡ä»¶å†…å®¹
            if file_type == "prompt":
                content = file_path.read_text(encoding='utf-8')
                
                if self.dry_run:
                    logger.info(f"ğŸ”„ [DRY RUN] å°†è¿ç§»prompt: {file_path}")
                    return True, f"s3://prompts/{company_code}/{doc_type_code}/{filename}", None
                
                # ä¸Šä¼ åˆ°S3
                success = await self.prompt_schema_manager.upload_prompt(
                    company_code, doc_type_code, content, filename
                )
                
            else:  # schema
                with open(file_path, 'r', encoding='utf-8') as f:
                    schema_data = json.load(f)
                
                if self.dry_run:
                    logger.info(f"ğŸ”„ [DRY RUN] å°†è¿ç§»schema: {file_path}")
                    return True, f"s3://schemas/{company_code}/{doc_type_code}/{filename}", None
                
                # ä¸Šä¼ åˆ°S3
                success = await self.prompt_schema_manager.upload_schema(
                    company_code, doc_type_code, schema_data, filename
                )
            
            if success:
                s3_key = f"s3://{file_type}s/{company_code}/{doc_type_code}/{filename}"
                logger.info(f"âœ… æ–‡ä»¶è¿ç§»æˆåŠŸ: {file_path} -> {s3_key}")
                return True, s3_key, None
            else:
                return False, None, "S3ä¸Šä¼ å¤±è´¥"
                
        except Exception as e:
            logger.error(f"âŒ æ–‡ä»¶è¿ç§»å¤±è´¥: {file_path}, é”™è¯¯: {e}")
            return False, None, str(e)
    
    def _map_paths_to_company_codes(self) -> Dict[str, str]:
        """
        æ˜ å°„æœ¬åœ°è·¯å¾„åˆ°å…¬å¸ä»£ç 
        è¿™é‡Œéœ€è¦æ ¹æ®å®é™…çš„å‘½åè§„åˆ™è¿›è¡Œè°ƒæ•´
        """
        path_mapping = {
            # ç¤ºä¾‹æ˜ å°„ï¼Œéœ€è¦æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´
            "hana": "HANA",
            "hkpc": "HKPC",
            "[Finance]_hkbn_billing": "HKBN_FINANCE",
            "assembly_built": "ASSEMBLY",
            "invoice": "INVOICE"
        }
        return path_mapping
    
    async def _update_database_paths(self, migrated_files: List[Dict]):
        """æ›´æ–°æ•°æ®åº“ä¸­çš„è·¯å¾„å¼•ç”¨"""
        logger.info("ğŸ—„ï¸ å¼€å§‹æ›´æ–°æ•°æ®åº“è·¯å¾„å¼•ç”¨")
        
        try:
            async with self.db_manager.get_async_connection() as conn:
                # è·å–æ‰€æœ‰é…ç½®
                configs = await conn.fetch("""
                    SELECT cdc.config_id, cdc.company_id, cdc.doc_type_id, 
                           cdc.prompt_path, cdc.schema_path,
                           c.company_code, dt.type_code
                    FROM company_document_configs cdc
                    JOIN companies c ON cdc.company_id = c.company_id
                    JOIN document_types dt ON cdc.doc_type_id = dt.doc_type_id
                    WHERE cdc.active = true
                """)
                
                path_mapping = self._map_paths_to_company_codes()
                
                for config in configs:
                    config_id = config['config_id']
                    current_prompt_path = config['prompt_path']
                    current_schema_path = config['schema_path']
                    company_code = config['company_code']
                    doc_type_code = config['type_code']
                    
                    # æŸ¥æ‰¾å¯¹åº”çš„è¿ç§»æ–‡ä»¶
                    new_prompt_path = None
                    new_schema_path = None
                    
                    for file_info in migrated_files:
                        if (file_info['status'] == 'success' and 
                            file_info['company_code'] == company_code and
                            file_info['doc_type_code'] == doc_type_code):
                            
                            if file_info['file_type'] == 'prompt':
                                new_prompt_path = file_info['s3_key']
                            elif file_info['file_type'] == 'schema':
                                new_schema_path = file_info['s3_key']
                    
                    # æ›´æ–°æ•°æ®åº“
                    updates_made = False
                    
                    if new_prompt_path and current_prompt_path != new_prompt_path:
                        if not self.dry_run:
                            await conn.execute("""
                                UPDATE company_document_configs 
                                SET prompt_path = $1, updated_at = NOW()
                                WHERE config_id = $2
                            """, new_prompt_path, config_id)
                        
                        self.reporter.add_database_update(
                            config_id, current_prompt_path, new_prompt_path, "prompt", "success"
                        )
                        updates_made = True
                        logger.info(f"âœ… æ›´æ–°promptè·¯å¾„: {config_id} -> {new_prompt_path}")
                    
                    if new_schema_path and current_schema_path != new_schema_path:
                        if not self.dry_run:
                            await conn.execute("""
                                UPDATE company_document_configs 
                                SET schema_path = $1, updated_at = NOW()
                                WHERE config_id = $2
                            """, new_schema_path, config_id)
                        
                        self.reporter.add_database_update(
                            config_id, current_schema_path, new_schema_path, "schema", "success"
                        )
                        updates_made = True
                        logger.info(f"âœ… æ›´æ–°schemaè·¯å¾„: {config_id} -> {new_schema_path}")
                    
                    if not updates_made:
                        logger.debug(f"ğŸ”„ é…ç½® {config_id} æ— éœ€æ›´æ–°")
                
        except Exception as e:
            logger.error(f"âŒ æ•°æ®åº“æ›´æ–°å¤±è´¥: {e}")
            self.reporter.add_error(f"æ•°æ®åº“æ›´æ–°å¤±è´¥: {e}")
    
    async def run_migration(self) -> bool:
        """è¿è¡Œè¿ç§»"""
        try:
            logger.info(f"ğŸš€ å¼€å§‹è¿ç§» {'(è¯•è¿è¡Œæ¨¡å¼)' if self.dry_run else ''}")
            
            # æ£€æŸ¥S3è¿æ¥
            if not is_s3_enabled():
                error_msg = "S3å­˜å‚¨æœªå¯ç”¨ï¼Œæ— æ³•è¿›è¡Œè¿ç§»"
                logger.error(f"âŒ {error_msg}")
                self.reporter.add_error(error_msg)
                self.reporter.finalize("failed")
                return False
            
            # å‘ç°æ–‡ä»¶
            files_to_migrate = self._discover_files()
            if not files_to_migrate:
                logger.warning("âš ï¸ æœªå‘ç°éœ€è¦è¿ç§»çš„æ–‡ä»¶")
                self.reporter.finalize("completed")
                return True
            
            # è¿ç§»æ–‡ä»¶
            migrated_files = []
            for file_info in files_to_migrate:
                logger.info(f"ğŸ”„ è¿ç§»æ–‡ä»¶: {file_info['file_path']}")
                
                success, s3_key, error = await self._migrate_file(file_info)
                
                status = "success" if success else "failed"
                self.reporter.add_file_result(
                    str(file_info['file_path']), file_info['file_type'], 
                    status, s3_key, error
                )
                
                if success:
                    migrated_files.append({
                        **file_info,
                        'status': 'success',
                        's3_key': s3_key
                    })
            
            # æ›´æ–°æ•°æ®åº“
            if migrated_files:
                await self._update_database_paths(migrated_files)
            
            # å®Œæˆè¿ç§»
            migration_status = "completed" if self.reporter.report["summary"]["total_files_failed"] == 0 else "completed_with_errors"
            self.reporter.finalize(migration_status)
            
            logger.info(f"âœ… è¿ç§»å®Œæˆï¼ŒçŠ¶æ€: {migration_status}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ è¿ç§»å¤±è´¥: {e}")
            self.reporter.add_error(f"è¿ç§»å¤±è´¥: {e}")
            self.reporter.finalize("failed")
            return False


async def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="å°†promptå’Œschemaæ–‡ä»¶è¿ç§»åˆ°S3å­˜å‚¨",
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    
    parser.add_argument(
        '--dry-run',
        action='store_true',
        help='è¯•è¿è¡Œæ¨¡å¼ï¼Œä¸å®é™…æ‰§è¡Œè¿ç§»'
    )
    
    parser.add_argument(
        '--no-backup',
        action='store_true',
        help='ç¦ç”¨å¤‡ä»½åŠŸèƒ½'
    )
    
    parser.add_argument(
        '--environment',
        choices=['local', 'sandbox', 'uat', 'production'],
        default=os.getenv('DATABASE_ENV', 'local'),
        help='æ•°æ®åº“ç¯å¢ƒ'
    )
    
    parser.add_argument(
        '--report-dir',
        type=Path,
        default=project_root / "migration_reports",
        help='æŠ¥å‘Šä¿å­˜ç›®å½•'
    )
    
    parser.add_argument(
        '--log-level',
        choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'],
        default='INFO',
        help='æ—¥å¿—çº§åˆ«'
    )
    
    args = parser.parse_args()
    
    # é…ç½®æ—¥å¿—
    logging.basicConfig(
        level=getattr(logging, args.log_level),
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # è®¾ç½®ç¯å¢ƒ
    os.environ['DATABASE_ENV'] = args.environment
    
    # åˆ›å»ºè¿ç§»å™¨
    migrator = PromptSchemaMigrator(
        dry_run=args.dry_run,
        backup_enabled=not args.no_backup
    )
    
    try:
        # åˆå§‹åŒ–
        await migrator.initialize()
        
        # è¿è¡Œè¿ç§»
        success = await migrator.run_migration()
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = migrator.reporter.save_report(args.report_dir)
        
        # æ‰“å°æ‘˜è¦
        migrator.reporter.print_summary()
        
        if report_file:
            print(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Š: {report_file}")
        
        # è¿”å›é€€å‡ºç 
        sys.exit(0 if success else 1)
        
    except KeyboardInterrupt:
        logger.info("\nğŸ‘‹ è¿ç§»è¢«ç”¨æˆ·ä¸­æ–­")
        migrator.reporter.finalize("cancelled")
        sys.exit(1)
        
    except Exception as e:
        logger.error(f"âŒ è¿ç§»è¿‡ç¨‹ä¸­å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
        migrator.reporter.add_error(f"æœªçŸ¥é”™è¯¯: {e}")
        migrator.reporter.finalize("failed")
        sys.exit(1)
        
    finally:
        await migrator.cleanup()


if __name__ == "__main__":
    asyncio.run(main())