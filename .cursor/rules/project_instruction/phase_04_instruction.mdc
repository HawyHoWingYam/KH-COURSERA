---
description: Phase 4 : End-to-End Workflow & Logic
globs: 
alwaysApply: false
---
# **Detailed Implementation Plan: End-to-End Workflow & Logic (Phase 4\)**

## **1\. Introduction**

**Purpose:** This document provides a granular, step-by-step refinement of "Part 4: End-to-End Workflow & Logic" as delineated in the "Project Instruction.pdf".1 Its objective is to furnish a detailed set of tasks and instructions to guide the implementation team in developing the core document processing functionalities of the platform.

**Scope Recap:** The focus of this document is exclusively on the five distinct workflows outlined within Part 4 of the project instructions: Automated Ingestion & Department Tagging, Scheduled Processing, Immediate Processing, Core AI Processing (FastAPI Backend), and Manual Re-triggering Logic. Specific code implementation details are explicitly out of the scope of this plan; the emphasis is on the sequence of operations, data transformations, system interactions, and critical decision points.

**Intended Audience:** This document is prepared for Technical Leads, Senior Developers, and Project Managers who will be directly involved in or oversee the development and deployment of this phase of the platform. It aims to provide clear, unambiguous, and actionable guidance.

**Document Structure:** The subsequent sections will systematically dissect each of the five workflows into a series of actionable tasks. Each workflow will be presented with an overview, a list of key technology components involved, a summary of its core logic, and a detailed table breaking down the implementation into specific tasks. The document also addresses cross-cutting considerations applicable to all workflows and concludes with a summary of key task groups and recommendations for testing and validation.

## **2\. Detailed Breakdown of End-to-End Workflows (Phase 4\)**

This section provides a comprehensive deconstruction of each workflow identified in Part 4 of the project instructions.1 Each sub-section details the sequence of operations, interactions between different system components, and specific tasks required for implementation.

### **2.1. Automated Ingestion & Department Tagging: Step-by-Step Tasks**

**Overview:** This workflow, as described in Part 4.1 of the project instructions 1, addresses the automated ingestion of new PDF files from an on-premise Windows server into Amazon S3. It also covers the subsequent creation of initial processing\_jobs records in the Amazon Aurora database, including the critical step of associating these jobs with the correct department.

**Key Components:**

* AWS DataSync  
* On-Premise Windows Server  
* Amazon S3  
* FastAPI Backend (potentially invoked via an AWS Lambda trigger from S3 events)  
* Amazon Aurora (PostgreSQL Compatible)

**Core Logic** 1**:**

1. The AWS DataSync Agent, installed on the on-premise Windows server, monitors parent folders named after specific departments (e.g., \\\\server\\share\\Dept\_A, \\\\server\\share\\Dept\_B).  
2. When a new PDF file is added to one of these monitored folders, AWS DataSync automatically uploads it to the corresponding S3 folder structure: s3://your-bucket-name/uploads/{department\_name}/. The {department\_name} in the S3 path is derived directly from the source folder name on the Windows server.  
3. A backend mechanism, likely an AWS Lambda function triggered by S3 ObjectCreated events (which can then interact with the FastAPI backend), is required to:  
   * Create a new record in the processing\_jobs table in the Amazon Aurora database.  
   * Extract the {department\_name} from the S3 object path.  
   * Set the initial status of this new job record to 'pending'.

**Detailed Tasks & Insights:**

Task Group 1: AWS DataSync Configuration & Monitoring  
This group of tasks focuses on setting up and maintaining the AWS DataSync service to ensure reliable file transfer from the on-premise environment to AWS S3.

* **Task AI-01: Define and Configure AWS DataSync Agent:** Install and configure the AWS DataSync Agent on the designated on-premise Windows server. This includes network configuration to allow communication with the AWS DataSync service endpoint.  
* **Task AI-02: Specify DataSync Source Locations:** Define the precise source locations on the Windows server. These are the parent folders named after departments (e.g., \\\\server\\share\\Dept\_A) that DataSync will monitor for new files.1  
* **Task AI-03: Configure DataSync Task for S3 Destination:** Create and configure a DataSync task to map these source folders to the correct S3 destination prefix: s3://your-bucket-name/uploads/{department\_name}/. This mapping is crucial for ensuring files land in department-specific S3 paths.  
* **Task AI-04: Establish DataSync Monitoring and Alerting:** Implement monitoring for the DataSync task, including alerts for file transfer successes, failures, and overall task health. This can be achieved using Amazon CloudWatch metrics and alarms.

Task Group 2: Backend Job Creation Mechanism (S3 Event Trigger)  
This group of tasks details the backend logic required to process the S3 event notifications generated when DataSync uploads new files. This typically involves an AWS Lambda function.

* **Task AI-05: Design and Implement S3 Event Notification:** Configure S3 event notifications for s3:ObjectCreated:\* events specifically on the uploads/ prefix within the designated S3 bucket. These notifications will serve as the trigger for the backend processing logic.  
* **Task AI-06: Develop AWS Lambda Function (or FastAPI Endpoint):** Develop the AWS Lambda function that will be invoked by the S3 event notifications. This function will contain the core logic for creating job records. Alternatively, if a direct S3 event to a FastAPI endpoint is chosen, ensure the endpoint is secure and scalable.  
* **Task AI-07: Department Name Extraction and Validation (Lambda/FastAPI):**  
  * Within the Lambda function (or FastAPI endpoint), implement logic to parse the S3 object key (provided in the S3 event data) to accurately extract the {department\_name} segment from the path.  
  * The reliability of this department name extraction is paramount. Any inconsistencies in the naming of the source folders on the on-premise Windows server 1 will directly propagate to errors in the S3 path created by DataSync. This, in turn, can lead to incorrect department association for the job or a complete failure to associate the job with any department. Such failures have downstream consequences, particularly affecting user visibility of jobs (as users typically view jobs filtered by their department 1) and the application of department-specific document type access rules.1  
  * Therefore, robust error handling for department name extraction is essential. A strategy must be defined for handling files uploaded to S3 paths where a department name cannot be reliably determined or where the extracted name does not correspond to an existing, valid department in the departments table.1 Options include moving such files to a designated "error" or "quarantine" S3 prefix and logging the issue for administrative review.  
  * Implement a query to the departments table in Amazon Aurora to validate that the extracted department\_name exists and to retrieve its corresponding department\_id. This step should include handling cases where the department name is not found in the database (e.g., logging an error, setting the job to a specific error state, or notifying an administrator).  
* **Task AI-08: processing\_jobs Record Creation (Lambda/FastAPI):**  
  * Populate the fields of a new processing\_jobs record:  
    * original\_filename: Extract from S3 event data or object metadata.  
    * s3\_pdf\_path: Construct from S3 event data (bucket name and object key).  
    * status: Set to 'pending' as per the project instruction.1  
    * created\_at: This field has a default value of CURRENT\_TIMESTAMP in the database schema 1, so it will be set automatically upon insertion if not explicitly provided.  
  * A critical consideration arises from the schema of the processing\_jobs table, which specifies uploader\_user\_id and doc\_type\_id as NOT NULL fields.1 However, the project instruction for automated ingestion (Part 4.1) 1 does not explicitly state how these fields should be populated in this automated workflow. This omission is significant because database NOT NULL constraints require these fields to have valid values upon record insertion. Without a valid uploader\_user\_id, accountability for the job's origin is diminished. More critically, without a doc\_type\_id, the Core AI Processing workflow (Part 4.4.c) 1 will be unable to retrieve the necessary prompt.txt and schema.json files, rendering the job unprocessable.  
  * To address this, the implementation team must decide on a strategy:  
    1. **Uploader User ID:** A dedicated "system user" or "automated process user" could be created in the users table. The user\_id of this system account would then be used to populate the uploader\_user\_id field for all jobs created via automated ingestion. This user might have a specific role with limited permissions.  
    2. **Document Type ID:** Several options exist:  
       * **Option A (Default/Unclassified):** Assign a default doc\_type\_id representing an "unclassified" or "pending manual review" status. This would require pre-defining such a document type in the document\_types table.  
       * **Option B (Inference):** Attempt to infer the doc\_type\_id based on the department\_name or other patterns in the S3 sub-folder structure (if any conventions exist beyond just the department name). This approach adds complexity and potential points of failure if the inference logic is not robust.  
       * **Option C (Schema Modification & Manual Assignment):** If the database schema can be altered to allow doc\_type\_id to be NULL initially (currently it is NOT NULL 1), then jobs could be created without a document type. A separate administrative workflow would then be needed for users to manually review these 'pending' jobs and assign an appropriate document type. This option represents a deviation from the current schema definition. The choice among these options must be made before implementation. For the purpose of this task list, it is assumed that a strategy (e.g., using a default doc\_type\_id) will be selected and implemented.  
  * Implement the database insertion logic for the new processing\_jobs record into the Amazon Aurora database.  
  * Incorporate comprehensive error handling for database insertion failures (e.g., connection issues, constraint violations), including logging and potential retry mechanisms or dead-letter queueing for the S3 event.

**Table: Task Breakdown for Automated Ingestion & Department Tagging**

| Task ID | Task Description | Key Components Involved | Inputs/Triggers | Outputs/Expected Results | Notes/Key Considerations |
| :---- | :---- | :---- | :---- | :---- | :---- |
| AI-01 | Define and Configure AWS DataSync Agent | AWS DataSync, On-Premise Windows Server | N/A | Configured DataSync Agent | Ensure network connectivity to AWS. |
| AI-02 | Specify DataSync Source Locations | AWS DataSync, On-Premise Windows Server | Department folder paths (e.g., \\\\server\\share\\Dept\_A) | DataSync configured with source paths | Folder names must match department names for S3 path derivation.1 |
| AI-03 | Configure DataSync Task for S3 Destination | AWS DataSync, Amazon S3 | S3 bucket name, prefix structure (uploads/{department\_name}/) | DataSync task created and active | Correct S3 path mapping is crucial. |
| AI-04 | Establish DataSync Monitoring and Alerting | AWS DataSync, Amazon CloudWatch | DataSync metrics | Monitoring dashboards and alerts for task health | Proactive issue detection. |
| AI-05 | Design and Implement S3 Event Notification | Amazon S3 | s3:ObjectCreated:\* events on uploads/ prefix | S3 events trigger downstream Lambda/FastAPI | Ensure correct prefix filtering. |
| AI-06 | Develop AWS Lambda Function (or FastAPI Endpoint) | AWS Lambda (or FastAPI), Amazon S3 | S3 event data | Lambda function ready to process S3 events | Choose invocation model (Lambda preferred for S3 triggers). |
| AI-07 | Department Name Extraction and Validation (Lambda/FastAPI) | AWS Lambda (or FastAPI), Amazon Aurora | S3 object key | Extracted and validated department\_id | Critical for data integrity. Implement robust error handling for unmatchable department names. See discussion on potential issues. |
| AI-08 | processing\_jobs Record Creation (Lambda/FastAPI) | AWS Lambda (or FastAPI), Amazon Aurora | S3 event data, validated department\_id, chosen uploader\_user\_id and doc\_type\_id | New record in processing\_jobs table with status \= 'pending' | Address NOT NULL constraints for uploader\_user\_id and doc\_type\_id.1 See discussion on resolution strategies. |

### **2.2. Scheduled Processing Implementation: Step-by-Step Tasks**

**Overview:** This workflow, as detailed in Part 4.2 of the project instructions 1, defines a recurring daily batch process. This process is responsible for identifying and processing all jobs that are currently in a 'pending' state.

**Key Components:**

* Amazon EventBridge Scheduler  
* FastAPI Backend  
* Amazon Aurora (PostgreSQL Compatible)

**Core Logic** 1**:**

1. Once daily, the Amazon EventBridge Scheduler triggers a designated FastAPI endpoint.  
2. This FastAPI endpoint queries the processing\_jobs table in the Amazon Aurora database to retrieve all records where the status field is equal to 'pending'.  
3. The endpoint then iterates through each of these pending jobs, executing the "Core AI Processing" logic (as defined in Part 4.4 of the project instructions 1) for each job.

**Detailed Tasks & Insights:**

Task Group 1: Amazon EventBridge Scheduler Configuration  
This set of tasks focuses on establishing the scheduled trigger for the batch processing job.

* **Task SP-01: Create EventBridge Schedule:** Define and create an Amazon EventBridge schedule. This includes specifying the recurrence pattern (e.g., daily at a specific Coordinated Universal Time (UTC)).  
* **Task SP-02: Configure Schedule Target (FastAPI Endpoint):** Configure the target for this EventBridge schedule to be the specific FastAPI endpoint designed to handle the batch processing. This setup will likely involve integrating EventBridge with Amazon API Gateway (if the FastAPI is exposed via API Gateway) or directly invoking an AWS Lambda function that wraps the FastAPI application for this specific task.  
* **Task SP-03: Ensure Target Invocation Permissions:** Grant the necessary IAM permissions to Amazon EventBridge to allow it to invoke the configured target (e.g., API Gateway endpoint or Lambda function).

Task Group 2: FastAPI Endpoint for Scheduled Processing  
These tasks cover the development of the backend logic that executes when triggered by EventBridge.

* **Task SP-04: Develop FastAPI Batch Processing Endpoint:** Implement the FastAPI endpoint that will be triggered by the EventBridge schedule. This endpoint will orchestrate the batch processing.  
* **Task SP-05: Query Pending Jobs (Endpoint Logic):**  
  * Within the endpoint, implement the database query logic to select all records from the processing\_jobs table where the status is 'pending'.  
  * Consider if the order of processing matters. If so, implement an ORDER BY clause in the query (e.g., ORDER BY created\_at ASC for First-In, First-Out processing).  
* **Task SP-06: Iterate and Process Jobs (Endpoint Logic):**  
  * Implement a loop to iterate through the list of job\_ids retrieved from the 'pending' jobs query.  
  * For each job\_id in the list, invoke the "Core AI Processing" logic (detailed in section 2.4 of this document).  
  * The iteration logic must be designed for resilience. If the "Core AI Processing" for a single job within the batch fails (which is handled by the Core AI Processing logic itself by setting the job's status to 'failed' and logging an error message 1), the overall batch process should ideally continue to the next job in the queue rather than terminating prematurely. This requires careful error handling *around* the call to the Core AI Processing function for each individual job within the loop.  
  * For scenarios involving a potentially very large number of pending jobs, consideration should be given to pagination or chunking of the job list. This is important if the FastAPI endpoint, or its underlying compute environment (e.g., AWS Lambda), has execution time limits. While the project documentation 1 does not specify such limits, it is a practical operational concern for long-running batch processes.  
* **Task SP-07: Implement Batch Logging and Monitoring (Endpoint Logic):**  
  * Implement comprehensive logging for the batch process itself. This should include log entries for events such as: batch process started, total number of pending jobs found, number of jobs successfully processed during the batch, number of jobs that failed processing, and batch process completed. These logs are invaluable for monitoring and troubleshooting.  
* The successful execution of this scheduled processing workflow is heavily contingent upon the correct population of the doc\_type\_id and uploader\_user\_id fields during the Automated Ingestion workflow (as discussed in Task AI-08). If doc\_type\_id is missing or invalid for a job created via automated ingestion, the Core AI Processing step (Part 4.4.c of 1) will inevitably fail when it attempts to fetch the associated prompt.txt and schema.json files. Consequently, any jobs that enter the system via automated ingestion without a valid doc\_type\_id will continuously fail during each scheduled batch run. This will lead to an accumulation of failed jobs in the database and inefficient use of processing resources.

**Table: Task Breakdown for Scheduled Processing Implementation**

| Task ID | Task Description | Key Components Involved | Inputs/Triggers | Outputs/Expected Results | Notes/Key Considerations |
| :---- | :---- | :---- | :---- | :---- | :---- |
| SP-01 | Create EventBridge Schedule | Amazon EventBridge Scheduler | Desired schedule (e.g., daily at 02:00 UTC) | EventBridge schedule created and enabled | Choose a suitable, off-peak time if possible. |
| SP-02 | Configure Schedule Target (FastAPI Endpoint) | Amazon EventBridge Scheduler, FastAPI (via API Gateway/Lambda) | FastAPI endpoint URL/ARN | EventBridge target configured | Ensure secure invocation method. |
| SP-03 | Ensure Target Invocation Permissions | Amazon EventBridge Scheduler, IAM | N/A | EventBridge has permission to invoke target | Least privilege principle for IAM role. |
| SP-04 | Develop FastAPI Batch Processing Endpoint | FastAPI | HTTP request from EventBridge | Endpoint ready to handle batch requests |  |
| SP-05 | Query Pending Jobs (Endpoint Logic) | FastAPI, Amazon Aurora | status \= 'pending' | List of pending job\_ids | Consider processing order (e.g., FIFO). |
| SP-06 | Iterate and Process Jobs (Endpoint Logic) | FastAPI | List of pending job\_ids | Each pending job is passed to Core AI Processing | Implement robust iteration; handle individual job failures gracefully. Consider time limits for large batches. |
| SP-07 | Implement Batch Logging and Monitoring (Endpoint Logic) | FastAPI, Amazon CloudWatch Logs | Batch execution events | Detailed logs of batch activity | Essential for operational monitoring. |

### **2.3. Immediate Processing Workflow: Step-by-Step Tasks**

**Overview:** This workflow, described in Part 4.3 of the project instructions 1, enables users to upload PDF files directly through the Next.js user interface for immediate, on-demand processing. A key feature of this workflow is the provision of real-time feedback to the user regarding the processing status via WebSockets.

**Key Components:**

* Next.js User Interface (Frontend)  
* FastAPI Backend  
* WebSockets  
* Amazon S3  
* Amazon Aurora (PostgreSQL Compatible)

**Core Logic** 1**:**

1. A user uploads a PDF file through the Next.js UI. Concurrently, the user selects a specific document\_type for the uploaded file from a provided list.  
2. The frontend application establishes a WebSocket connection with the FastAPI backend.  
3. The backend immediately initiates the "Core AI Processing" logic (defined in Part 4.4 of 1) for the uploaded file.  
4. Throughout the processing, the backend sends real-time status updates (e.g., 'Processing PDF...', 'Calling AI Service...', 'Generating Excel...') to the frontend over the established WebSocket connection.

**Detailed Tasks & Insights:**

Task Group 1: Frontend (Next.js UI) Implementation  
These tasks focus on building the user interface elements and client-side logic for immediate file uploads.

* **Task IP-01: Develop File Upload Component:** Create the file upload component in the Next.js application. This component should allow users to select a PDF file from their local system.  
* **Task IP-02: Develop Document Type Selection Dropdown:** Implement a dropdown menu that allows the user to select the document\_type for the file being uploaded. This list of document types must be dynamically populated and filtered based on the currently logged-in user's department and the document types explicitly assigned to that department via the department\_doc\_type\_access table.1  
* **Task IP-03: Implement File Upload to S3 (or Backend Stream):**  
  * Determine and implement the file transfer mechanism. The project instruction for automated ingestion (Part 4.1 of 1) indicates that uploads are directed to s3://your-bucket-name/uploads/{department\_name}/. For immediate UI uploads, a similar path might be used, or alternatively, files could be uploaded to a specific "immediate-uploads" temporary prefix in S3 before being formally processed and potentially moved by the backend. Another option is to stream the file directly to a backend endpoint.  
  * A crucial point for clarification is: *Where does the UI upload the file initially for immediate processing?* For this task list, it is assumed that the file is first uploaded to a designated S3 location (e.g., a temporary staging area or directly to a path like uploads/{department\_name}/immediate/{filename}), and the resulting S3 path is then communicated to the backend.  
* **Task IP-04: Implement WebSocket Client Logic:** Develop the client-side WebSocket logic in Next.js. This includes establishing a connection to the FastAPI WebSocket endpoint when the user submits a file for immediate processing.  
* **Task IP-05: Implement Real-time UI Updates:** Design and implement UI elements to display the real-time status messages received from the backend via the WebSocket connection. This should include a visual progress bar or status text updates.  
* **Task IP-06: Handle WebSocket Connection State:** Implement robust handling for WebSocket connection events, such as connection errors, unexpected closures, and successful connection termination.

Task Group 2: Backend (FastAPI) WebSocket and Processing Logic  
These tasks involve setting up the server-side WebSocket handling and orchestrating the immediate processing.

* **Task IP-07: Implement FastAPI WebSocket Endpoint(s):** Develop the WebSocket endpoint(s) in the FastAPI application that the Next.js frontend will connect to.  
* **Task IP-08: WebSocket Connection Management (FastAPI):** Implement logic to manage the lifecycle of WebSocket connections, including accepting new connections, handling incoming messages, and managing disconnections.  
* **Task IP-09: Receive Upload Request and Parameters (FastAPI):**  
  * Design the protocol for the frontend to send the necessary information to the backend once the WebSocket connection is established. This will typically include the S3 path of the uploaded PDF file and the doc\_type\_id selected by the user in the UI. This information might be sent as an initial message over the WebSocket or via a preceding HTTP request that then triggers the WebSocket interaction.  
  * A processing\_jobs record must be created for each user-initiated immediate upload. This is fundamental because all document processing activities within the system are tracked and managed through the processing\_jobs table.1 This workflow is not an exception.  
  * Therefore, the backend must create this job record either before initiating or at the very beginning of the Core AI Processing. The record should be populated with:  
    * original\_filename (derived from the uploaded file).  
    * s3\_pdf\_path (the S3 location where the UI uploaded the file).  
    * status can be set initially to 'pending' and then immediately updated to 'processing' by the Core AI logic, or set directly to 'processing' if the Core AI logic is invoked synchronously.  
    * uploader\_user\_id (this must be the user\_id of the authenticated user performing the upload, obtained from their session).  
    * doc\_type\_id (the ID selected by the user in the UI).  
* **Task IP-10: Initiate Core AI Processing (FastAPI):**  
  * Once the processing\_jobs record is successfully created and its job\_id is known, the backend will invoke the "Core AI Processing" logic (detailed in section 2.4) for this newly created job\_id.  
* **Task IP-11: Implement Real-time Status Updates via WebSocket (FastAPI):**  
  * The project instruction (Part 4.3 of 1) specifies that the backend sends real-time status updates like 'Processing...', 'Generating Excel...' over the WebSocket. However, the "Core AI Processing" logic as defined in Part 4.4 of 1 does not explicitly mention emitting such granular sub-status updates.  
  * This implies a need for careful design. Either the Core AI Processing module itself must be adapted to yield or send these intermediate status updates, or the FastAPI function handling the immediate processing request needs to wrap calls to different stages of the Core AI Processing and send distinct WebSocket messages accordingly. For instance, a message could be sent after PDF download, before the Gemini API call, after the Gemini API call succeeds, and before Excel generation starts.  
  * Define the specific points within or around the Core AI Processing workflow where status messages will be generated and transmitted over the WebSocket to the connected client.  
* **Task IP-12: Final Result Communication and Connection Closure (FastAPI):**  
  * After the Core AI Processing for the job completes (whether in a 'success' or 'failed' state), the backend must send a final status message over the WebSocket. If successful, this message could include direct download links for the generated JSON and Excel files.1 If failed, it should include the error message.  
  * The WebSocket connection should then be closed gracefully, or, if the application design requires it, kept open for potential further actions from the user.

**Table: Task Breakdown for Immediate Processing Workflow**

| Task ID | Task Description | Key Components Involved | Inputs/Triggers | Outputs/Expected Results | Notes/Key Considerations |
| :---- | :---- | :---- | :---- | :---- | :---- |
| IP-01 | Develop File Upload Component | Next.js | User interaction | UI for file selection | Standard file input handling. |
| IP-02 | Develop Document Type Selection Dropdown | Next.js, FastAPI (for list population) | User's department, department\_doc\_type\_access table data | UI dropdown with allowed document types | List must be filtered by department access.1 |
| IP-03 | Implement File Upload to S3 (or Backend Stream) | Next.js, Amazon S3 (or FastAPI) | Selected PDF file | File uploaded to S3, S3 path available | Clarify S3 upload path for immediate uploads. |
| IP-04 | Implement WebSocket Client Logic | Next.js | Upload submission | WebSocket connection established with backend | Use a robust WebSocket client library. |
| IP-05 | Implement Real-time UI Updates | Next.js | WebSocket messages from backend | UI displays progress and status updates | Clear visual feedback for the user. |
| IP-06 | Handle WebSocket Connection State | Next.js | WebSocket events | Graceful handling of connection lifecycle | Inform user of connection issues. |
| IP-07 | Implement FastAPI WebSocket Endpoint(s) | FastAPI | N/A | WebSocket endpoint available |  |
| IP-08 | WebSocket Connection Management (FastAPI) | FastAPI | Client connection requests | Active WebSocket connections managed | Handle multiple concurrent connections. |
| IP-09 | Receive Upload Request and Parameters; Create processing\_jobs Record (FastAPI) | FastAPI, Amazon Aurora | S3 path, doc\_type\_id, authenticated user ID | New processing\_jobs record created | Crucial for tracking; populate all required fields correctly. |
| IP-10 | Initiate Core AI Processing (FastAPI) | FastAPI | job\_id of the new job | Core AI Processing started |  |
| IP-11 | Implement Real-time Status Updates via WebSocket (FastAPI) | FastAPI, Core AI Processing | Progress stages within Core AI Processing | Status messages sent to client | Design how Core AI communicates progress for WebSocket updates. |
| IP-12 | Final Result Communication and Connection Closure (FastAPI) | FastAPI | Completion of Core AI Processing (success/failure) | Final status, download links (if success), or error message sent; WebSocket closed |  |

### **2.4. Core AI Processing Logic (FastAPI Backend): Detailed Task Breakdown**

**Overview:** This section details the implementation of the central, reusable backend service responsible for the actual document analysis and data extraction. This "Core AI Processing" logic, outlined in Part 4.4 of the project instructions 1, is invoked by Scheduled Processing, Immediate Processing, and the Manual Re-triggering workflows.

**Key Components:**

* FastAPI Backend  
* Amazon S3  
* Amazon Aurora (PostgreSQL Compatible)  
* Google Gemini API

**Input:** The primary input to this service is a job\_id, which corresponds to an existing record in the processing\_jobs table.

Core Logic 1:  
The service performs the following sequence of steps for a given job\_id:

1. Sets the status of the job in the processing\_jobs table to 'processing'.  
2. Downloads the raw PDF file from the s3\_pdf\_path specified in the job's record.  
3. Retrieves the doc\_type\_id associated with the job and uses it to fetch the corresponding prompt.txt and schema.json configuration files from their S3 locations (paths are stored in the document\_types table).  
4. Calls the Google Gemini API, providing the content of the PDF document and the retrieved prompt.  
5. **On Success from Gemini API:**  
   * Logs the input\_token\_count and output\_token\_count from the API response to the api\_usage table, linking this usage record to the job\_id.  
   * Saves the JSON output returned by the Gemini API to the S3 path s3://your-bucket-name/processed/json/.  
   * Generates an Excel file from the JSON data and saves it to s3://your-bucket-name/processed/excel/.  
   * Updates the processing\_jobs record with the S3 paths of the newly created JSON and Excel files and sets the job status to 'success'.  
6. **On Failure (during any step, including the Gemini API call):**  
   * Logs detailed error information into the error\_message field of the processing\_jobs record.  
   * Sets the job status to 'failed'.

**Detailed Tasks & Insights:**

* **Task CAI-01: Function Definition and Input Validation**  
  * Define a clear service function or method within the FastAPI application that accepts job\_id as its primary parameter.  
  * As an initial step, this function must query the processing\_jobs table using the provided job\_id. This is to ensure the job actually exists and to retrieve essential details such as s3\_pdf\_path and doc\_type\_id. Implement robust handling for cases where the job\_id is not found in the database (e.g., log an error and return an appropriate failure response).  
* **Task CAI-02 (Corresponds to 4.4.a): Update Job Status to 'Processing'**  
  * Implement the database update logic to change the status field of the processing\_jobs record (for the given job\_id) to 'processing'. This indicates that the job has been picked up and active work has commenced.  
* **Task CAI-03 (Corresponds to 4.4.b): Download PDF from S3**  
  * Implement S3 client logic to download the PDF file from the s3\_pdf\_path obtained from the job's record.  
  * This step must include error handling for potential S3 issues, such as the file not being found at the specified path (which could indicate an earlier problem or data inconsistency) or access denied errors (indicating IAM permission issues).  
* **Task CAI-04 (Corresponds to 4.4.c): Retrieve Document Type Configurations**  
  * Using the doc\_type\_id retrieved from the processing\_jobs record, query the document\_types table to obtain the s3\_prompt\_path and s3\_schema\_path for the specific document type.  
  * Implement S3 client logic to download the prompt.txt and schema.json files from these respective S3 paths.  
  * Error handling is crucial here: address scenarios where the configuration files are not found at the S3 paths specified in the database, or other S3-related access/download issues. A missing prompt or schema would prevent further processing.  
* **Task CAI-05 (Corresponds to 4.4.d): Call Google Gemini API**  
  * Implement the logic to prepare the request for the Google Gemini API. This will likely involve extracting text content, and potentially image data, from the downloaded PDF. The project instruction 1 refers to "document content," implying the full relevant content of the PDF.  
  * Make the API call to the Google Gemini service, providing the extracted document content and the content of the loaded prompt.txt file. The schema.json might be used by Gemini or to structure the prompt for desired output format.  
  * The Gemini API key must be managed securely. While the Admin Portal (Part 5 of 1) includes a page for updating this key, for backend services, it is best practice to store and retrieve such sensitive credentials using a service like AWS Secrets Manager.2 The FastAPI application would then fetch the key from Secrets Manager at runtime.  
  * Implement comprehensive handling for responses from the Gemini API, including successful responses and various error conditions (e.g., API-specific errors, authentication failures, rate limiting, network timeouts).  
* **Task CAI-06 (Corresponds to 4.4.e): On Gemini Success**  
  * **Task CAI-06.1 (Corresponds to 4.4.e.i): Log API Usage**  
    * Extract the input\_token\_count and output\_token\_count from the successful Gemini API response.  
    * Insert a new record into the api\_usage table. This record must be linked to the current job\_id and store the token counts along with a timestamp (the api\_call\_timestamp defaults to CURRENT\_TIMESTAMP 1).  
    * The api\_usage table 1 plays a vital role in tracking the costs associated with Google Gemini API calls. Accurate and consistent logging at this step is crucial for financial oversight, for identifying document types or prompts that are particularly expensive to process, and for potential departmental cost allocation. This data directly supports features like the Admin Dashboard's "token usage for cost analysis" chart.1  
  * **Task CAI-06.2 (Corresponds to 4.4.e.ii): Save Processed Files to S3**  
    * Save the JSON response received from the Gemini API to the specified S3 path: s3://your-bucket-name/processed/json/. A clear and unique naming convention for these files is essential. For example, filenames could incorporate the job\_id and/or the original\_filename (e.g., {job\_id}\_{original\_filename}.json) to ensure uniqueness and traceability. The project instruction 1 currently only specifies the prefix.  
    * Implement logic to convert the JSON data (returned by Gemini) into an Excel file. The schema.json (retrieved in Task CAI-04) might be instrumental here, potentially defining the structure, columns, and data types for the Excel output.  
    * Save the generated Excel file to the S3 path: s3://your-bucket-name/processed/excel/, again using a consistent and unique naming convention (e.g., {job\_id}\_{original\_filename}.xlsx).  
    * Consider the atomicity of these file-saving operations. For instance, if the JSON file saves successfully but the subsequent Excel generation or S3 upload fails, the job is left in an inconsistent state. The system needs a clear rule: does a partial success (one file saved, one failed) still count as 'success', or should it be marked as 'failed', or perhaps a new 'partial\_success' status should be introduced? The current project instruction 1 implies a binary outcome of full 'success' or 'failed'. If full success requires both files, then failure to save either should lead to a 'failed' status for the job.  
  * **Task CAI-06.3 (Corresponds to 4.4.e.iii): Update Job Record to 'Success'**  
    * Update the processing\_jobs record for the current job\_id with the following information:  
      * Set s3\_json\_path to the full S3 path of the saved JSON file.  
      * Set s3\_excel\_path to the full S3 path of the saved Excel file.  
      * Set status to 'success'.  
      * Ensure any pre-existing error\_message in the job record is cleared or set to NULL.  
* **Task CAI-07 (Corresponds to 4.4.f): On Failure (Any Step or Gemini Failure)**  
  * This block of logic should be executed if any preceding step in the Core AI Processing fails (e.g., PDF download, config file retrieval, Gemini API call, file saving).  
  * **Task CAI-07.1 (Corresponds to 4.4.f.i): Log Error Details**  
    * Capture comprehensive error information. This should include details about which step failed, the specific error message received (e.g., from S3 client, Gemini API, or internal logic), and potentially a stack trace if applicable.  
    * Update the error\_message field in the processing\_jobs record for the current job\_id with this detailed error information.  
  * **Task CAI-07.2 (Corresponds to 4.4.f.ii): Set Job Status to 'Failed'**  
    * Update the status field of the processing\_jobs record to 'failed'.  
* If this Core AI Processing module is also intended to provide granular WebSocket updates when invoked in "immediate processing" mode (as discussed in Task IP-11), then tasks for sending these WebSocket messages need to be carefully interleaved within this logic. For example:  
  * After successfully completing Task CAI-02 (status update to 'processing'), a WebSocket message "Status: Initializing processing" could be sent.  
  * After successfully completing Task CAI-03 (PDF download), a message "Status: PDF downloaded" could be sent.  
  * After successfully completing Task CAI-04 (config retrieval), a message "Status: AI configuration retrieved" could be sent.  
  * Immediately before Task CAI-05 (calling Gemini), a message "Status: Calling AI service..." could be sent.  
  * After successfully completing Task CAI-06.2 (saving Excel), a message "Status: Excel generation complete" could be sent. This requires the Core AI Processing module to be aware of the context in which it's running (e.g., via a parameter indicating "immediate mode") or to utilize a callback mechanism to delegate message sending to the calling function (like the WebSocket handler in section 2.3).

**Table: Task Breakdown for Core AI Processing Logic**

| Task ID | Task Description | Key Components Involved | Inputs/Triggers | Outputs/Expected Results | Notes/Key Considerations |
| :---- | :---- | :---- | :---- | :---- | :---- |
| CAI-01 | Function Definition and Input Validation | FastAPI, Amazon Aurora | job\_id | Validated job\_id and fetched job details (or error) | Handle job\_id not found. |
| CAI-02 | Update Job Status to 'Processing' | FastAPI, Amazon Aurora | job\_id | processing\_jobs.status updated to 'processing' |  |
| CAI-03 | Download PDF from S3 | FastAPI, Amazon S3 | s3\_pdf\_path from job record | PDF file downloaded to backend | Handle S3 errors (file not found, access denied). |
| CAI-04 | Retrieve Document Type Configurations | FastAPI, Amazon Aurora, Amazon S3 | doc\_type\_id from job record | prompt.txt and schema.json downloaded | Handle errors if config files are missing. |
| CAI-05 | Call Google Gemini API | FastAPI, Google Gemini API | PDF content, prompt content | Response from Gemini API (JSON data or error) | Secure Gemini API key management (e.g., AWS Secrets Manager 4). Handle API errors. |
| CAI-06.1 | Log API Usage (On Gemini Success) | FastAPI, Amazon Aurora (api\_usage table) | input\_token\_count, output\_token\_count from Gemini | New record in api\_usage table | Crucial for cost tracking.1 |
| CAI-06.2 | Save Processed Files to S3 (On Gemini Success) | FastAPI, Amazon S3 | Gemini JSON output, (Excel derived from JSON) | JSON and Excel files saved to processed/json/ and processed/excel/ S3 prefixes | Define unique file naming. Address atomicity of saving both files. |
| CAI-06.3 | Update Job Record to 'Success' (On Gemini Success) | FastAPI, Amazon Aurora | job\_id, S3 paths for JSON/Excel | processing\_jobs record updated: s3\_json\_path, s3\_excel\_path set, status \= 'success', error\_message cleared |  |
| CAI-07.1 | Log Error Details (On Failure) | FastAPI, Amazon Aurora | Error information from failed step | processing\_jobs.error\_message updated | Capture comprehensive error details. |
| CAI-07.2 | Set Job Status to 'Failed' (On Failure) | FastAPI, Amazon Aurora | job\_id | processing\_jobs.status updated to 'failed' |  |
| (Conditional) | Interleave WebSocket Status Updates | FastAPI, WebSockets | Progress points within Core AI | Status messages sent if in "immediate" mode | Requires context awareness or callback mechanism. |

### **2.5. Manual Re-triggering Mechanism: Step-by-Step Tasks**

**Overview:** This workflow, detailed in Part 4.5 of the project instructions 1, provides a mechanism for users (both 'admin' and 'user' roles) to initiate a retry for a job that has previously failed. A key feature of this retry mechanism is the ability to change the Document Type assigned to the job before re-processing.

**Key Components:**

* Next.js User Interface (Admin Portal and User Portal)  
* FastAPI Backend  
* Amazon Aurora (PostgreSQL Compatible)

**Core Logic** 1**:**

1. In the UI (Job Management pages of Admin or User Portals), a user or admin clicks a "Retry" button associated with a failed job.  
2. A modal dialog opens, allowing the user to confirm the retry action. Crucially, this modal also allows the user to select a different Document Type for the retry attempt.  
3. Upon submission from the modal, the backend performs the following:  
   * It creates a *new* record in the processing\_jobs table. This is done to preserve the history of the original failed attempt.  
   * This new job record is linked to the original PDF file (i.e., it uses the same s3\_pdf\_path).  
   * The backend then initiates the "Core AI Processing" logic for this newly created job record.

**Detailed Tasks & Insights:**

Task Group 1: Frontend (Next.js UI) Implementation  
These tasks focus on the UI elements required for initiating a job retry.

* **Task MR-01: Implement "Retry" Button:** Add a "Retry" button to the Job Management pages in both the Admin Portal and User Portal. This button should only be active or visible for jobs that have a status of 'failed'.1  
* **Task MR-02: Develop Confirmation and Document Type Change Modal:**  
  * Create a modal component that appears when the "Retry" button is clicked.  
  * This modal should display key details of the original failed job (e.g., filename, original failure reason if available).  
  * The modal must include a dropdown menu for selecting or changing the doc\_type\_id. This dropdown should be pre-populated with the doc\_type\_id used in the original failed attempt but must allow the user to select any other doc\_type\_id. For users with the 'user' role, this list of available document types must be filtered according to their department's access rights, as defined in the department\_doc\_type\_access table.1 Admins should typically see all available document types.  
* **Task MR-03: API Call on Modal Submission:** When the user confirms the retry action in the modal, the frontend should make an API call to a dedicated backend endpoint for re-triggering jobs. This API call must pass the job\_id of the *original failed job* and the (potentially new) doc\_type\_id selected by the user.

Task Group 2: Backend (FastAPI) Re-triggering Logic  
These tasks detail the server-side logic for handling retry requests.

* **Task MR-04: Develop FastAPI Re-triggering Endpoint:** Create a new FastAPI endpoint to receive and process job re-trigger requests from the frontend.  
* **Task MR-05: Implement Authorization for Re-triggering:**  
  * Before proceeding, the endpoint must verify that the authenticated user making the request has the necessary permissions to re-trigger the specified original job\_id. According to the project instructions 1:  
    * Admins can view and re-trigger jobs from all users and departments.  
    * Users can view and re-trigger their own jobs and all jobs belonging to other users within the same department.  
  * This requires checking the user's role and, if they are a 'user', their department affiliation against the department of the original job.  
* **Task MR-06: Retrieve Original Failed Job Details:**  
  * Query the processing\_jobs table using the job\_id of the original failed job. This query is necessary to fetch details like original\_filename and, critically, the s3\_pdf\_path.  
  * A decision is needed regarding the uploader\_user\_id for the new job record created during the retry. The instruction "linked to the original file" 1 clearly means the s3\_pdf\_path is reused. For the uploader\_user\_id of the *new* job, it is more appropriate for audit and accountability purposes to use the user\_id of the user who is initiating the retry action, rather than copying it from the original failed job. This clearly attributes the retry attempt to the correct individual.  
* **Task MR-07: Create New processing\_jobs Record:**  
  * Insert a new record into the processing\_jobs table. This new record represents the retry attempt. It should be populated as follows:  
    * original\_filename: Copied from the original failed job record.  
    * s3\_pdf\_path: Copied from the original failed job record (this is crucial for "linked to the original file" as per 1).  
    * status: Set to 'pending' initially (or directly to 'processing' if the Core AI Processing is to be invoked immediately and synchronously by this endpoint).  
    * uploader\_user\_id: Set to the user\_id of the authenticated user who is performing the retry action (as determined in Task MR-06).  
    * doc\_type\_id: Set to the new (or re-confirmed) doc\_type\_id received from the UI modal.  
    * s3\_json\_path, s3\_excel\_path, and error\_message should all be NULL for this new job record initially.  
  * The creation of a new job record for each retry attempt is explicitly required by the project instruction (Part 4.5 of 1) "to preserve the history of the original failure." This approach is excellent for auditability, as it allows for tracking multiple retry attempts for a single original file and analyzing whether changes in doc\_type\_id lead to successful processing. While not explicitly stated as a requirement in 1, the UI might later be enhanced to show relationships or links between an original failed job and its subsequent retry attempts.  
* **Task MR-08: Initiate Core AI Processing for the New Job:**  
  * Once the new processing\_jobs record has been successfully created and its job\_id is available, invoke the "Core AI Processing" logic (detailed in section 2.4) for this *new* job\_id.  
  * Consider whether this re-trigger action is expected to provide real-time WebSocket feedback to the user similar to the Immediate Processing workflow. The project instruction 1 does not specify this for retries. If real-time feedback is desired, then the mechanisms described in section 2.3 (Immediate Processing) for WebSocket communication would need to be integrated here as well. For the current scope, it is assumed that the re-trigger is an asynchronous action unless specified otherwise, meaning the user might check the job status later on the Job Management page.

**Table: Task Breakdown for Manual Re-triggering Mechanism**

| Task ID | Task Description | Key Components Involved | Inputs/Triggers | Outputs/Expected Results | Notes/Key Considerations |
| :---- | :---- | :---- | :---- | :---- | :---- |
| MR-01 | Implement "Retry" Button | Next.js (Admin & User Portals) | Failed job entries in UI | "Retry" button available for failed jobs | Button should only target jobs with status \= 'failed'. |
| MR-02 | Develop Confirmation and Document Type Change Modal | Next.js | User clicks "Retry" button | Modal displayed with job details and doc\_type\_id dropdown | doc\_type\_id list filtered by user's department access.1 |
| MR-03 | API Call on Modal Submission | Next.js, FastAPI | Original job\_id, new/confirmed doc\_type\_id | API request sent to backend re-trigger endpoint |  |
| MR-04 | Develop FastAPI Re-triggering Endpoint | FastAPI | HTTP request from frontend | Endpoint ready to handle retry requests |  |
| MR-05 | Implement Authorization for Re-triggering | FastAPI, Amazon Aurora | Authenticated user, original job\_id | User permission to retry job verified | Enforce admin vs. user retry rules.1 |
| MR-06 | Retrieve Original Failed Job Details | FastAPI, Amazon Aurora | Original job\_id | original\_filename, s3\_pdf\_path fetched | Determine uploader\_user\_id for the new job (recommend retrying user). |
| MR-07 | Create New processing\_jobs Record | FastAPI, Amazon Aurora | Details from original job, new doc\_type\_id, retrying uploader\_user\_id | New processing\_jobs record created with status \= 'pending' | Preserves history of original failure.1 |
| MR-08 | Initiate Core AI Processing for the New Job | FastAPI, Core AI Processing | job\_id of the new retry job | Core AI Processing started for the new job | Consider if real-time WebSocket feedback is needed for retries. |

## **3\. Cross-Cutting Considerations & Dependencies**

Successful implementation of the Phase 4 workflows requires attention to several overarching aspects that impact multiple components of the system.

**3.1. Database Interaction Patterns**

* **Central Role of processing\_jobs Table:** The processing\_jobs table is the cornerstone for tracking the state of every document processed by the system. All five workflows detailed in Part 4 (Automated Ingestion, Scheduled Processing, Immediate Processing, Core AI Processing, and Manual Re-triggering) will perform read and/or write operations on this table.1 Its schema and the integrity of its data are paramount.  
* **Significance of api\_usage Table:** The api\_usage table, which is directly linked to processing\_jobs via job\_id, is critical for monitoring and managing the costs associated with the Google Gemini API. Accurate population of this table during the Core AI Processing workflow is essential for the cost analysis features of the Admin Portal dashboard.1  
* **Idempotency Considerations:** For operations that might be subject to retries (e.g., due to transient network issues before a final state is committed to the database), it is important to consider if any individual steps within the workflows need to be designed as idempotent. Idempotency ensures that performing an operation multiple times has the same effect as performing it once. While not explicitly mandated in Phase 4 of 1, designing for idempotency where appropriate can significantly enhance the robustness and fault tolerance of the system, particularly for backend processes interacting with external services or databases.  
* **Database Connectivity:** The FastAPI backend will require a robust mechanism to connect to the Amazon Aurora PostgreSQL database. This typically involves using SQLAlchemy with appropriate database drivers (e.g., psycopg2-binary) and managing connection pools effectively.5 Connection strings and credentials should be securely managed, ideally using AWS Secrets Manager for database credentials 2, which can then be accessed by the FastAPI application.

**3.2. S3 Bucket and Path Conventions Adherence**

* Strict and consistent adherence to the Amazon S3 bucket structure and path conventions defined in Part 2 of the project instructions 1 is critical for all S3 interactions within Phase 4\. This includes:  
  * s3://your-bucket-name/uploads/{department\_name}/ for raw PDFs from automated ingestion.  
  * s3://your-bucket-name/processed/json/ for JSON outputs from the Gemini API.  
  * s3://your-bucket-name/processed/excel/ for Excel files generated from JSON data.  
  * s3://your-bucket-name/configs/prompts/ for storing prompt.txt and schema.json files that define document types.  
* Clear, unique, and traceable naming conventions must be defined and implemented for the processed JSON and Excel files stored in the processed/ S3 prefixes. Incorporating the job\_id or a universally unique identifier (UUID) into filenames is a common practice to ensure uniqueness and facilitate easy lookup.

**3.3. Error Logging and Reporting Consistency**

* All error messages logged to the processing\_jobs.error\_message field must be comprehensive, structured, and contain sufficient detail to enable effective debugging by administrators or support personnel.  
* It is highly recommended to implement centralized logging for all backend services, including the FastAPI application and any AWS Lambda functions. AWS CloudWatch Logs is a suitable service for this, providing a consolidated view of system behavior, errors, and performance metrics across different components.

**3.4. Security Considerations (High-Level)**

* **Gemini API Key Management:** The Google Gemini API key is a sensitive credential. While Part 5 of 1 mentions a Global Settings Page in the Admin Portal for inputting and updating this key, the backend service (FastAPI) that directly uses this key must access it securely. Storing the API key in AWS Secrets Manager and having the FastAPI application retrieve it at runtime is a recommended best practice.4 The Admin Portal would then interact with Secrets Manager to update the key.  
* **IAM Roles and Permissions:** All backend services (FastAPI application running on EC2/ECS/Lambda, and any Lambda functions) must operate under AWS Identity and Access Management (IAM) roles that adhere to the principle of least privilege. These roles should grant only the necessary permissions to access other AWS services such as S3 (e.g., read from uploads/ and configs/, write to processed/ 7), Amazon Aurora, Amazon EventBridge, and AWS Secrets Manager.  
* **Authentication and Authorization for Endpoints:** While AWS Cognito is responsible for user authentication for the frontend portals 1, the FastAPI backend endpoints themselves (especially those handling immediate processing requests, manual re-triggering, and any data modification APIs) must validate the user's session or token and enforce the defined authorization rules. For example, ensuring a 'user' role can only re-trigger jobs belonging to their own profile or department.1  
* **Network Security:**  
  * The Amazon Aurora database should be deployed in private subnets within a Virtual Private Cloud (VPC) to prevent direct access from the public internet.9 The FastAPI backend, if running in the same VPC, can then access the database over private network connections.  
  * If the FastAPI application is exposed to the internet (e.g., via API Gateway), appropriate security measures like Web Application Firewalls (WAF) should be considered.  
  * Use of public and private subnets should follow best practices. Public subnets can host internet-facing resources like NAT Gateways or Application Load Balancers, while private subnets host backend resources like the database and application servers.11 NAT Gateways allow instances in private subnets to initiate outbound internet connections (e.g., to call the Google Gemini API) while preventing inbound connections from the internet.12

**3.5. Configuration Management**

* A clear strategy for managing environment-specific configurations for the FastAPI application is needed. This includes S3 bucket names, the Google Gemini API endpoint URL, database connection details (though credentials should be in Secrets Manager), and other configurable parameters. Common approaches include using environment variables (which can be injected by the hosting environment like ECS, Lambda, or Elastic Beanstalk) or dedicated configuration files that are not committed to source control.

## **4\. Summary of Key Task Groups and Recommendations**

This document has provided a detailed refinement of the end-to-end workflows critical for Phase 4 of the project. The implementation of these workflows will establish the core document processing capabilities of the platform.

Recap of Major Task Categories:  
The implementation effort for Phase 4 can be summarized into the following major categories:

1. **Automated File Ingestion:** Setting up AWS DataSync, S3 event handling, and initial processing\_jobs record creation with robust department tagging and resolution of uploader\_user\_id and doc\_type\_id ambiguities.  
2. **Scheduled Batch Processing:** Configuring Amazon EventBridge Scheduler and developing the FastAPI endpoint to iterate through and process pending jobs using the Core AI Processing logic.  
3. **User-Driven Immediate Processing:** Implementing the Next.js UI for file upload and document type selection, establishing WebSocket communication with the FastAPI backend, and providing real-time status updates during processing.  
4. **Centralized Core AI Processing:** Developing the reusable FastAPI service for PDF download, document configuration retrieval, Google Gemini API interaction, API usage logging, and processed file (JSON, Excel) generation and storage.  
5. **Auditable Manual Job Re-triggering:** Implementing the UI and backend logic for retrying failed jobs, including the ability to change document types and ensuring new job records are created to preserve history.

Key Dependencies Highlighted:  
Several critical dependencies and decision points have been identified:

* **Resolution of uploader\_user\_id and doc\_type\_id for Automated Ingestion:** A clear strategy must be defined and implemented to populate these NOT NULL fields 1 for jobs created via the automated ingestion workflow. Failure to do so will prevent these jobs from being processed.  
* **Consistent S3 Path and File Naming Conventions:** Strict adherence to the defined S3 structure 1 and the establishment of clear, unique naming conventions for processed files are essential for data organization and retrieval.  
* **Reliable External API (Gemini) Interaction:** Secure management of the Gemini API key and robust error handling for API calls are crucial for the stability of the Core AI Processing logic.  
* **Granular WebSocket Status Updates:** Design decisions are needed on how the Core AI Processing module will facilitate the real-time status updates required for the Immediate Processing workflow.

Recommendations for Testing and Validation:  
Thorough testing is paramount to ensure the reliability, accuracy, and performance of these interconnected workflows. A multi-layered testing strategy is recommended:

* **Unit Tests:** Focus on testing individual functions and modules within the FastAPI backend in isolation. Examples include:  
  * PDF content extraction logic.  
  * Client for interacting with the Google Gemini API (mocking the actual API calls).  
  * Logic for converting JSON data to Excel format.  
  * Database query functions.  
* **Integration Tests:** Verify the interactions between different components of the system:  
  * Test the complete flow from an S3 ObjectCreated event through the Lambda/FastAPI function to the creation of a processing\_jobs record in Amazon Aurora.  
  * Test the Amazon EventBridge Scheduler trigger and the successful invocation of the scheduled batch processing endpoint in FastAPI.  
  * Test the full WebSocket communication lifecycle between the Next.js frontend and the FastAPI backend, including connection establishment, message passing, and status updates during immediate processing.  
  * Test the Core AI Processing service as a whole, using various types of PDF documents and different prompt.txt/schema.json combinations to ensure correct behavior with the actual Gemini API (in a test environment).  
* **End-to-End Tests:** Simulate the complete lifecycle for each of the main workflows:  
  * **Automated Ingestion & Scheduled Processing:** Upload a PDF to a monitored folder on the on-premise server, verify its transfer to S3 by DataSync, confirm the creation of a 'pending' job record, and then trigger the scheduled process to verify the job is processed to 'success' or 'failed'.  
  * **Immediate Processing:** Upload a PDF via the UI, select a document type, and verify real-time status updates and the final outcome (successful processing with downloadable files or a clear failure message).  
  * **Manual Re-triggering:** Identify a (deliberately) failed job, use the UI to re-trigger it (optionally changing the document type), and verify that a new job record is created and processed.  
* **Failure Scenario Testing:** Proactively test how the system handles various error conditions:  
  * Invalid or malformed S3 paths.  
  * Failures from the Google Gemini API (e.g., authentication errors, rate limits, invalid responses).  
  * Malformed or unreadable PDF files.  
  * Incorrect or missing document\_type configurations (prompt/schema files).  
  * Database connectivity issues or insertion failures. Verify that errors are logged correctly in processing\_jobs.error\_message, job statuses are updated appropriately to 'failed', and retry mechanisms function as expected.  
* **Scalability and Performance Testing:**  
  * Particularly for the Scheduled Processing workflow, test its performance with a large volume of pending jobs to identify potential bottlenecks and ensure the FastAPI endpoint and database can handle the load within acceptable timeframes.  
  * Assess the response time of the Immediate Processing workflow under concurrent user load.

By addressing these tasks and considerations methodically, the development team can successfully implement the robust and efficient end-to-end document processing workflows outlined for Phase 4\.



